{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-CTDpczmykS"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import tarfile\n",
        "import gdown\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, Sampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import QuadMesh\n",
        "import seaborn as sn\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhKP6gCErKNg"
      },
      "outputs": [],
      "source": [
        "ckpt_dir = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data'\n",
        "dataset_path = \"/content/drive/MyDrive/msc_project/datasets\"\n",
        "# dataset_file_name = {\n",
        "#     \"imdb62\": 'full_imdb62.csv',\n",
        "#     \"blog\": 'full_blog.csv',\n",
        "#     \"turing\": \"turing_ori_0208.csv\"\n",
        "# }\n",
        "datasets = {\n",
        "    'contrax_datasets.tar': 'https://drive.google.com/uc?id=1T3VgMe-dCy5QVI7b1K2KdfL-2e2gq2Rn'\n",
        "}\n",
        "os.makedirs(dataset_path, exist_ok=True)\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "random.seed(0)\n",
        "np.random.seed(0)\n",
        "torch.manual_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YXk1SZGjqOha"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cmJelnvqQC9"
      },
      "outputs": [],
      "source": [
        "def get_new_fig(fn, figsize=[9, 9]):\n",
        "    \"\"\" Init graphics \"\"\"\n",
        "    fig1 = plt.figure(fn, figsize)\n",
        "    ax1 = fig1.gca()  # Get Current Axis\n",
        "    ax1.cla()  # clear existing plot\n",
        "    return fig1, ax1\n",
        "\n",
        "\n",
        "def configcell_text_and_colors(array_df, lin, col, oText, facecolors, posi, fz, fmt, show_null_values=0):\n",
        "    \"\"\"\n",
        "      config cell text and colors\n",
        "      and return text elements to add and to dell\n",
        "      @TODO: use fmt\n",
        "    \"\"\"\n",
        "    text_add = [];\n",
        "    text_del = [];\n",
        "    cell_val = array_df[lin][col]\n",
        "    tot_all = array_df[-1][-1]\n",
        "    per = (float(cell_val) / tot_all) * 100\n",
        "    curr_column = array_df[:, col]\n",
        "    ccl = len(curr_column)\n",
        "\n",
        "    # last line  and/or last column\n",
        "    if (col == (ccl - 1)) or (lin == (ccl - 1)):\n",
        "        # tots and percents\n",
        "        if (cell_val != 0):\n",
        "            if (col == ccl - 1) and (lin == ccl - 1):\n",
        "                tot_rig = 0\n",
        "                for i in range(array_df.shape[0] - 1):\n",
        "                    tot_rig += array_df[i][i]\n",
        "                per_ok = (float(tot_rig) / cell_val) * 100\n",
        "            elif (col == ccl - 1):\n",
        "                tot_rig = array_df[lin][lin]\n",
        "                per_ok = (float(tot_rig) / cell_val) * 100\n",
        "            elif (lin == ccl - 1):\n",
        "                tot_rig = array_df[col][col]\n",
        "                per_ok = (float(tot_rig) / cell_val) * 100\n",
        "            per_err = 100 - per_ok\n",
        "        else:\n",
        "            per_ok = per_err = 0\n",
        "\n",
        "        per_ok_s = ['%.2f%%' % (per_ok), '100%'][per_ok == 100]\n",
        "\n",
        "        # text to DEL\n",
        "        text_del.append(oText)\n",
        "\n",
        "        # text to ADD\n",
        "        font_prop = fm.FontProperties(weight='bold', size=fz)\n",
        "        text_kwargs = dict(color='w', ha=\"center\", va=\"center\", gid='sum', fontproperties=font_prop)\n",
        "        lis_txt = ['%d' % (cell_val), per_ok_s, '%.2f%%' % (per_err)]\n",
        "        lis_kwa = [text_kwargs]\n",
        "        dic = text_kwargs.copy();\n",
        "        dic['color'] = 'g';\n",
        "        lis_kwa.append(dic);\n",
        "        dic = text_kwargs.copy();\n",
        "        dic['color'] = 'r';\n",
        "        lis_kwa.append(dic);\n",
        "        lis_pos = [(oText._x, oText._y - 0.3), (oText._x, oText._y), (oText._x, oText._y + 0.3)]\n",
        "        for i in range(len(lis_txt)):\n",
        "            newText = dict(x=lis_pos[i][0], y=lis_pos[i][1], text=lis_txt[i], kw=lis_kwa[i])\n",
        "            # print 'lin: %s, col: %s, newText: %s' %(lin, col, newText)\n",
        "            text_add.append(newText)\n",
        "        # print '\\n'\n",
        "\n",
        "        # set background color for sum cells (last line and last column)\n",
        "        carr = [0.27, 0.30, 0.27, 1.0]\n",
        "        if (col == ccl - 1) and (lin == ccl - 1):\n",
        "            carr = [0.17, 0.20, 0.17, 1.0]\n",
        "        facecolors[posi] = carr\n",
        "\n",
        "    else:\n",
        "        if (per > 0):\n",
        "            txt = '%s\\n%.2f%%' % (cell_val, per)\n",
        "        else:\n",
        "            if (show_null_values == 0):\n",
        "                txt = ''\n",
        "            elif (show_null_values == 1):\n",
        "                txt = '0'\n",
        "            else:\n",
        "                txt = '0\\n0.0%'\n",
        "        oText.set_text(txt)\n",
        "\n",
        "        # main diagonal\n",
        "        if (col == lin):\n",
        "            # set color of the textin the diagonal to white\n",
        "            oText.set_color('w')\n",
        "            # set background color in the diagonal to blue\n",
        "            facecolors[posi] = [0.35, 0.8, 0.55, 1.0]\n",
        "        else:\n",
        "            oText.set_color('r')\n",
        "\n",
        "    return text_add, text_del\n",
        "\n",
        "\n",
        "def insert_totals(df_cm):\n",
        "    \"\"\" insert total column and line (the last ones) \"\"\"\n",
        "    sum_col = []\n",
        "    for c in df_cm.columns:\n",
        "        sum_col.append(df_cm[c].sum())\n",
        "    sum_lin = []\n",
        "    for item_line in df_cm.iterrows():\n",
        "        sum_lin.append(item_line[1].sum())\n",
        "    df_cm['sum_lin'] = sum_lin\n",
        "    sum_col.append(np.sum(sum_lin))\n",
        "    df_cm.loc['sum_col'] = sum_col\n",
        "\n",
        "\n",
        "def pretty_plot_confusion_matrix(df_cm, annot=True, cmap=\"Oranges\", fmt='.2f', fz=11,\n",
        "                                 lw=0.5, cbar=False, figsize=[8, 8], show_null_values=0, pred_val_axis='y'):\n",
        "    \"\"\"\n",
        "      print conf matrix with default layout (like matlab)\n",
        "      params:\n",
        "        df_cm          dataframe (pandas) without totals\n",
        "        annot          print text in each cell\n",
        "        cmap           Oranges,Oranges_r,YlGnBu,Blues,RdBu, ... see:\n",
        "        fz             fontsize\n",
        "        lw             linewidth\n",
        "        pred_val_axis  where to show the prediction values (x or y axis)\n",
        "                        'col' or 'x': show predicted values in columns (x axis) instead lines\n",
        "                        'lin' or 'y': show predicted values in lines   (y axis)\n",
        "    \"\"\"\n",
        "    if (pred_val_axis in ('col', 'x')):\n",
        "        xlbl = 'Predicted'\n",
        "        ylbl = 'Actual'\n",
        "    else:\n",
        "        xlbl = 'Actual'\n",
        "        ylbl = 'Predicted'\n",
        "        df_cm = df_cm.T\n",
        "\n",
        "    # create \"Total\" column\n",
        "    insert_totals(df_cm)\n",
        "\n",
        "    # this is for print allways in the same window\n",
        "    fig, ax1 = get_new_fig('Conf matrix default', figsize)\n",
        "\n",
        "    # thanks for seaborn\n",
        "    ax = sn.heatmap(df_cm, annot=annot, annot_kws={\"size\": fz}, linewidths=lw, ax=ax1,\n",
        "                    cbar=cbar, cmap=cmap, linecolor='w', fmt=fmt)\n",
        "\n",
        "    # set ticklabels rotation\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, fontsize=10)\n",
        "    ax.set_yticklabels(ax.get_yticklabels(), rotation=25, fontsize=10)\n",
        "\n",
        "    # Turn off all the ticks\n",
        "    for t in ax.xaxis.get_major_ticks():\n",
        "        t.tick1On = False\n",
        "        t.tick2On = False\n",
        "    for t in ax.yaxis.get_major_ticks():\n",
        "        t.tick1On = False\n",
        "        t.tick2On = False\n",
        "\n",
        "    # face colors list\n",
        "    quadmesh = ax.findobj(QuadMesh)[0]\n",
        "    facecolors = quadmesh.get_facecolors()\n",
        "\n",
        "    # iter in text elements\n",
        "    array_df = np.array(df_cm.to_records(index=False).tolist())\n",
        "    text_add = [];\n",
        "    text_del = [];\n",
        "    posi = -1  # from left to right, bottom to top.\n",
        "    for t in ax.collections[0].axes.texts:  # ax.texts:\n",
        "        pos = np.array(t.get_position()) - [0.5, 0.5]\n",
        "        lin = int(pos[1]);\n",
        "        col = int(pos[0]);\n",
        "        posi += 1\n",
        "        # print ('>>> pos: %s, posi: %s, val: %s, txt: %s' %(pos, posi, array_df[lin][col], t.get_text()))\n",
        "\n",
        "        # set text\n",
        "        txt_res = configcell_text_and_colors(array_df, lin, col, t, facecolors, posi, fz, fmt, show_null_values)\n",
        "\n",
        "        text_add.extend(txt_res[0])\n",
        "        text_del.extend(txt_res[1])\n",
        "\n",
        "    # remove the old ones\n",
        "    for item in text_del:\n",
        "        item.remove()\n",
        "    # append the new ones\n",
        "    for item in text_add:\n",
        "        ax.text(item['x'], item['y'], item['text'], **item['kw'])\n",
        "\n",
        "    # titles and legends\n",
        "    ax.set_title('Confusion matrix')\n",
        "    ax.set_xlabel(xlbl)\n",
        "    ax.set_ylabel(ylbl)\n",
        "    plt.tight_layout()  # set layout slim\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_confusion_matrix_from_data(y_test, predictions, columns=None, annot=True, cmap=\"Oranges\",\n",
        "                                    fmt='.2f', fz=11, lw=0.5, cbar=False, figsize=[8, 8], show_null_values=0,\n",
        "                                    pred_val_axis='lin'):\n",
        "    \"\"\"\n",
        "        plot confusion matrix function with y_test (actual values) and predictions (predic),\n",
        "        whitout a confusion matrix yet\n",
        "    \"\"\"\n",
        "    from sklearn.metrics import confusion_matrix\n",
        "    from pandas import DataFrame\n",
        "\n",
        "    # data\n",
        "    if (not columns):\n",
        "        from string import ascii_uppercase\n",
        "        columns = ['class %s' % (i) for i in list(ascii_uppercase)[0:len(np.unique(y_test))]]\n",
        "\n",
        "    confm = confusion_matrix(y_test, predictions)\n",
        "    cmap = 'Oranges';\n",
        "    fz = 11;\n",
        "    figsize = [9, 9];\n",
        "    show_null_values = 2\n",
        "    df_cm = DataFrame(confm, index=columns, columns=columns)\n",
        "    pretty_plot_confusion_matrix(df_cm, fz=fz, cmap=cmap, figsize=figsize, show_null_values=show_null_values,\n",
        "                                 pred_val_axis=pred_val_axis)\n",
        "\n",
        "\n",
        "def fil_sent(sent):\n",
        "    \"\"\"\n",
        "    Filter stopwords\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_sentence = ' '.join([w for w in sent.split() if not w in stop_words])\n",
        "    return filtered_sentence\n",
        "\n",
        "\n",
        "def process(sent):\n",
        "    \"\"\"\n",
        "    Apply stemming\n",
        "    \"\"\"\n",
        "    sent = str(sent)\n",
        "    ps = PorterStemmer()\n",
        "    return fil_sent(' '.join([ps.stem(str(x).lower()) for x in word_tokenize(sent)]))\n",
        "\n",
        "\n",
        "def extract_style(text):\n",
        "    \"\"\"\n",
        "    Extracting stylometric features of a text\n",
        "    \"\"\"\n",
        "\n",
        "    text = str(text)\n",
        "    len_text = len(text)\n",
        "    len_words = len(text.split())\n",
        "    avg_len = np.mean([len(t) for t in text.split()])\n",
        "    num_short_w = len([t for t in text.split() if len(t) < 3])\n",
        "    per_digit = sum(t.isdigit() for t in text) / len(text)\n",
        "    per_cap = sum(1 for t in text if t.isupper()) / len(text)\n",
        "    f_a = sum(1 for t in text if t.lower() == \"a\") / len(text)\n",
        "    f_b = sum(1 for t in text if t.lower() == \"b\") / len(text)\n",
        "    f_c = sum(1 for t in text if t.lower() == \"c\") / len(text)\n",
        "    f_d = sum(1 for t in text if t.lower() == \"d\") / len(text)\n",
        "    f_e = sum(1 for t in text if t.lower() == \"e\") / len(text)\n",
        "    f_f = sum(1 for t in text if t.lower() == \"f\") / len(text)\n",
        "    f_g = sum(1 for t in text if t.lower() == \"g\") / len(text)\n",
        "    f_h = sum(1 for t in text if t.lower() == \"h\") / len(text)\n",
        "    f_i = sum(1 for t in text if t.lower() == \"i\") / len(text)\n",
        "    f_j = sum(1 for t in text if t.lower() == \"j\") / len(text)\n",
        "    f_k = sum(1 for t in text if t.lower() == \"k\") / len(text)\n",
        "    f_l = sum(1 for t in text if t.lower() == \"l\") / len(text)\n",
        "    f_m = sum(1 for t in text if t.lower() == \"m\") / len(text)\n",
        "    f_n = sum(1 for t in text if t.lower() == \"n\") / len(text)\n",
        "    f_o = sum(1 for t in text if t.lower() == \"o\") / len(text)\n",
        "    f_p = sum(1 for t in text if t.lower() == \"p\") / len(text)\n",
        "    f_q = sum(1 for t in text if t.lower() == \"q\") / len(text)\n",
        "    f_r = sum(1 for t in text if t.lower() == \"r\") / len(text)\n",
        "    f_s = sum(1 for t in text if t.lower() == \"s\") / len(text)\n",
        "    f_t = sum(1 for t in text if t.lower() == \"t\") / len(text)\n",
        "    f_u = sum(1 for t in text if t.lower() == \"u\") / len(text)\n",
        "    f_v = sum(1 for t in text if t.lower() == \"v\") / len(text)\n",
        "    f_w = sum(1 for t in text if t.lower() == \"w\") / len(text)\n",
        "    f_x = sum(1 for t in text if t.lower() == \"x\") / len(text)\n",
        "    f_y = sum(1 for t in text if t.lower() == \"y\") / len(text)\n",
        "    f_z = sum(1 for t in text if t.lower() == \"z\") / len(text)\n",
        "    f_1 = sum(1 for t in text if t.lower() == \"1\") / len(text)\n",
        "    f_2 = sum(1 for t in text if t.lower() == \"2\") / len(text)\n",
        "    f_3 = sum(1 for t in text if t.lower() == \"3\") / len(text)\n",
        "    f_4 = sum(1 for t in text if t.lower() == \"4\") / len(text)\n",
        "    f_5 = sum(1 for t in text if t.lower() == \"5\") / len(text)\n",
        "    f_6 = sum(1 for t in text if t.lower() == \"6\") / len(text)\n",
        "    f_7 = sum(1 for t in text if t.lower() == \"7\") / len(text)\n",
        "    f_8 = sum(1 for t in text if t.lower() == \"8\") / len(text)\n",
        "    f_9 = sum(1 for t in text if t.lower() == \"9\") / len(text)\n",
        "    f_0 = sum(1 for t in text if t.lower() == \"0\") / len(text)\n",
        "    f_e_0 = sum(1 for t in text if t.lower() == \"!\") / len(text)\n",
        "    f_e_1 = sum(1 for t in text if t.lower() == \"-\") / len(text)\n",
        "    f_e_2 = sum(1 for t in text if t.lower() == \":\") / len(text)\n",
        "    f_e_3 = sum(1 for t in text if t.lower() == \"?\") / len(text)\n",
        "    f_e_4 = sum(1 for t in text if t.lower() == \".\") / len(text)\n",
        "    f_e_5 = sum(1 for t in text if t.lower() == \",\") / len(text)\n",
        "    f_e_6 = sum(1 for t in text if t.lower() == \";\") / len(text)\n",
        "    f_e_7 = sum(1 for t in text if t.lower() == \"'\") / len(text)\n",
        "    f_e_8 = sum(1 for t in text if t.lower() == \"/\") / len(text)\n",
        "    f_e_9 = sum(1 for t in text if t.lower() == \"(\") / len(text)\n",
        "    f_e_10 = sum(1 for t in text if t.lower() == \")\") / len(text)\n",
        "    f_e_11 = sum(1 for t in text if t.lower() == \"&\") / len(text)\n",
        "    richness = len(list(set(text.split()))) / len(text.split())\n",
        "\n",
        "    return pd.Series(\n",
        "        [avg_len, len_text, len_words, num_short_w, per_digit, per_cap, f_a, f_b, f_c, f_d, f_e, f_f, f_g, f_h, f_i,\n",
        "         f_j, f_k, f_l, f_m, f_n, f_o, f_p, f_q, f_r, f_s, f_t, f_u, f_v, f_w, f_x, f_y, f_z, f_0, f_1, f_2, f_3,\n",
        "         f_4, f_5, f_6, f_7, f_8, f_9, f_e_0, f_e_1, f_e_2, f_e_3, f_e_4, f_e_5, f_e_6, f_e_7, f_e_8, f_e_9, f_e_10,\n",
        "         f_e_11, richness])\n",
        "\n",
        "\n",
        "def build_train_test(df, source, limit, per_author=None, seed=None):\n",
        "    # Select top N senders and build Train and Test\n",
        "    # list_spk = list(pd.DataFrame(df['From'].value_counts().iloc[:limit]).reset_index()['index'])\n",
        "    list_spk = list(pd.DataFrame(df['From'].value_counts().iloc[:limit]).reset_index().iloc[:, 0])\n",
        "    sub_df = df[df['From'].isin(list_spk)]\n",
        "\n",
        "    if per_author is not None:\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    if source == 'turing':\n",
        "        sub_df = sub_df[\n",
        "            [\n",
        "                'From', 'content', 'content_tfidf', \"avg_len\", \"len_text\", \"len_words\", \"num_short_w\", \"per_digit\",\n",
        "                \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\", \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \"f_k\", \"f_l\", \"f_m\",\n",
        "                \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \"f_s\", \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \"f_1\",\n",
        "                \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \"f_7\", \"f_8\", \"f_9\", \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \"f_e_4\",\n",
        "                \"f_e_5\", \"f_e_6\", \"f_e_7\", \"f_e_8\", \"f_e_9\", \"f_e_10\", \"f_e_11\", \"richness\", \"train\"\n",
        "            ]\n",
        "        ]\n",
        "    else:\n",
        "        sub_df = sub_df[\n",
        "            [\n",
        "                'From', 'content', 'content_tfidf', \"avg_len\", \"len_text\", \"len_words\", \"num_short_w\", \"per_digit\",\n",
        "                \"per_cap\", \"f_a\", \"f_b\", \"f_c\", \"f_d\", \"f_e\", \"f_f\", \"f_g\", \"f_h\", \"f_i\", \"f_j\", \"f_k\", \"f_l\", \"f_m\",\n",
        "                \"f_n\", \"f_o\", \"f_p\", \"f_q\", \"f_r\", \"f_s\", \"f_t\", \"f_u\", \"f_v\", \"f_w\", \"f_x\", \"f_y\", \"f_z\", \"f_0\", \"f_1\",\n",
        "                \"f_2\", \"f_3\", \"f_4\", \"f_5\", \"f_6\", \"f_7\", \"f_8\", \"f_9\", \"f_e_0\", \"f_e_1\", \"f_e_2\", \"f_e_3\", \"f_e_4\",\n",
        "                \"f_e_5\", \"f_e_6\", \"f_e_7\", \"f_e_8\", \"f_e_9\", \"f_e_10\", \"f_e_11\", \"richness\"\n",
        "            ]\n",
        "        ]\n",
        "    sub_df = sub_df.dropna()\n",
        "\n",
        "    print(\"Number of texts : \", len(sub_df))\n",
        "\n",
        "    dict_nlp_enron = {}\n",
        "    k = 0\n",
        "\n",
        "    for val in np.unique(sub_df.From):\n",
        "        dict_nlp_enron[val] = k\n",
        "        k += 1\n",
        "\n",
        "    sub_df['Target'] = sub_df['From'].apply(lambda x: dict_nlp_enron[x])\n",
        "\n",
        "    if source == 'turing':\n",
        "        perc = 0.5\n",
        "        print(\"Percentage: \" + str(perc))\n",
        "        full_train = sub_df[sub_df[\"train\"] == 1]\n",
        "        nlp_train = full_train[['content', 'Target']]\n",
        "\n",
        "        full_test = sub_df[sub_df[\"train\"] == 0]\n",
        "        test_dict = full_test[['content', 'Target']]\n",
        "\n",
        "        full_valid = sub_df[sub_df[\"train\"] == 2]\n",
        "        val_dict = full_valid[['content', 'Target']]\n",
        "\n",
        "        shrinked_train = nlp_train\n",
        "        shrinked_test = test_dict\n",
        "        shrinked_val = val_dict\n",
        "        for l in range(20):\n",
        "            part_train = nlp_train[nlp_train[\"Target\"] == l]\n",
        "            part_train = part_train[:int(len(part_train) * perc)]\n",
        "            part_test = test_dict[test_dict[\"Target\"] == l]\n",
        "            part_test = part_test[:int(len(part_test) * perc)]\n",
        "            part_val = val_dict[val_dict[\"Target\"] == l]\n",
        "            part_val = part_val[:int(len(part_val) * perc)]\n",
        "            if l == 0:\n",
        "                shrinked_train = part_train\n",
        "                shrinked_test = part_test\n",
        "                shrinked_val = part_val\n",
        "            else:\n",
        "                shrinked_train = pd.concat([shrinked_train, part_train], axis=0)\n",
        "                shrinked_test = pd.concat([shrinked_test, part_test], axis=0)\n",
        "                shrinked_val = pd.concat([shrinked_val, part_val], axis=0)\n",
        "\n",
        "        return shrinked_train, shrinked_test, shrinked_val\n",
        "\n",
        "    if 'blog' in source or 'imdb62' in source:\n",
        "        perc = 0.75\n",
        "        print(\"seed: \" + str(seed))\n",
        "\n",
        "        if seed is None:\n",
        "            seed = 0\n",
        "\n",
        "        # ind = train_test_split(sub_df[['content', 'Target']], test_size=0.2, stratify=sub_df['Target'],\n",
        "        #                        random_state=seed)\n",
        "        ind = train_test_split(sub_df, test_size=0.2, stratify=sub_df['Target'],\n",
        "                        random_state=seed)\n",
        "        ind_train = list(ind[0].index)\n",
        "        nlp_train = sub_df.loc[ind_train]\n",
        "\n",
        "        val_test_sub_df = ind[1]\n",
        "        # ind2 = train_test_split(val_test_sub_df[['content', 'Target']], test_size=0.5,\n",
        "        #                         stratify=val_test_sub_df['Target'], random_state=seed)\n",
        "        ind2 = train_test_split(val_test_sub_df, test_size=0.5,\n",
        "                        stratify=val_test_sub_df['Target'], random_state=seed)\n",
        "        ind_val = list(ind2[0].index)\n",
        "        ind_test = list(ind2[1].index)\n",
        "        val_dict = val_test_sub_df.loc[ind_val]\n",
        "        test_dict = val_test_sub_df.loc[ind_test]\n",
        "\n",
        "        if 'blog' in source:\n",
        "            shrinked_train = nlp_train\n",
        "            shrinked_test = test_dict\n",
        "            shrinked_val = val_dict\n",
        "            for l in range(50):\n",
        "                part_train = nlp_train[nlp_train[\"Target\"] == l]\n",
        "                part_train = part_train[:int(len(part_train) * perc)]\n",
        "                part_test = test_dict[test_dict[\"Target\"] == l]\n",
        "                part_test = part_test[:int(len(part_test) * perc)]\n",
        "                part_val = val_dict[val_dict[\"Target\"] == l]\n",
        "                part_val = part_val[:int(len(part_val) * perc)]\n",
        "                if l == 0:\n",
        "                    shrinked_train = part_train\n",
        "                    shrinked_test = part_test\n",
        "                    shrinked_val = part_val\n",
        "                else:\n",
        "                    shrinked_train = pd.concat([shrinked_train, part_train], axis=0)\n",
        "                    shrinked_test = pd.concat([shrinked_test, part_test], axis=0)\n",
        "                    shrinked_val = pd.concat([shrinked_val, part_val], axis=0)\n",
        "\n",
        "            return shrinked_train, shrinked_test, shrinked_val\n",
        "\n",
        "        return nlp_train, val_dict, test_dict\n",
        "\n",
        "    ind = train_test_split(sub_df[['content', 'Target']], test_size=0.2, stratify=sub_df['Target'], random_state=seed)\n",
        "    ind_train = list(ind[0].index)\n",
        "    ind_test = list(ind[1].index)\n",
        "    nlp_train = sub_df.loc[ind_train]\n",
        "    test_dict = sub_df.loc[ind_test]\n",
        "\n",
        "    return nlp_train, test_dict\n",
        "\n",
        "\n",
        "def is_name_in_email(name, email):\n",
        "    \"\"\"\n",
        "    Removing emails from Enron where name is in email\n",
        "    \"\"\"\n",
        "\n",
        "    if str(name).lower() in str(email).lower():\n",
        "        return 1\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "\n",
        "def load_dataset_dataframe(source):\n",
        "    print(\"Loading and processing dataframe\")\n",
        "\n",
        "    # dataset_path = \"datasets\"\n",
        "    # dataset_file_name = {\n",
        "    #     \"imdb62\": 'full_imdb62.csv',\n",
        "    #     \"blog\": 'full_blog.csv',\n",
        "    #     \"turing\": \"turing_ori_0208.csv\"\n",
        "    # }\n",
        "\n",
        "    df = None\n",
        "    if source == \"imdb62\":\n",
        "        df = pd.read_csv(os.path.join(dataset_path, dataset_file_name[source]), index_col=0)\n",
        "    elif source == \"blog\":\n",
        "        df = pd.read_csv(os.path.join(dataset_path, dataset_file_name[source]))\n",
        "    elif source == 'diffusiondb':\n",
        "        df = pd.read_csv(os.path.join(dataset_path, dataset_file_name[source]))\n",
        "        df = df[['prompt', 'user_name']]\n",
        "        df.columns = ['content', 'Target']\n",
        "    else:\n",
        "        df = pd.read_csv(os.path.join(dataset_path, dataset_file_name[source]))\n",
        "        df.sort_values(by=['train', 'From'], inplace=True, ascending=[False, True])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def save_model(ckpt_dir, cp_name, model):\n",
        "    \"\"\"\n",
        "    Create directory /Checkpoint under exp_data_path and save encoder as cp_name\n",
        "    \"\"\"\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    saving_model_path = os.path.join(ckpt_dir, cp_name)\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        model = model.module  # convert to non-parallel form\n",
        "    torch.save(model.state_dict(), saving_model_path)\n",
        "    print(f'Model saved: {saving_model_path}')\n",
        "\n",
        "\n",
        "def load_model_dic(model, ckpt_path, verbose=True, strict=True):\n",
        "    \"\"\"\n",
        "    Load weights to model and take care of weight parallelism\n",
        "    \"\"\"\n",
        "    assert os.path.exists(ckpt_path), f\"trained model {ckpt_path} does not exist\"\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(ckpt_path), strict=strict)\n",
        "    except:\n",
        "        state_dict = torch.load(ckpt_path)\n",
        "        state_dict = {k.partition('module.')[2]: state_dict[k] for k in state_dict.keys()}\n",
        "        model.load_state_dict(state_dict, strict=strict)\n",
        "    if verbose:\n",
        "        print(f'Model loaded: {ckpt_path}')\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_UzVYFbrBs0"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTxovZR8r0LK"
      },
      "outputs": [],
      "source": [
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        super().__init__()\n",
        "        self.x = torch.from_numpy(x).float()\n",
        "        self.y = torch.from_numpy(y).float()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "\n",
        "class BertDataset(Dataset):\n",
        "    def __init__(self, x, y, tokenizer, length=128, return_idx=False):\n",
        "        super(BertDataset, self).__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.length = length\n",
        "        self.x = x\n",
        "        self.return_idx = return_idx\n",
        "        self.y = torch.tensor(y)\n",
        "        self.tokens_cache = {}\n",
        "\n",
        "    def tokenize(self, x):\n",
        "        dic = self.tokenizer.batch_encode_plus(\n",
        "            [x],  # input must be a list\n",
        "            max_length=self.length,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return [x[0] for x in dic.values()]  # get rid of the first dim\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        int_idx = int(idx)\n",
        "        assert idx == int_idx\n",
        "        idx = int_idx\n",
        "        if idx not in self.tokens_cache:\n",
        "            self.tokens_cache[idx] = self.tokenize(self.x[idx])\n",
        "        input_ids, token_type_ids, attention_mask = self.tokens_cache[idx]\n",
        "        if self.return_idx:\n",
        "            return input_ids, token_type_ids, attention_mask, self.y[idx], idx, self.x[idx]\n",
        "        return input_ids, token_type_ids, attention_mask, self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "\n",
        "class TrainSampler(Sampler):\n",
        "    def __init__(self, dataset, batch_size, sim_ratio=0.5):\n",
        "        super().__init__(None)\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.x = dataset.x\n",
        "        self.y = dataset.y\n",
        "        self.sim_ratio = sim_ratio\n",
        "        self.num_pos_samples = int(batch_size * sim_ratio)\n",
        "        print(f'train sampler with batch size = {batch_size} and postive sample ratio = {sim_ratio}')\n",
        "\n",
        "        self.length = len(list(self.__iter__()))\n",
        "\n",
        "    def __iter__(self):\n",
        "        indices = list(range(len(self.y)))\n",
        "        label_cluster = {}\n",
        "        for i in indices:\n",
        "            label = self.y[i].item()\n",
        "            if label not in label_cluster:\n",
        "                label_cluster[label] = []\n",
        "            label_cluster[label].append(i)\n",
        "        for key, value in label_cluster.items():\n",
        "            random.shuffle(value)\n",
        "\n",
        "        assert len(label_cluster[0]) > self.num_pos_samples, \\\n",
        "            f\"only {len(label_cluster[0])} samples in each class, but {self.num_pos_samples} pos samples needed\"\n",
        "\n",
        "        # too time-consuming, i.e., O(|D||C|/|B|)s\n",
        "        batch_indices = []\n",
        "        flag = True\n",
        "        while flag:\n",
        "            # find a valid positive sample class\n",
        "            available_classes = list(filter(lambda x: len(label_cluster[x]) >= self.num_pos_samples,\n",
        "                                            list(range(max(self.y) + 1))))\n",
        "            if len(available_classes) == 0:\n",
        "                break\n",
        "            class_count = random.choice(available_classes)\n",
        "\n",
        "            # fill in positive samples\n",
        "            batch_indices.append(label_cluster[class_count][-self.num_pos_samples:])\n",
        "            del label_cluster[class_count][-self.num_pos_samples:]\n",
        "\n",
        "            # fill in negative samples\n",
        "            for i in range(self.batch_size - self.num_pos_samples):\n",
        "                available_classes = list(filter(lambda x: len(label_cluster[x]) > 0, list(range(max(self.y) + 1))))\n",
        "                if class_count in available_classes:\n",
        "                    available_classes.remove(class_count)\n",
        "                if len(available_classes) == 0:\n",
        "                    flag = False\n",
        "                    break\n",
        "                rand_class = random.choice(available_classes)\n",
        "                batch_indices[-1].append(label_cluster[rand_class].pop())\n",
        "\n",
        "            random.shuffle(batch_indices[-1])\n",
        "\n",
        "        random.shuffle(batch_indices)\n",
        "        all = sum(batch_indices, [])\n",
        "\n",
        "        return iter(all)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "\n",
        "class TrainSamplerMultiClass(Sampler):\n",
        "    def __init__(self, dataset, batch_size, num_classes, samples_per_author):\n",
        "        super().__init__(None)\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.x = dataset.x\n",
        "        self.y = dataset.y\n",
        "        self.num_classes = num_classes\n",
        "        self.samples_per_author = samples_per_author\n",
        "        assert batch_size // num_classes * num_classes == batch_size, \\\n",
        "            f'batch size {batch_size} is not a multiple of num of classes {num_classes}'\n",
        "        print(f'train sampler with batch size = {batch_size} and {num_classes} classes in a batch')\n",
        "        self.length = len(list(self.__iter__()))\n",
        "\n",
        "    def __iter__(self):\n",
        "        indices = list(range(len(self.y)))\n",
        "        label_cluster = {}\n",
        "        for i in indices:\n",
        "            label = self.y[i].item()\n",
        "            if label not in label_cluster:\n",
        "                label_cluster[label] = []\n",
        "            label_cluster[label].append(i)\n",
        "\n",
        "        assert len(label_cluster) > self.num_classes, \\\n",
        "            f'number of available classes {label_cluster} < required classes {self.num_classes}'\n",
        "\n",
        "        num_samples_per_class_batch = self.batch_size // self.num_classes\n",
        "        min_class_samples = min([len(x) for x in label_cluster.values()])\n",
        "        assert min_class_samples > self.samples_per_author, \\\n",
        "            f\"expected {self.samples_per_author} per author, but got {min_class_samples} in the dataset\"\n",
        "        class_samples_needed = self.samples_per_author // num_samples_per_class_batch * num_samples_per_class_batch\n",
        "\n",
        "        dataset_matrix = []\n",
        "        for key, value in label_cluster.items():\n",
        "            random.shuffle(value)\n",
        "            # value = [key] * len(value)    # debugging use\n",
        "            dataset_matrix.append(torch.tensor(value[:class_samples_needed]).view(num_samples_per_class_batch, -1))\n",
        "\n",
        "        tuples = torch.cat(dataset_matrix, dim=1).transpose(1, 0).split(1, dim=0)\n",
        "        tuples = [x.flatten().tolist() for x in tuples]\n",
        "        random.shuffle(tuples)\n",
        "        all = sum(tuples, [])\n",
        "\n",
        "        print(f'from dataset sampler: batch size {self.batch_size}, num of classes in a batch {self.num_classes}, '\n",
        "              f'num of samples per author in total {self.samples_per_author} (specified) / {class_samples_needed} (true).'\n",
        "              f'dataset size {len(all)}')\n",
        "\n",
        "        return iter(all)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "\n",
        "class TrainSamplerMultiClassUnit(Sampler):\n",
        "    def __init__(self, dataset, sample_unit_size):\n",
        "        super().__init__(None)\n",
        "        self.x = dataset.x\n",
        "        self.y = dataset.y\n",
        "        self.sample_unit_size = sample_unit_size\n",
        "        print(f'train sampler with sample unit size {sample_unit_size}')\n",
        "        self.length = len(list(self.__iter__()))\n",
        "\n",
        "    def __iter__(self):\n",
        "        indices = list(range(len(self.y)))\n",
        "        label_cluster = {}\n",
        "        for i in indices:\n",
        "            label = self.y[i].item()\n",
        "            if label not in label_cluster:\n",
        "                label_cluster[label] = []\n",
        "            label_cluster[label].append(i)\n",
        "\n",
        "        dataset_matrix = []\n",
        "        for key, value in label_cluster.items():\n",
        "            random.shuffle(value)\n",
        "            num_valid_samples = len(value) // self.sample_unit_size * self.sample_unit_size\n",
        "            dataset_matrix.append(torch.tensor(value[:num_valid_samples]).view(self.sample_unit_size, -1))\n",
        "\n",
        "        tuples = torch.cat(dataset_matrix, dim=1).transpose(1, 0).split(1, dim=0)\n",
        "        # print(torch.cat(dataset_matrix, dim=1).transpose(1, 0).shape)\n",
        "        # print(len(tuples))\n",
        "        tuples = [x.flatten().tolist() for x in tuples]\n",
        "        # print(len(tuples))\n",
        "        random.shuffle(tuples)\n",
        "        all = sum(tuples, [])\n",
        "\n",
        "        print(f'from dataset sampler: original dataset size {len(self.y)}, resampled dataset size {len(all)}. '\n",
        "              f'sample unit size {self.sample_unit_size}')\n",
        "\n",
        "        return iter(all)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "\n",
        "class EnsembleDataset(Dataset):\n",
        "    def __init__(self, x_style, x_char, x_bert, y):\n",
        "        super(EnsembleDataset, self).__init__()\n",
        "        self.x_style = x_style\n",
        "        self.x_char = x_char\n",
        "        self.x_bert = x_bert\n",
        "        self.y = y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x_style[idx], self.x_char[idx], torch.tensor(self.x_bert['input_ids'][idx]), \\\n",
        "               torch.tensor(self.x_bert['attention_mask'][idx]), self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "\n",
        "class TransformerEnsembleDataset(Dataset):\n",
        "    def __init__(self, x, y, tokenizers, lengths):\n",
        "        super(TransformerEnsembleDataset, self).__init__()\n",
        "        self.x = x\n",
        "        self.tokenizers = tokenizers\n",
        "        self.lengths = lengths\n",
        "        self.caches = [{} for i in range(len(tokenizers))]\n",
        "        self.y = torch.tensor(y)\n",
        "\n",
        "    def tokenize(self, x, i):\n",
        "        dic = self.tokenizers[i].batch_encode_plus(\n",
        "            batch_text_or_text_pairs=[x],  # input must be a list\n",
        "            max_length=self.lengths[i],\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_token_type_ids=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        return [x[0] for x in dic.values()]  # get rid of the first dim\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if idx not in self.caches[0]:\n",
        "            for i in range(len(self.tokenizers)):\n",
        "                self.caches[i][idx] = self.tokenize(self.x[idx], i)\n",
        "\n",
        "        return [self.caches[i][idx] for i in range(len(self.tokenizers))], self.y[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpW7_8AtrpS6"
      },
      "source": [
        "# Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NsMeKSo0rr_i"
      },
      "outputs": [],
      "source": [
        "def compute_sim_matrix(feats):\n",
        "    \"\"\"\n",
        "    Takes in a batch of features of size (bs, feat_len).\n",
        "    \"\"\"\n",
        "    sim_matrix = F.cosine_similarity(feats.unsqueeze(2).expand(-1, -1, feats.size(0)),\n",
        "                                     feats.unsqueeze(2).expand(-1, -1, feats.size(0)).transpose(0, 2),\n",
        "                                     dim=1)\n",
        "\n",
        "    return sim_matrix\n",
        "\n",
        "\n",
        "def compute_target_matrix(labels):\n",
        "    \"\"\"\n",
        "    Takes in a label vector of size (bs)\n",
        "    \"\"\"\n",
        "    label_matrix = labels.unsqueeze(-1).expand((labels.shape[0], labels.shape[0]))\n",
        "    trans_label_matrix = torch.transpose(label_matrix, 0, 1)\n",
        "    target_matrix = (label_matrix == trans_label_matrix).type(torch.float)\n",
        "\n",
        "    return target_matrix\n",
        "\n",
        "\n",
        "def contrastive_loss(pred_sim_matrix, target_matrix, temperature, labels):\n",
        "    return F.kl_div(F.softmax(pred_sim_matrix / temperature).log(), F.softmax(target_matrix / temperature),\n",
        "                    reduction=\"batchmean\", log_target=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9AzA-KfsFEj"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3vD0cCasHp-"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0):\n",
        "        super().__init__()\n",
        "        print(f'Logistic Regression classifier of dim ({in_dim} {hid_dim} {out_dim})')\n",
        "\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_dim, hid_dim, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hid_dim, out_dim, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_feat=False):\n",
        "        out = self.nn(x)\n",
        "        if return_feat:\n",
        "            return out, x\n",
        "        return out\n",
        "\n",
        "\n",
        "class BertClassifier(nn.Module):\n",
        "    FEAT_LEN = 768\n",
        "\n",
        "    def __init__(self, raw_bert, classifier):\n",
        "        super().__init__()\n",
        "        self.bert = raw_bert\n",
        "        self.fc = classifier\n",
        "\n",
        "    def forward(self, x, return_feat=False):\n",
        "        # x is a tokenized input\n",
        "        # feature = self.bert(input_ids=x[0], token_type_ids=x[1], attention_mask=x[2])\n",
        "        feature = self.bert(input_ids=x[0], attention_mask=x[2])\n",
        "        # out = self.fc(feature.pooler_output.flatten(1))       # not good for our task     # (BS, E)\n",
        "        out = self.fc(feature.last_hidden_state.flatten(1))  # (BS, T, E)\n",
        "        if return_feat:\n",
        "            return out, feature.last_hidden_state.flatten(1)\n",
        "        return out\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class BertClassiferHyperparams:\n",
        "    mlp_size: int\n",
        "    token_len: int\n",
        "    embed_len: int\n",
        "\n",
        "\n",
        "class SimpleEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    The simplest ensemble model, ie, averaging\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, components):  # components is a list of models\n",
        "        super(SimpleEnsemble, self).__init__()\n",
        "        self.components = components\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        assert len(self.components) == len(inputs)\n",
        "        preds = []\n",
        "        for model, input in zip(self.components, inputs):\n",
        "            preds.append(model(input))\n",
        "        return sum(preds) / len(preds)\n",
        "\n",
        "\n",
        "class FixedWeightEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    Learn a fixed set of weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, components):\n",
        "        super(FixedWeightEnsemble, self).__init__()\n",
        "        self.components = components\n",
        "        self.weights = nn.Linear(1, len(components), bias=False)\n",
        "        self.weightsInput = torch.tensor([1], dtype=torch.float).cuda()\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        assert len(self.components) == len(inputs)\n",
        "\n",
        "        preds = []\n",
        "        for model, input in zip(self.components, inputs):\n",
        "            pred = model(input)\n",
        "            preds.append(pred)\n",
        "\n",
        "        weights = self.weights(self.weightsInput)\n",
        "        for i, weight in enumerate(weights):\n",
        "            preds[i] = preds[i] * weight\n",
        "\n",
        "        return sum(preds)\n",
        "\n",
        "\n",
        "class DynamicWeightEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    Learn the dynamic weights for different components\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, components, total_feat_len, dropout=0.2, hidden_len=256):\n",
        "        super(DynamicWeightEnsemble, self).__init__()\n",
        "        self.components = components\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(total_feat_len, hidden_len, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_len, len(components), bias=True),\n",
        "            nn.Softmax()\n",
        "        )\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        assert len(self.components) == len(inputs)\n",
        "\n",
        "        preds, feats = [], []\n",
        "        for model, input in zip(self.components, inputs):\n",
        "            pred, feat = model(input, return_feat=True)\n",
        "            preds.append(pred)\n",
        "            feats.append(feat)\n",
        "\n",
        "        weights = self.attention(torch.cat(feats, dim=1))\n",
        "        weights = torch.transpose(weights, 0, 1)\n",
        "        for i in range(weights.size(0)):\n",
        "            for j in range(weights.size(1)):\n",
        "                preds[i][j] *= weights[i][j]\n",
        "\n",
        "        return sum(preds)\n",
        "\n",
        "\n",
        "class AggregateFeatEnsemble(nn.Module):\n",
        "    \"\"\"\n",
        "    Learn the dynamic weights for different components\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, components, total_feat_len, num_classes, dropout=0.2, hidden_len=256):\n",
        "        super(AggregateFeatEnsemble, self).__init__()\n",
        "        self.components = nn.ModuleList(components)\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(total_feat_len, hidden_len, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_len, num_classes, bias=True)\n",
        "        )\n",
        "        #         self.nn2 = nn.Sequential(\n",
        "        #             nn.Dropout(dropout),\n",
        "        #             nn.Linear(total_feat_len, hidden_len, bias=True),\n",
        "        #             nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "        #             nn.Dropout(dropout),\n",
        "        #             nn.Linear(hidden_len, num_classes, bias=True)\n",
        "        #         )\n",
        "        print(f'aggregate feat ensemble, input feat len {total_feat_len}, hidden size {hidden_len}')\n",
        "\n",
        "    def forward(self, inputs, return_feats=False, return_preds=False):\n",
        "        assert len(self.components) == len(inputs)\n",
        "\n",
        "        preds, feats = [], []\n",
        "        for model, input in zip(self.components, inputs):\n",
        "            pred, feat = model(input, return_feat=True)\n",
        "            preds.append(pred)\n",
        "            feats.append(feat)\n",
        "\n",
        "        #         hidden_feat = self.nn(torch.cat(feats, dim=1))\n",
        "        #         pred = self.nn2(hidden_feat)\n",
        "        pred = self.nn(torch.cat(feats, dim=1))\n",
        "\n",
        "        out = [pred]\n",
        "        if return_feats:\n",
        "            out.append(feats)\n",
        "        if return_preds:\n",
        "            out.append(preds)\n",
        "        if len(out) == 1:\n",
        "            return out[0]\n",
        "        else:\n",
        "            return out\n",
        "\n",
        "    # def forward(self, feats):\n",
        "    #     return self.nn(feats)\n",
        "\n",
        "\n",
        "class EnsembleClassifier(nn.Module):\n",
        "    FEAT_LEN = 768\n",
        "\n",
        "    def __init__(self, raw_bert, styleClassifier, charClassifier, bertClassifier, finalClassifier):\n",
        "        super().__init__()\n",
        "        self.bert = raw_bert\n",
        "        self.styleClassifier = styleClassifier\n",
        "        self.charClassifier = charClassifier\n",
        "        self.bertClassifier = bertClassifier\n",
        "        self.finalClassifier = finalClassifier\n",
        "\n",
        "    def forward(self, x, return_feat=False):\n",
        "        # x is a tokenized input\n",
        "        # print(\"ENS Forward\")\n",
        "\n",
        "        stylePred = self.styleClassifier(x[0])\n",
        "\n",
        "        charPred = self.charClassifier(x[1])\n",
        "\n",
        "        bertFeature = self.bert(x[2], x[3]).last_hidden_state.flatten(1)\n",
        "        bertPred = self.bertClassifier(bertFeature)\n",
        "        # print(stylePred.shape)\n",
        "        # print(charPred.shape)\n",
        "        # print(bertFeature.shape)\n",
        "        # print(bertPred.shape)\n",
        "        # print(x[0].shape)\n",
        "        # print(x[1].shape)\n",
        "        ensembleTensor = torch.cat((stylePred, charPred, bertPred, x[0], x[1], bertFeature), dim=1)\n",
        "        # out = self.fc(feature.pooler_output.flatten(1))\n",
        "        out = self.finalClassifier(ensembleTensor)\n",
        "        if return_feat:\n",
        "            return out, bertFeature\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jkIU-yvqGgS"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VaZA-6L5qH-6"
      },
      "outputs": [],
      "source": [
        "def train_bert(train_dict, test_dic, tqdm_on, model_name, embed_len, id, num_epochs, base_bs, base_lr,\n",
        "               mask_classes, coefficient, num_authors, val_dic=None):\n",
        "    print(f'mask classes = {mask_classes}')\n",
        "\n",
        "    # tokenizer and pretrained model\n",
        "    tokenizer, extractor = None, None\n",
        "    if 'bert-base' in model_name:\n",
        "        from transformers import BertTokenizer, BertModel\n",
        "        tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "        extractor = BertModel.from_pretrained(model_name)\n",
        "    elif 'deberta' in model_name:\n",
        "        from transformers import DebertaTokenizer, DebertaModel\n",
        "        tokenizer = DebertaTokenizer.from_pretrained(model_name)\n",
        "        extractor = DebertaModel.from_pretrained(model_name)\n",
        "    else:\n",
        "        raise NotImplementedError(f\"model {model_name} not implemented\")\n",
        "\n",
        "    # update extractor\n",
        "    for param in extractor.parameters():\n",
        "        param.requires_grad = True\n",
        "\n",
        "    # get dataset\n",
        "    train_x, train_y = train_dict['content'].tolist(), train_dict['Target'].tolist()\n",
        "    test_x, test_y = test_dic['content'].tolist(), test_dic['Target'].tolist()\n",
        "\n",
        "    if val_dic is not None:\n",
        "        val_x, val_y = val_dic['content'].tolist(), val_dic['Target'].tolist()\n",
        "\n",
        "    # training config\n",
        "    ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "    num_tokens, hidden_dim, out_dim = 256, 512, num_authors\n",
        "    model = BertClassifier(extractor, LogisticRegression(embed_len * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "    model = nn.DataParallel(model).cuda()\n",
        "\n",
        "    optimizer = torch.optim.AdamW(params=model.parameters(), lr=base_lr * ngpus, weight_decay=3e-4)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    train_set = BertDataset(train_x, train_y, tokenizer, num_tokens)\n",
        "    test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "\n",
        "    if val_dic is not None:\n",
        "        val_set = BertDataset(val_x, val_y, tokenizer, num_tokens)\n",
        "\n",
        "    temperature, sample_unit_size = 0.1, 6\n",
        "    print(f'coefficient, temperature, sample_unit_size = {coefficient, temperature, sample_unit_size}')\n",
        "\n",
        "    # logger\n",
        "    exp_dir = os.path.join(ckpt_dir,\n",
        "                           f'{id}_{model_name.split(\"/\")[-1]}_coe{coefficient}_temp{temperature}_unit{sample_unit_size}_epoch{num_epochs}')\n",
        "    writer = SummaryWriter(os.path.join(exp_dir, 'board'))\n",
        "\n",
        "    # load data\n",
        "    train_sampler = TrainSamplerMultiClassUnit(train_set, sample_unit_size=sample_unit_size)\n",
        "    train_loader = DataLoader(train_set, batch_size=base_bs * ngpus, sampler=train_sampler, shuffle=False,\n",
        "                              num_workers=4 * ngpus, pin_memory=True, drop_last=True)\n",
        "    test_loader = DataLoader(test_set, batch_size=base_bs * ngpus, shuffle=False, num_workers=4 * ngpus,\n",
        "                             pin_memory=True, drop_last=True)\n",
        "\n",
        "    if val_dic is not None:\n",
        "        val_loader = DataLoader(val_set, batch_size=base_bs * ngpus, shuffle=False, num_workers=4 * ngpus,\n",
        "                                pin_memory=True, drop_last=True)\n",
        "\n",
        "    final_test_acc = None\n",
        "    final_train_preds, final_test_preds = [], []\n",
        "    best_acc = -1\n",
        "    best_tv_acc = -1\n",
        "\n",
        "    # training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        train_acc = AverageMeter()\n",
        "        train_loss = AverageMeter()\n",
        "        train_loss_1 = AverageMeter()\n",
        "        train_loss_2 = AverageMeter()\n",
        "\n",
        "        # decay coefficient\n",
        "        # coefficient = coefficient - 1 / num_epochs\n",
        "\n",
        "        # training\n",
        "        model.train()\n",
        "        pg = tqdm(train_loader, leave=False, total=len(train_loader), disable=not tqdm_on)\n",
        "        for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "            x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "            # print(x[0].shape)\n",
        "            pred, feats = model(x, return_feat=True)\n",
        "\n",
        "            # classification loss\n",
        "            loss_1 = criterion(pred, y.long())\n",
        "\n",
        "            # generate the mask\n",
        "            mask = y.clone().cpu().apply_(lambda x: x not in mask_classes).type(torch.bool).cuda()\n",
        "            feats, pred, y = feats[mask], pred[mask], y[mask]\n",
        "            if len(y) == 0:\n",
        "                continue\n",
        "\n",
        "            # contrastive learning\n",
        "            sim_matrix = compute_sim_matrix(feats)\n",
        "            target_matrix = compute_target_matrix(y)\n",
        "            loss_2 = contrastive_loss(sim_matrix, target_matrix, temperature, y)\n",
        "\n",
        "            # total loss\n",
        "            # loss = loss_1 + coefficient * loss_2\n",
        "            loss = loss_1\n",
        "\n",
        "            acc = (pred.argmax(1) == y).sum().item() / len(y)\n",
        "            train_acc.update(acc)\n",
        "            train_loss.update(loss.item())\n",
        "            train_loss_1.update(loss_1.item())\n",
        "            train_loss_2.update(loss_2.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            pg.set_postfix({\n",
        "                'train acc': '{:.6f}'.format(train_acc.avg),\n",
        "                'train L1': '{:.6f}'.format(train_loss_1.avg),\n",
        "                'train L2': '{:.6f}'.format(train_loss_2.avg),\n",
        "                'train L': '{:.6f}'.format(train_loss.avg),\n",
        "                'epoch': '{:03d}'.format(epoch)\n",
        "            })\n",
        "\n",
        "            # iteration logger\n",
        "            step = i + epoch * len(pg)\n",
        "            writer.add_scalar(\"train-iteration/L1\", loss_1.item(), step)\n",
        "            writer.add_scalar(\"train-iteration/L2\", loss_2.item(), step)\n",
        "            writer.add_scalar(\"train-iteration/L\", loss.item(), step)\n",
        "            writer.add_scalar(\"train-iteration/acc\", acc, step)\n",
        "\n",
        "        print('train acc: {:.6f}'.format(train_acc.avg), 'train L1 {:.6f}'.format(train_loss_1.avg),\n",
        "              'train L2 {:.6f}'.format(train_loss_2.avg), 'train L {:.6f}'.format(train_loss.avg), f'epoch {epoch}')\n",
        "\n",
        "        # epoch logger\n",
        "        writer.add_scalar(\"train/L1\", train_loss_1.avg, epoch)\n",
        "        writer.add_scalar(\"train/L2\", train_loss_2.avg, epoch)\n",
        "        writer.add_scalar(\"train/L\", train_loss.avg, epoch)\n",
        "        writer.add_scalar(\"train/acc\", train_acc.avg, epoch)\n",
        "\n",
        "        # validation\n",
        "        if val_dic is not None:\n",
        "            model.eval()\n",
        "            pg = tqdm(val_loader, leave=False, total=len(val_loader), disable=not tqdm_on)\n",
        "            with torch.no_grad():\n",
        "                tv_acc = AverageMeter()  # tv stands for train_val\n",
        "                tv_loss_1 = AverageMeter()\n",
        "                tv_loss_2 = AverageMeter()\n",
        "                tv_loss = AverageMeter()\n",
        "                for i, (x1, x2, x3, y) in enumerate(pg):\n",
        "                    x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "                    pred, feats = model(x, return_feat=True)\n",
        "\n",
        "                    # classification\n",
        "                    loss_1 = criterion(pred, y.long())\n",
        "\n",
        "                    # contrastive learning\n",
        "                    sim_matrix = compute_sim_matrix(feats)\n",
        "                    target_matrix = compute_target_matrix(y)\n",
        "                    loss_2 = contrastive_loss(sim_matrix, target_matrix, temperature, y)\n",
        "\n",
        "                    # total loss\n",
        "                    # loss = loss_1 + coefficient * loss_2\n",
        "                    loss = loss_1\n",
        "\n",
        "                    # logger\n",
        "                    tv_acc.update((pred.argmax(1) == y).sum().item() / len(y))\n",
        "                    # test_acc.update(\n",
        "                    #     f1_score(y.cpu().detach().numpy(), pred.argmax(1).cpu().detach().numpy(), average='macro'))\n",
        "                    tv_loss.update(loss.item())\n",
        "                    tv_loss_1.update(loss_1.item())\n",
        "                    tv_loss_2.update(loss_2.item())\n",
        "\n",
        "                    pg.set_postfix({\n",
        "                        'train_val acc': '{:.6f}'.format(tv_acc.avg),\n",
        "                        'epoch': '{:03d}'.format(epoch)\n",
        "                    })\n",
        "\n",
        "        # testing\n",
        "        model.eval()\n",
        "        pg = tqdm(test_loader, leave=False, total=len(test_loader), disable=not tqdm_on)\n",
        "        with torch.no_grad():\n",
        "            test_acc = AverageMeter()\n",
        "            test_loss_1 = AverageMeter()\n",
        "            test_loss_2 = AverageMeter()\n",
        "            test_loss = AverageMeter()\n",
        "            for i, (x1, x2, x3, y) in enumerate(pg):\n",
        "                x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "                pred, feats = model(x, return_feat=True)\n",
        "\n",
        "                # classification\n",
        "                loss_1 = criterion(pred, y.long())\n",
        "\n",
        "                # contrastive learning\n",
        "                sim_matrix = compute_sim_matrix(feats)\n",
        "                target_matrix = compute_target_matrix(y)\n",
        "                loss_2 = contrastive_loss(sim_matrix, target_matrix, temperature, y)\n",
        "\n",
        "                # total loss\n",
        "                # loss = loss_1 + coefficient * loss_2\n",
        "                loss = loss_1\n",
        "\n",
        "                # logger\n",
        "                test_acc.update((pred.argmax(1) == y).sum().item() / len(y))\n",
        "                # test_acc.update(\n",
        "                #     f1_score(y.cpu().detach().numpy(), pred.argmax(1).cpu().detach().numpy(), average='macro'))\n",
        "                test_loss.update(loss.item())\n",
        "                test_loss_1.update(loss_1.item())\n",
        "                test_loss_2.update(loss_2.item())\n",
        "\n",
        "                pg.set_postfix({\n",
        "                    'test acc': '{:.6f}'.format(test_acc.avg),\n",
        "                    'epoch': '{:03d}'.format(epoch)\n",
        "                })\n",
        "\n",
        "        # logging\n",
        "        if val_dic is not None:\n",
        "            writer.add_scalar(\"tv/L1\", tv_loss_1.avg, epoch)\n",
        "            writer.add_scalar(\"tv/L2\", tv_loss_2.avg, epoch)\n",
        "            writer.add_scalar(\"tv/L\", tv_loss.avg, epoch)\n",
        "            writer.add_scalar(\"tv/acc\", tv_acc.avg, epoch)\n",
        "\n",
        "        writer.add_scalar(\"test/L1\", test_loss_1.avg, epoch)\n",
        "        writer.add_scalar(\"test/L2\", test_loss_2.avg, epoch)\n",
        "        writer.add_scalar(\"test/L\", test_loss.avg, epoch)\n",
        "        writer.add_scalar(\"test/acc\", test_acc.avg, epoch)\n",
        "\n",
        "        # scheduler.step(test_loss.avg)\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f'epoch {epoch}, train acc {train_acc.avg}, test acc {test_acc.avg}')\n",
        "\n",
        "        final_test_acc = test_acc.avg\n",
        "\n",
        "        # save model\n",
        "        if test_acc.avg:\n",
        "            if test_acc.avg >= best_acc:\n",
        "                cur_models = os.listdir(exp_dir)\n",
        "                for cur_model in cur_models:\n",
        "                    if cur_model.endswith(\".pt\"):\n",
        "                        os.remove(os.path.join(exp_dir, cur_model))\n",
        "                save_model(exp_dir, f'{id}_val{final_test_acc:.5f}_e{epoch}.pt', model)\n",
        "        best_acc = max(best_acc, test_acc.avg)\n",
        "\n",
        "        if val_dic is not None:\n",
        "            print(f'epoch {epoch}, train val acc {tv_acc.avg}')\n",
        "            final_tv_acc = tv_acc.avg\n",
        "            best_tv_acc = max(best_tv_acc, tv_acc.avg)\n",
        "\n",
        "    # save checkpoint\n",
        "    save_model(exp_dir, f'{id}_val{final_test_acc:.5f}_finale{epoch}.pt', model)\n",
        "\n",
        "    print(\n",
        "        f'Training complete after {num_epochs} epochs. Final val acc = {final_tv_acc}, '\n",
        "        f'best val acc = {best_tv_acc}, best test acc = {best_acc}.'\n",
        "        f'Final test acc {final_test_acc}')\n",
        "\n",
        "    return final_test_acc, final_train_preds, final_test_preds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBuLHq2em9lz"
      },
      "source": [
        "# main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "je5L1MwiGZB1"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    datasets = ['imdb62', 'blog', 'turing', 'diffusiondb']\n",
        "    parser = argparse.ArgumentParser(description=f'Training models for datasets {datasets}')\n",
        "    parser.add_argument('--dataset', type=str, help='dataset used for training', choices=datasets)\n",
        "    parser.add_argument('--id', type=str, default='0', help='experiment id')\n",
        "    parser.add_argument('--gpu', type=str, help='the cuda devices used for training', default=\"0,1,2,3\")\n",
        "    parser.add_argument('--tqdm', type=bool, help='whether tqdm is on', default=False)\n",
        "    parser.add_argument('--authors', type=int, help='number of authors', default=None)\n",
        "    parser.add_argument('--samples-per-auth', type=int, help='number of samples per author', default=None)\n",
        "    parser.add_argument('--epochs', type=int, default=5)\n",
        "    parser.add_argument('--model', type=str, default='microsoft/deberta-base')\n",
        "    parser.add_argument('--coe', type=float, default=1)\n",
        "\n",
        "    # dataset - num of authors mapping\n",
        "    default_num_authors = {\n",
        "        'imdb62': 62,\n",
        "        'blog': 50,\n",
        "        'turing': 20,\n",
        "        'diffusiondb': 100,\n",
        "    }\n",
        "\n",
        "    training_args = [\n",
        "    '--dataset', 'diffusiondb',\n",
        "    '--id', 'diffusiondb1000_cls',\n",
        "    '--gpu', '0',\n",
        "    '--tqdm', 'True',\n",
        "    '--authors', '100',\n",
        "    '--epochs', '10',\n",
        "    '--model', 'bert-base-cased'\n",
        "    ]\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args(training_args)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "    source = args.dataset\n",
        "    num_authors = args.authors if args.authors is not None else default_num_authors[args.dataset]\n",
        "    print(' '.join(f'{k}={v}' for k, v in vars(args).items()))  # print all args\n",
        "\n",
        "    # masked classes\n",
        "    mask_classes = {\n",
        "        'blog': [],\n",
        "        'imdb62': [],\n",
        "        'turing': [],\n",
        "        'diffusiondb': [],\n",
        "    }\n",
        "\n",
        "    # load data and remove emails containing the sender's name\n",
        "    # df = load_dataset_dataframe(source)\n",
        "    nlp_train = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/train_random100_label_2.csv')\n",
        "    nlp_val = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/val_random100_label_2.csv')\n",
        "    nlp_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_2.csv')\n",
        "    nlp_train = nlp_train[['prompt', 'user_name']]\n",
        "    nlp_train.columns = ['content', 'Target']\n",
        "    nlp_val = nlp_val[['prompt', 'user_name']]\n",
        "    nlp_val.columns = ['content', 'Target']\n",
        "    nlp_test = nlp_test[['prompt', 'user_name']]\n",
        "    nlp_test.columns = ['content', 'Target']\n",
        "\n",
        "    if args.authors is not default_num_authors[args.dataset]:\n",
        "        warnings.warn(f\"Number of authors for dataset {args.dataset} is {default_num_authors[args.dataset]}, \"\n",
        "                      f\"but got {args.authors} instead. \")\n",
        "\n",
        "    if args.samples_per_auth is not None:\n",
        "        warnings.warn(f\"Number of samples per author specified as {args.samples_per_auth}, which is a \"\n",
        "                      f\"dangerous argument. \")\n",
        "\n",
        "    limit = num_authors\n",
        "    print(\"Number of authors: \", limit)\n",
        "\n",
        "    # select top N senders and build train and test\n",
        "    # nlp_train, nlp_val, nlp_test = build_train_test(df, source, limit, per_author=args.samples_per_auth, seed=0)\n",
        "\n",
        "    # train\n",
        "    if 'enron' in source or 'imdb62' in source or 'blog' in source:\n",
        "        train_bert(nlp_train, nlp_test, args.tqdm, args.model, 768, args.id, args.epochs, base_bs=8, base_lr=1e-5,\n",
        "                   mask_classes=mask_classes[args.dataset], coefficient=args.coe, num_authors=num_authors,\n",
        "                   val_dic=nlp_val)\n",
        "    elif 'turing' in source:\n",
        "        train_bert(nlp_train, nlp_test, args.tqdm, args.model, 768, args.id, args.epochs, base_bs=7, base_lr=5e-6,\n",
        "                   mask_classes=mask_classes[args.dataset], coefficient=args.coe, num_authors=num_authors,\n",
        "                   val_dic=nlp_val)\n",
        "    else:\n",
        "        train_bert(nlp_train, nlp_test, args.tqdm, args.model, 768, args.id, args.epochs, base_bs=24, base_lr=2e-5,\n",
        "                   mask_classes=mask_classes[args.dataset], coefficient=args.coe, num_authors=num_authors, val_dic=nlp_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "eO0-6Uw0l-WQ"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    datasets = ['imdb62', 'blog', 'turing', 'diffusiondb']\n",
        "    parser = argparse.ArgumentParser(description=f'Training models for datasets {datasets}')\n",
        "    parser.add_argument('--dataset', type=str, help='dataset used for training', choices=datasets)\n",
        "    parser.add_argument('--id', type=str, default='0', help='experiment id')\n",
        "    parser.add_argument('--gpu', type=str, help='the cuda devices used for training', default=\"0,1,2,3\")\n",
        "    parser.add_argument('--tqdm', type=bool, help='whether tqdm is on', default=False)\n",
        "    parser.add_argument('--authors', type=int, help='number of authors', default=None)\n",
        "    parser.add_argument('--samples-per-auth', type=int, help='number of samples per author', default=None)\n",
        "    parser.add_argument('--epochs', type=int, default=5)\n",
        "    parser.add_argument('--model', type=str, default='microsoft/deberta-base')\n",
        "    parser.add_argument('--coe', type=float, default=1)\n",
        "\n",
        "    # dataset - num of authors mapping\n",
        "    default_num_authors = {\n",
        "        'imdb62': 62,\n",
        "        'blog': 50,\n",
        "        'turing': 20,\n",
        "        'diffusiondb': 100,\n",
        "    }\n",
        "\n",
        "    training_args = [\n",
        "    '--dataset', 'diffusiondb',\n",
        "    '--id', 'diffusiondb1000_cls',\n",
        "    '--gpu', '0',\n",
        "    '--tqdm', 'True',\n",
        "    '--authors', '100',\n",
        "    '--epochs', '20',\n",
        "    '--model', 'bert-base-cased'\n",
        "    ]\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args(training_args)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "    source = args.dataset\n",
        "    num_authors = args.authors if args.authors is not None else default_num_authors[args.dataset]\n",
        "    print(' '.join(f'{k}={v}' for k, v in vars(args).items()))  # print all args\n",
        "\n",
        "    # masked classes\n",
        "    mask_classes = {\n",
        "        'blog': [],\n",
        "        'imdb62': [],\n",
        "        'turing': [],\n",
        "        'diffusiondb': [],\n",
        "    }\n",
        "\n",
        "    # load data and remove emails containing the sender's name\n",
        "    # df = load_dataset_dataframe(source)\n",
        "    nlp_train = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/train_random100_label_2.csv')\n",
        "    nlp_val = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/val_random100_label_2.csv')\n",
        "    nlp_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_2.csv')\n",
        "    nlp_train = nlp_train[['prompt', 'user_name']]\n",
        "    nlp_train.columns = ['content', 'Target']\n",
        "    nlp_val = nlp_val[['prompt', 'user_name']]\n",
        "    nlp_val.columns = ['content', 'Target']\n",
        "    nlp_test = nlp_test[['prompt', 'user_name']]\n",
        "    nlp_test.columns = ['content', 'Target']\n",
        "\n",
        "    if args.authors is not default_num_authors[args.dataset]:\n",
        "        warnings.warn(f\"Number of authors for dataset {args.dataset} is {default_num_authors[args.dataset]}, \"\n",
        "                      f\"but got {args.authors} instead. \")\n",
        "\n",
        "    if args.samples_per_auth is not None:\n",
        "        warnings.warn(f\"Number of samples per author specified as {args.samples_per_auth}, which is a \"\n",
        "                      f\"dangerous argument. \")\n",
        "\n",
        "    limit = num_authors\n",
        "    print(\"Number of authors: \", limit)\n",
        "\n",
        "    # select top N senders and build train and test\n",
        "    # nlp_train, nlp_val, nlp_test = build_train_test(df, source, limit, per_author=args.samples_per_auth, seed=0)\n",
        "\n",
        "    # train\n",
        "    if 'enron' in source or 'imdb62' in source or 'blog' in source:\n",
        "        train_bert(nlp_train, nlp_test, args.tqdm, args.model, 768, args.id, args.epochs, base_bs=8, base_lr=1e-5,\n",
        "                   mask_classes=mask_classes[args.dataset], coefficient=args.coe, num_authors=num_authors,\n",
        "                   val_dic=nlp_val)\n",
        "    elif 'turing' in source:\n",
        "        train_bert(nlp_train, nlp_test, args.tqdm, args.model, 768, args.id, args.epochs, base_bs=7, base_lr=5e-6,\n",
        "                   mask_classes=mask_classes[args.dataset], coefficient=args.coe, num_authors=num_authors,\n",
        "                   val_dic=nlp_val)\n",
        "    else:\n",
        "        train_bert(nlp_train, nlp_test, args.tqdm, args.model, 768, args.id, args.epochs, base_bs=24, base_lr=8e-5,\n",
        "                   mask_classes=mask_classes[args.dataset], coefficient=args.coe, num_authors=num_authors, val_dic=nlp_val)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EnPIWQDcm8Hb"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    datasets = ['imdb62', 'blog', 'turing', 'diffusiondb']\n",
        "    parser = argparse.ArgumentParser(description=f'Training models for datasets {datasets}')\n",
        "    parser.add_argument('--dataset', type=str, help='dataset used for training', choices=datasets)\n",
        "    parser.add_argument('--id', type=str, default='0', help='experiment id')\n",
        "    parser.add_argument('--gpu', type=str, help='the cuda devices used for training', default=\"0,1,2,3\")\n",
        "    parser.add_argument('--tqdm', type=bool, help='whether tqdm is on', default=False)\n",
        "    parser.add_argument('--authors', type=int, help='number of authors', default=None)\n",
        "    parser.add_argument('--samples-per-auth', type=int, help='number of samples per author', default=None)\n",
        "    parser.add_argument('--epochs', type=int, default=5)\n",
        "    parser.add_argument('--model', type=str, default='microsoft/deberta-base')\n",
        "    parser.add_argument('--coe', type=float, default=1)\n",
        "\n",
        "    # dataset - num of authors mapping\n",
        "    default_num_authors = {\n",
        "        'imdb62': 62,\n",
        "        'blog': 50,\n",
        "        'turing': 20,\n",
        "        'diffusiondb': 100,\n",
        "    }\n",
        "\n",
        "    training_args = [\n",
        "    '--dataset', 'diffusiondb',\n",
        "    '--id', 'diffusiondb100',\n",
        "    '--gpu', '0',\n",
        "    '--tqdm', 'True',\n",
        "    '--authors', '100',\n",
        "    '--epochs', '20',\n",
        "    '--model', 'bert-base-cased'\n",
        "    ]\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args(training_args)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "    source = args.dataset\n",
        "    num_authors = args.authors if args.authors is not None else default_num_authors[args.dataset]\n",
        "    print(' '.join(f'{k}={v}' for k, v in vars(args).items()))  # print all args\n",
        "\n",
        "    # masked classes\n",
        "    mask_classes = {\n",
        "        'blog': [],\n",
        "        'imdb62': [],\n",
        "        'turing': [],\n",
        "        'diffusiondb': [],\n",
        "    }\n",
        "\n",
        "    # load data and remove emails containing the sender's name\n",
        "    # df = load_dataset_dataframe(source)\n",
        "    nlp_train = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/train_random100_label_1.csv')\n",
        "    nlp_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "    nlp_train = nlp_train[['prompt', 'user_name']]\n",
        "    nlp_train.columns = ['content', 'Target']\n",
        "    nlp_test = nlp_test[['prompt', 'user_name']]\n",
        "    nlp_test.columns = ['content', 'Target']\n",
        "\n",
        "    if args.authors is not default_num_authors[args.dataset]:\n",
        "        warnings.warn(f\"Number of authors for dataset {args.dataset} is {default_num_authors[args.dataset]}, \"\n",
        "                      f\"but got {args.authors} instead. \")\n",
        "\n",
        "    if args.samples_per_auth is not None:\n",
        "        warnings.warn(f\"Number of samples per author specified as {args.samples_per_auth}, which is a \"\n",
        "                      f\"dangerous argument. \")\n",
        "\n",
        "    limit = num_authors\n",
        "    print(\"Number of authors: \", limit)\n",
        "\n",
        "    # select top N senders and build train and test\n",
        "    # nlp_train, nlp_val, nlp_test = build_train_test(df, source, limit, per_author=args.samples_per_auth, seed=0)\n",
        "\n",
        "    # train\n",
        "    if 'enron' in source or 'imdb62' in source or 'blog' in source:\n",
        "        train_bert(nlp_train, nlp_test, args.tqdm, args.model, 768, args.id, args.epochs, base_bs=8, base_lr=1e-5,\n",
        "                   mask_classes=mask_classes[args.dataset], coefficient=args.coe, num_authors=num_authors,\n",
        "                   val_dic=nlp_val)\n",
        "    elif 'turing' in source:\n",
        "        train_bert(nlp_train, nlp_test, args.tqdm, args.model, 768, args.id, args.epochs, base_bs=7, base_lr=5e-6,\n",
        "                   mask_classes=mask_classes[args.dataset], coefficient=args.coe, num_authors=num_authors,\n",
        "                   val_dic=nlp_val)\n",
        "    else:\n",
        "        train_bert(nlp_train, nlp_test, args.tqdm, args.model, 768, args.id, args.epochs, base_bs=24, base_lr=2e-5,\n",
        "                   mask_classes=mask_classes[args.dataset], coefficient=args.coe, num_authors=num_authors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Noq5aWcMyHYm"
      },
      "outputs": [],
      "source": [
        "train_x, train_y = nlp_train['content'].tolist(), nlp_train['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, num_authors\n",
        "train_set = BertDataset(train_x, train_y, tokenizer, num_tokens)\n",
        "train_sampler = TrainSamplerMultiClassUnit(train_set, sample_unit_size=2)\n",
        "train_loader = DataLoader(train_set, batch_size=8, sampler=train_sampler, shuffle=False,\n",
        "                              num_workers=4, pin_memory=True, drop_last=True)\n",
        "pg = tqdm(train_loader, leave=False, total=len(train_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, num_authors\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  print(x1.shape)\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  print(y.shape)\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  print(pred.shape)\n",
        "  print(feats.shape)\n",
        "  # generate the mask\n",
        "  mask = y.clone().cpu().apply_(lambda x: x not in mask_classes).type(torch.bool).cuda()\n",
        "  feats, y = feats[mask], y[mask]\n",
        "  if len(y) == 0:\n",
        "      continue\n",
        "\n",
        "  # contrastive learning\n",
        "  sim_matrix = compute_sim_matrix(feats)\n",
        "  print(sim_matrix)\n",
        "  target_matrix = compute_target_matrix(y)\n",
        "  print(target_matrix)\n",
        "  loss_2 = contrastive_loss(sim_matrix, target_matrix, 0.1, y)\n",
        "  print(F.softmax(sim_matrix / 0.1).log())\n",
        "  print(F.softmax(target_matrix / 0.1))\n",
        "\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvTFz4wv0Cir"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/datasets/full_blog.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0wa3ESSd8jvJ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/train_random100_1.csv')\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OMK8jzL0g-T"
      },
      "outputs": [],
      "source": [
        "list_spk = pd.DataFrame(df['From'].value_counts().iloc[:limit]).reset_index()\n",
        "list_spk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmCODjLet87P"
      },
      "source": [
        "# Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsbuNgAFsenc"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    for name, link in datasets.items():\n",
        "        if name in os.listdir(dataset_path):\n",
        "            continue\n",
        "        gdown.download(link, name, quiet=False)\n",
        "\n",
        "    tar = tarfile.open(list(datasets.keys())[0])\n",
        "    tar.extractall(path='datasets')\n",
        "    tar.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FScW3Dt7yoFF"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KORwqHx8skpH"
      },
      "source": [
        "# Analyze the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvbgIxe7n0_F"
      },
      "source": [
        "## TSNE plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9_dwXa48n2Gy"
      },
      "outputs": [],
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# Randomly select 10 authors\n",
        "selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_bert-base-cased_coe1_temp0.1_unit2_epoch20/diffusiondb100_val0.77108_e17.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# # Create a color map for authors\n",
        "# unique_authors = df['user_name'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# # Plot t-SNE results\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     indices = df['user_name'] == author\n",
        "#     plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "# plt.title(\"t-SNE Visualization of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# # Step 4: Calculate Cluster Centers\n",
        "# cluster_centers = df.groupby('user_name')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# # Plot cluster centers with the same color coding\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "# plt.title(\"Cluster Centers of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pp7F8M5gwYGR"
      },
      "outputs": [],
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_cls_bert-base-cased_coe1_temp0.1_unit2_epoch20/diffusiondb100_cls_val0.77510_e7.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# # Create a color map for authors\n",
        "# unique_authors = df['user_name'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# # Plot t-SNE results\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     indices = df['user_name'] == author\n",
        "#     plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "# plt.title(\"t-SNE Visualization of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# # Step 4: Calculate Cluster Centers\n",
        "# cluster_centers = df.groupby('user_name')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# # Plot cluster centers with the same color coding\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "# plt.title(\"Cluster Centers of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4T26BSsEy8wG"
      },
      "outputs": [],
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_supcon_bert-base-cased_coe1_temp0.1_unit2_epoch30/diffusiondb100_supcon_val0.78125_e26.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_supcon_lib_mining_bert-base-cased_coe1_temp0.1_unit2_epoch30/diffusiondb100_supcon_lib_mining_val0.78472_e22.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "v_fiQ_yEfCB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMrE0cnvEiMk"
      },
      "source": [
        "## TSNE plot 1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_topicseparate100_label_1.csv')\n",
        "df = df[['prompt', 'user_label']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# Randomly select 10 authors\n",
        "selected_authors = np.random.choice(df['Target'].unique(), size=20, replace=False)\n",
        "df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe1_para_topic_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe1_para_topic_val0.48859_e29.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# # Create a color map for authors\n",
        "# unique_authors = df['user_name'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# # Plot t-SNE results\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     indices = df['user_name'] == author\n",
        "#     plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "# plt.title(\"t-SNE Visualization of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# # Step 4: Calculate Cluster Centers\n",
        "# cluster_centers = df.groupby('user_name')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# # Plot cluster centers with the same color coding\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "# plt.title(\"Cluster Centers of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "qqm8QeA_AD_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_topicseparate100_label_1.csv')\n",
        "# df = df[['prompt', 'user_label']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=20, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe1_para_topic_1_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe1_para_topic_1_val0.48462_e26.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# # Create a color map for authors\n",
        "# unique_authors = df['user_name'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# # Plot t-SNE results\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     indices = df['user_name'] == author\n",
        "#     plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "# plt.title(\"t-SNE Visualization of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# # Step 4: Calculate Cluster Centers\n",
        "# cluster_centers = df.groupby('user_name')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# # Plot cluster centers with the same color coding\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "# plt.title(\"Cluster Centers of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "l7CK2AQGBNsn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_topicseparate100_label_1.csv')\n",
        "# df = df[['prompt', 'user_label']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=20, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_supcon_coe1_para_topic_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_supcon_coe1_para_topic_val0.48413_e26.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# # Create a color map for authors\n",
        "# unique_authors = df['user_name'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# # Plot t-SNE results\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     indices = df['user_name'] == author\n",
        "#     plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "# plt.title(\"t-SNE Visualization of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# # Step 4: Calculate Cluster Centers\n",
        "# cluster_centers = df.groupby('user_name')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# # Plot cluster centers with the same color coding\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "# plt.title(\"Cluster Centers of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "-A38YpCtEVTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v6O0clo-EV2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "from transformers import BertGenerationEncoder\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "# model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "model = BertGenerationEncoder.from_pretrained('bert-base-cased')\n",
        "ckpt_path = \"/content/drive/MyDrive/msc_project/model/contrastive/club/content_encoder_supcon_18.pt\"\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  # pred, feats = model(x, return_feat=True)\n",
        "  outputs = model(input_ids=x[0], attention_mask=x[2])\n",
        "  feats = outputs.last_hidden_state.flatten(1)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "WXm62yTNmkjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "199eoRG43Tgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# Randomly select 10 authors\n",
        "selected_authors = np.random.choice(df['Target'].unique(), size=20, replace=False)\n",
        "df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
        "model = model.cuda()\n",
        "all_feats = model.encode(test_x, convert_to_tensor=False)\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "RZrV5HO13Dfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## current! - tsne plot"
      ],
      "metadata": {
        "id": "zhcVD6HF-Llu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBceq0X7EiMq"
      },
      "outputs": [],
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# Randomly select 10 authors\n",
        "selected_authors = np.random.choice(df['Target'].unique(), size=20, replace=False)\n",
        "df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_supcon_para_bert-base-cased_coe1_temp0.1_unit2_epoch30/diffusiondb100_supcon_para_val0.72321_e29.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# # Create a color map for authors\n",
        "# unique_authors = df['user_name'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# # Plot t-SNE results\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     indices = df['user_name'] == author\n",
        "#     plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "# plt.title(\"t-SNE Visualization of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# # Step 4: Calculate Cluster Centers\n",
        "# cluster_centers = df.groupby('user_name')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# # Plot cluster centers with the same color coding\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "# plt.title(\"Cluster Centers of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=20, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe1_para_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe1_para_val0.73264_e16.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "\n",
        "# Create a color map for authors\n",
        "# unique_authors = df['Target'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n",
        "\n"
      ],
      "metadata": {
        "id": "mQFPlZ-ahiY1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_try2/style_encoder_supcon_9.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "\n",
        "# Create a color map for authors\n",
        "# unique_authors = df['Target'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "4E_Uur_X-aFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    EncoderDecoderModel,\n",
        "    BertGenerationDecoder,\n",
        "    BertGenerationEncoder,\n",
        "    BertTokenizer,\n",
        "    BertModel)\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "print(test_x)\n",
        "print(test_y)\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "print(unique_authors)\n",
        "print(color_map)\n",
        "\n",
        "\n",
        "# from transformers import BertTokenizer, BertModel\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "# extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "# num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "# test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "# test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "#                              pin_memory=True)\n",
        "\n",
        "# pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "# ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "# num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "# model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "\n",
        "\n",
        "\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_try2/content_encoder_supcon_9.pt'\n",
        "\n",
        "import torch\n",
        "from transformers import BertGenerationEncoder, BertConfig\n",
        "\n",
        "# Load your model's state_dict from the .pt file\n",
        "state_dict = torch.load(ckpt_path)\n",
        "\n",
        "# Reinitialize the model with the same configuration used during training\n",
        "# You can load a default or custom configuration if needed\n",
        "config = BertConfig.from_pretrained('bert-base-cased')  # or use your specific configuration\n",
        "model = BertGenerationEncoder(config)\n",
        "\n",
        "# Load the weights into the model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# The model is now loaded and ready for inference or further training\n",
        "\n",
        "\n",
        "# model = BertGenerationEncoder.from_pretrained(ckpt_path)\n",
        "# model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "# model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  # x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  outputs = model(x1, attention_mask=x3)\n",
        "  # all_feats.append(outputs.last_hidden_state.flatten(1).cpu().detach().numpy())\n",
        "  # all_labels.append(y.cpu().detach().numpy())\n",
        "  all_feats.append(outputs.last_hidden_state.flatten(1).numpy())\n",
        "  all_labels.append(y.numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "\n",
        "# Create a color map for authors\n",
        "# unique_authors = df['Target'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "o7WD0TTenMj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    EncoderDecoderModel,\n",
        "    BertGenerationDecoder,\n",
        "    BertGenerationEncoder,\n",
        "    BertTokenizer,\n",
        "    BertModel)\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "# test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "# print(test_x)\n",
        "# print(test_y)\n",
        "# unique_authors = df['Target'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "# print(unique_authors)\n",
        "# print(color_map)\n",
        "\n",
        "test_x = ['walking around obi castle town, miyazaki, japan. volumetric lighting, spring early morning, late afternoon, cherry blossom trees, nice weather, few clouds, realistic illustration, perfectly shaded, soft painting, art by krenz cushart and wenjun lin', 'real life Pokemon, cute!!!, fluffy!!!, ultra realistic!!!, golden hour, ultra detailed, sharp focus', 'parked 2 0 2 2 dodge charger srt hellcat, fog, rain, volumetric lighting, beautiful, golden hour, golden ratio, sharp focus, highly detailed, cgsociety', 'Donald Duck as a Star Trek Voyager cast member. highly detailed, 4k, CGI, Photoreal, frame from the tv show.', 'photo taken of an epic intricate, ultra detailed, super realistic gritty, wet, slimy, lifelike sculpture of a nightmarish hellish alien creature with tentacle dreadlocks created by weta workshop for james cameron, zoomed in shots, photorealistic, sharp focus, white wall coloured workshop, cold blueish colour temperature, f 0. 4', 'Tifa Lockheart, intricate, seductive, erotic, tempting, portrait, character photography,', 'regal aristocratic a young!!!, long blond haired tom cruise as the vampire lestat de lioncourt portrait, luxurious indoor setting, atmospheric lighting, painted, menacing, intricate, volumetric lighting, rich deep colours masterpiece, sharp focus, ultra detailed, by leesha hannigan, ross tran, thierry doizon, kai carpenter, ignacio fernandez rios', 'retro futuristic vintage cars in showroom, atmospheric lighting, painted, intricate, volumetric lighting, beautiful, daytime, sunny weather, slight overcast, sharp focus, deep colours, ultra detailed, by leesha hannigan, ross tran, thierry doizon, kai carpenter, ignacio fernandez rios', 'wide angle shot of dilapidated fallout 5 tropical coastal city in real life, desolate, dilapidated, empty streets, nightmarish, some rusted retro futuristic fallout vintage style parked vehicles like cars, buses, trucks, trams, sunny weather, few clouds, volumetric lighting, photorealistic, daytime, autumn, sharp focus, ultra detailed, cgsociety', 'majestic gracious regal seductive isis priestess portrait, ancient egyptian, atmospheric lighting, curvy, painted, intricate, volumetric lighting, beautiful, rich deep colours masterpiece, golden hour, sharp focus, ultra detailed, by leesha hannigan, ross tran, thierry doizon, kai carpenter, ignacio fernandez rios', 'fallout 5, charismatic beautiful rugged brunette female protagonist, portrait, outdoors ruined cityscape, atmospheric lighting, painted, intricate, volumetric lighting, beautiful, foggy, daytime, slight overcast weather, sharp focus, deep colours, ultra detailed, by leesha hannigan, ross tran, thierry doizon, kai carpenter, ignacio fernandez rios', 'majestic gracious regal aristocratic blond female vampire portrait, atmospheric lighting, painted, voluptuous, menacing, intricate, volumetric lighting, beautiful, rich deep colours masterpiece, sharp focus, ultra detailed, by leesha hannigan, ross tran, thierry doizon, kai carpenter, ignacio fernandez rios', 'aerith gainsborough in red cottagecore dress, portrait, illustration, rim light, top light, overcast cloudy weather, perfectly shaded, soft painting, art by krenz cushart and wenjun lin', 'close - up photo of a real life smurf, f 1. 4, garden, golden ratio, rim light, top light, overcast day', 'real life pokemon, cute!!!, happy mood!!!, adorable!!!, fluffy!!!, ultra realistic!!!, golden hour, sharp focus', 'real life Pokemon, creepy!!!, scaly!!!, menacing, evil, ultra realistic!!!, daytime, slight overcast, sharp focus', 'Dream Theater, images and words new cover art', 'battle hardened, overpowering, pragmatic, charismatic character from the animatrix 2, face centered portrait, confident, ruined cityscape, sterile minimalistic room, architecture, fog, volumetric lighting, illustration, perfectly shaded, greenish tinge, cold lights soft painting, art by krenz cushart and wenjun lin', 'a photo of a shiny retro futuristic vintage car parked at deserted scenic viewpoint in alaska, volumetric lighting, serene, epic, beautiful, summer morning dew, sharp focus, ultra detailed, cgsociety', 'silent hill in real life, streets, sombre, parked cars, overcast, blankets of fog pockets, rain, volumetric lighting, beautiful, night time, autumn, sharp focus, 7 0 s visuals, ultra detailed, cgsociety', 'os homens da perna de pau e seus peixes. by marcel caram', 'blade runner. woodchucks in blade runner', 'the scariest image ever seen, by yves tanguy', 'axolotl themed final boss, rendered on a playstation 1', 'meteorite themed cosmic horror, re - imagined as the pepsi logo. city sized clay sculpture in a huge room. billowing clouds. brightest sun barely visible through the light fog', 'deep sea horror by ricardo bofill', 'image in the style of Hugh Ferriss. Black and dark grey. Tall, wide, imposing building in a dramatically lit metropolis. eerie. incomprehensible size.', 'keanu reeves dog hybrid rendered in n 6 4. nintendo 6 4 graphics keanu reeves mixed with a dog', 'a painting by ricardo bofill', 'Photorealistic image of a Rayquaza Elephant hybrid. realism, high definition', 'painterly brush strokes. brutalism hybrid. hugh ferriss and john singer sargent', 'planet sized deep sea trench', 'lithograph of a mans face that is made of wires, veins, soil and roots', 'large red bear on a street in new york city in the style of picasso', 'clay sculpture. portrait of a man with a hollow head. instead of a skull, there is a loose wire mesh, with gold liquid spilling out. wire mesh skull with gaps. painted clay sculpture. neon green or pink or yellow colored background, intense lighting and shadows. astonishing detail', 'a picture of a large building in the sky, a matte painting by mike winkelmann, cgsociety, deconstructivism, matte painting, matte drawing, cryengine', 'subtle mist. gargantuan creature off in the distance', 'afrofuturism hologram by magritte', 'an axolotl made of broken glass, astonishing detail. hyper realistic', 'green spiders. abstract collage made of paper, clay, and twine', 'ink illustration detailed portrait of DC black adam, artwork by mike mignola', 'a lomographic photo of old pacific rim ( 2 0 1 3 ) jaeger, standing in typical japanese yard in small town, hikone on background, cinestill, bokeh', 'jeff goldblum anime style, by Hiroyuki Imaishi, studio trigger', 'Metal Gear, Soild Snake, by ashley wood, character design, concept art', \"v for vendetta by ashley wood, yoji shinkawa, jamie hewlett, 6 0's french movie poster, french impressionism, black red white colors, palette knife and brush strokes, dutch tilt\", \"anime key visual concept art of marvel ghost rider, riding a red akira motorcycle, by ashley wood, yoji shinkawa, jamie hewlett, 6 0's french movie poster, french impressionism, vivid colors, palette knife and brush strokes, style of kawacy and makoto shinkai and greg rutkowski\", 'key anime visuals of maria de medeiros in a still from the anime your name ( 2 0 1 6 ) directed by makoto shinkai', 'a street skateboarding visual anime, in downtown tampa florida, by studio gainax, studio trigger, detailed, sharp, asymmetrical face, slice of life', 'batman cyber ninja animated movie still by kamikaze douga', \"tv head anime robot, wearing red hoodie, holding katana, medium portrait by Ashley Wood, Yoji Shinkawa, Jamie Hewlett, 60's French movie poster, French Impressionism, vivid colors, palette knife and brush strokes, paint drips, Dutch tilt, 8k, hd, high resolution print\", 'award winning color photo of,  tony hawk, skateboarding, doing 540 in the 1986 vert contest, fisheye lens, detailed faces, detailed skateboard, 8k, balanced composition', 'audrey hepburn, fashion portrait, illustrated by david downtown, color ink', 'attack of the 5 0 foot woman ( 1 9 5 8 ) film as a giant japanese cosplay girl towering over buildings', 'idris elba in a still from the anime your name ( 2 0 1 6 ) directed by makoto shinkai', \"flik ( a bug's life 1 9 9 8 ) is a stormtrooper, holding a blaster, in the death star corridor, in a still of star wars episode iv a new hope ( 1 9 7 7 )\", 'in the style of frank frazetta, a highly detailed matte portrait painting of conan the barbarian, standing on a mountain holding a sword, by ashley wood, triumph pose, eerie magazine cover, red orange brown colors, impressionism, palette knife and brush strokes, photorealistic, detailed, intricate, 4 k, focused, extreme details, masterpiece', \"highly detailed orange blue canti standing triumph after a battle from flcl ( 2 0 0 1 ), hoody, style of ashley wood, yoji shinkawa, jamie hewlett, 6 0's french movie poster, french impressionism, vivid colors, palette knife and brush strokes, grunge aesthetic, dynamic composition\", \"close up of flik ( a bug's life ), holding a blaster, in a still of star wars episode iv a new hope ( 1 9 7 7 ), by george lucas, death star scifi corridor\", 'a close up portrait of Astro boy in the style of Megaman, weapon on a ready looking determined overlooking a cyberpunk city in the background, full face portrait composition, 2D drawing by Mike Mignola, Yoji Shinkawa, flat colors, chiaroscuro lighting', '3 5 mm photo of 1 9 4 0 batmoblie car designed by norman bel geddes, driving fast on the streets of gotham', 'photograph of a sunset in the beach, windy weather, reflections, white birds flying on sky', 'a mughal era painting of a modern cruise ship in arctic ocean', 'Kpop girl riding a motorcycle, 4k photography', 'hyper realistic photograph of wonder woman chopping trees in a rainforest at night', 'i wanted the fame but not the cover of news week', 'Amy Jackson blowing up blue balloon for ASMR', 'walter white holding the wwe champion belt, art by dean ellis', 'photograph of selena gomez riding a blue sports bike on a crowded street', 'Saul Goodman 3d meme', 'Memes of 2022', 'a flying DeLorean chasing a train, 1980s photorealistic style', 'guard of the kingdom in neptune, matte painting, behance hd', 'portrait of an ancient female assassin in white dress riding a motorcycle, photorealistic digital art', 'photo of marilyn monroe riding a motorcycle at night, cosmic horror, vibrant artwork by ellis dean', 'Ahri from League of Legends, photorealistic style', 'dove camwron blowing white balloon, mountains in background, stock photo', 'D.va from overwatch driving a racecar', 'A police lamborghini chasing a flying motorcycle', 'Luigi in GTA San Andreas', 'portrait of a woman holding a chainsaw at night wearing a jason mask, matte painting, volumetric lighting, post apocalyptic scenario, destroyed mirror world, dense fog in background, daytime', 'a cartoon face of a girl smiling with her eyes closed. she has red glasses, ears with piercings, red lips, eyelashes, pink cheeks, and black hair on a yellow background. the cartoony face is 2 d.', 'A two sided girl. The left side shows her with no changes. The right side however, shows her in a robotic form. She has a smiling expression!', 'a photorealistic picture of an evil man with a mask resembling jason voorhees using his own axe to kill a pig in the slaughterhouse', 'a neon sign depicting a rainbow brain with yellow electric bolts coming out of it', \"mario as a macy's parade balloon\", 'an extremely creepy picture of a bone in the road with blood and other bones which are part of a skeleton', 'an emoji of a yellow smiley face with the eyes becoming green circles with dollar signs, and the smiley face is sticking its tongue out with the tongue being green too and has a dollar sign too, apple emoji', 'a carbon - made peppa pig in real life', 'cave drawings of people eating oreos', 'a comic book featuring peppa pig', 'a giant long haired black - yellow dog and a giant long haired black - red dog', 'boobs', 'an abandoned city hall with the door being covered by some maersk shipping containers', 'A place full of land dinosaurs, along with flying dinosaurs and sea dinosaurs, 2D animation, 2D art, kids artwork', 'a giant, long - haired dog which has raised ears, white - black fur, and a tongue sticking out of it, smiling next to a gas station', 'peppa pig using ak - 4 7', 'a photorealistic image of a man without eyes and with his limbs sliced off, dead, with blood all over him, and some people doing a crime scene', 'a billboard advertisement for an insurance service with an eggplant', 'a flickr screenshot of an abandoned internet cafe with a blue wall mural, and a japanese sign, with artworks of people playing on computers on the mural', 'pixel man nft', 'tracer from overwatch drawn by hajime sorayama accurate, highly detailed', 'zbrush sculpt of female rogue world of warcraft stylized, artstation, character concept art, octane render, unreal engine 5', 'zbrush sculpture of ryu from street fighter trending on artstation S- 3814655743 n-5', 'portrait of strong female chaos angel, beautiful! coherent! by frank frazetta, by brom, strong line, deep color, spiked metal armor, maximalist', 'grass seamless texture', 'rafael grassetti figure sculpt zbrush', 'wrath from full metal alchemist as god of war amazing details 8 k beautiful ultra realistic by adam hughes sharp focus cinematic lightning', '3 x 3 array of tileable grass textures, grunge, stylized', 'jinx from league of legends, model, intricate, elegant, highly detailed, ray tracing, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by artgerm and greg rutkowski and alphonse mucha, 8 k', 'warrior', 'digital painting of jaina proudmoore amazing details 8 k beautiful ultra realistic sharp focus cinematic lightning highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration artgerm, tomasz alen kopera, peter mohrbacher, donato giancola, joseph christian leyendecker, wlop, frank frazetta', 'tracer from overwatch as ciri from witche r 5', 'goku in fortnite', 'tracer from overwatch portrait, close up, zbrush artstation concept art, intricate details, highly detailed portrait cinematic lightning, octane render, 8 k hd by artgerm', 'he black and gold geometric mother of death', 'portrait of jaina proudmoore amazing details 8 k beautiful ultra realistic sharp focus cinematic lightning highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration sozomaika', 'team fortress 2 in the style of yoji shinkawa', 'tracer megan fox fine _ detail _ anime _ realistic _ shaded _ lighting _ dramatic _ poster _ by _ ilya _ kuvshinov', 'medieval bikes in the style of h. r giger', 'portrait of ciri the witcher 5 in the style of jeehyung lee mirco cabbia amazing details 4 k beautiful ultra realistic sharp focus cinematic lightning highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, concept art', 'detailed, well-lit studio photo of an elegently dressed young girl who resembles Anya Taylor Joy looking at an elegant, detailed, complex mechanical steampunk brass orrery with a glowing sectioned glass sun, her face illuminated by the glass, elegant highly detailed digital painting artstation smooth sharp focus illustration, by Michael Whelan, James Gurney, John Williams Waterhouse, and Donato Giancola', 'realistic detailed 14-year old girl wearing future cybernetic battle armor by Alphonse Mucha, Ayami Kojima, Amano, Charlie Bowater, Karol Bak, Greg Hildebrandt, Jean Delville, and Donato Giancola, Art Nouveau, Neo-Gothic, gothic, rich deep colors', 'Detailed Interior of a flooded cathedral, light of god, light shafts, candles, stunning atmosphere, in Style of Peter Mohrbacher, cinematic lighting', 'a teenage girl lying on the floor, wearing a nightgown, by Frederic Leighton', 'a realistic face portrait of a teenage girl who looks like Uma Thurmond and Anya Taylor Joy with an anxious expression and parted lips, wearing mechanical robotic battle armor, by John William Waterhouse, Frederic Leighton, Alphonse Mucha, Edward Burne Jones', 'a full body art nouveau portrait of a 16-year old girl who resembles Audrey Hepburn and Saoirse Ronan with a worried, intense gaze and slightly opened mouth, wearing sheer silks and ornate intricate iridescent mother-of-pearl jewelry, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by John William Waterhouse and Bouguereau and Donato Giancola and alphonse mucha', 'a gorfal corfunzel by Hanz Freighly,', 'a realistic portrait of a teenage girl lying on the floor, wearing a nightgown like Flaming June, by Frederic Leighton, Alphonse Mucha, Edward Burne Jones', 'realistic detailed 14-year old girl wearing future cybernetic battle armor by Donato Giancola, Art Nouveau, Neo-Gothic, gothic, rich deep colors', 'an early photograph of a steampunk cathedral with god rays from the 19th century', 'an armored lich king screaming and getting up from his throne, by Mike Mignola', 'a full body art nouveau portrait of a fully armored samurai astronaut, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by John William Waterhouse and William Adolphe Bouguereau and Donato Giancola and Alphonse Mucha', 'dramtically lit, high quality studio photo of a girl who looks like 16-year old Audrey Hepburn and Scarlett Johansson, with parted lips and stunning, anxious eyes, wearing a silver satin gown, by Steve McCurry', 'Detailed Interior of a cathedral made of fruit and vegetables, light of god, light shafts, candles, stunning atmosphere, in Style of Peter Mohrbacher, cinematic lighting', 'a full body art nouveau portrait of a 16-year old girl who resembles Emma Watson, Saoirse Ronan and Anya Taylor Joy, ornate intricate golden battle armor, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, illustration, art by John William Waterhouse and greg rutkowski and Donato Giancola and alphonse mucha', 'realistic detailed face portrait of Joan of Arc wearing iridescent armor by Alphonse Mucha, art nouveau', 'realistic detailed stained glass of 16-year old girl who looks like Scarlett Johansson and Audrey Hepburn, as Anna from Frozen by Alphonse Mucha, Louis Comfort Tiffany, Ayami Kojima, Amano, Charlie Bowater, Karol Bak, Greg Hildebrandt, Jean Delville, and Mark Brooks, Art Nouveau, Neo-Gothic, gothic, rich deep colors', 'realistic detailed face portrait of 16-year old Scarlett Johansson as Joan of Arc wearing iridescent armor by Alphonse Mucha, art nouveau', 'a robotic tortoise from Horizon Zero Dawn, trending on Artstation, 4k HD', 'well-lit studio photograph of cutaway of the interior of a human cell, showing the nucleus and organelles, made of fruit floating in jello', 'a highly detailed Mark Zuckerberg latex mask by Rick Baker, high quality, Hollywood special effects, makeup', 'movie still of sean connery as gandalf in the lord of the rings, 4 k, high quality', 'painting of rivendell by maxfield parrish, stunning, beautiful, very detailed, waterfalls, elven architecture!!!!!!!!!!!!, 4 k, high quality', 'photo of Richard Stallman using a smartphone', 'detailed, intrincate painting of a biomechanic being painted by Oscar Chichoni, HD, high quality', 'steampunk Batman illustration, high quality, very detailed, dramatic, artstation', 'first-person footage of a skinwalker running towards the camera', 'ryan gosling eating cereal, wearing a black t - shirt, looking at the camera, 8 k, high quality', 'still of Betty White as Agent Smith in The Matrix', 'Anguish by Zdzislaw Beksinski, intrincate, hightly detailed, 4k, high quality', 'Zack de la Rocha jumping on a trampoline', 'a nuclear mushroom cloud inside of a bottle, studio photography, 4 k, high quality', 'a fat orange cat peacefuly floating in outer space', \"a train stopped in front of a dead whale that's blocking the train tracks, highly detailed, 4k, high quality\", 'a perfect pug - turtle hybrid, new reptile species, 8 k, high quality', 'Donald Trump as a goblin painted by Alan Lee, muted colors, folklore, goblincore, high quality', 'movie still of timothee chalamet as frodo in the lord of the rings, 4 k, high quality', 'a fire tornado destroying everything in its path, 8 k, high quality', 'an AI that creates other AIs, artstation, intrincate, highly detailed, epic', 'Brad Pitt failing to use the microwave, looking frustrated and angry', 'Ancient Greek statue of Donald Trump looking smug', 'Joe Biden wrestling Vladimir Putin, golden hour, in a garden, artstation, by J. C. Leyendecker and Peter Paul Rubens,', 'a beautiful woman in a swimsuit lays by the pacific sea, turquoise waters and palm trees, extremely detailed oil painting, sargent and leyendecker, savrasov levitan polenov, bruce pennington, tim hildebrandt, digital art, landscape painting, trending on artstation, masterpiece', 'Fidel Castro as a Capybara', 'actual photograph of UFO hovering over LA, award winning, golden hour,', 'two freaky floating twin nuns', 'ben stiller laughing wearing a yarmulke, award winning portrait, 5 0 mm photo', 'ferrari f 4 0 sports car on a racetrack during an overcast day, realistic 4 k octane beautifully detailed render, 4 k post - processing, highly detailed, intricate complexity, epic composition, magical atmosphere, cinematic lighting, masterpiece, ultra hd', 'Jesus and the Devil playing cards in a garden, photorealistic, award winning, 8k, trending on major art outlets,', 'lisa ann movie star wearing nun robes and habit', 'an old man smoking a pipe, sitting in a wooden rocking chair on a front porch, Whittling a piece of wood, by Norman Rockwell', 'a beautiful day at a tropical pool,colorised,photograph', 'Painting of muscular Elon Musk. Art by william adolphe bouguereau. During golden hour. Extremely detailed. Beautiful. 4K. Award winning.', 'tom cruise as bob ross, cinematic still, actively painting, amazing photo', 'a rabbit listening to the radio', 'dolce & gabbana campaign featuring sofia vergara as a cowgirl, long eyeslashes, huge juicy lips, big seductive eyes, unprocessed colors, # nofilter, shot by annie leibovitz, realistic vfx simulation', 'Aphex Twin sitting by a grand piano, background made of large folding curtains, dark, hyper detailed, hyper realistic, 8K phot realistic, black and white color, dimly lit, dark,', 'a clearing in a forest with a cabin, Disney cartoon, animation, high detail, colorful', 'Jesus Christ rising out from a tomb in a cliff side, cinematic perspective, movie shot, 8k, full hd', 'cinematic still, blade runner, roger rabbit in a flying delorean, high quality, futuristic', 'teletubbies dressed in adidas in a rave party', 'a painting of hip hop dancers by edgar degas', 'a guy with an orange cap and black shirt sketching graffiti on a blackbook on a green sofa closed to his friend drinking a beer', 'a car made of donuts', 'picture of darth vader by vivian maier', 'a burning dollar bill', 'mcdonalds commercial with the joker', 'rza playing chess with marcel duchamps in cadaques', 'mugshot of the pope', 'a man saying a secret to a computer', 'type made of cows', 'a nightmare', 'a robbery in a toy store', 'a spray paint can burning', 'couche de soleil sur montagne des alpes a l aquarelle', 'a robot whispering to a man', 'picture of a slave robot drawing smileys', 'daft punk pic by vivian maier', 'peace', 'picture by a microscope of a virus with a smiley', 'fractal horse by giger, partially skeleton, partially robot, deep focus, d & d, dark fantasy, intricate glow accents, elegant, highly detailed, digital painting, artstation, concept art, matte, sharp focus, 8 k 3 d, hearthstone, art by artgerm and greg rutkowski and alphonse mucha', 'chinese princess in a long silk dress, pale, beautiful symmetric face, kissing a black giant dragon, fantasy art, highly detailed art, cinematic atmosphere, volumetric lighting, glow, trending on artstation, by wlop, by le vuong, by tom bagshaw', 'fractal lovers by giger, golden ratio, deep focus, d & d, dark fantasy, intricate glow accents, elegant, highly detailed, digital painting, artstation, concept art, matte, sharp focus, octane render, hearthstone, art by artgerm and greg rutkowski and alphonse mucha', 'beautiful sun goddess, in silver armour, full body, porcelain highlighted skin, detailed face with manga style traits, detailed golden hair accessories, glowing pattern on skin, iridescent fractal whirls in flowy hair, passionate pose, intricate, elegant, sharp focus, highly detailed linework, fantasy, concept art, trending on artstation, 3 d 8 k, by artgerm and greg rutkowski, mucha, giger, beksinski, ross tran', 'beautiful sun goddess, in silver flowy dress, porcelain highlighted skin, detailed face with big sad eyes, detailed golden hair accessories, glowing pattern on skin, iridescent fractal whirls in flowy hair, passionate pose, intricate, elegant, sharp focus, highly detailed linework, fantasy, concept art, popart elements, trending on artstation, 3 d 8 k, by artgerm and greg rutkowski, mucha, giger, beksinski, ross tran', 'portrait of 3 women with flowy hair, wings, confident pose, pixie, genshin impact, intricate, elegant, sharp focus, soft bokeh, illustration, highly detailed, concept art, matte, trending on artstation, bright colors, art by wlop and artgerm and greg rutkowski, mucha, giger, marvel comics', 'two ethereal hummingbird goddesses dressed in fractal feathers, beautiful porcelain faces, passionate poses, by kinkade, by giger, shepard fairey, botticelli, john singer sargent, pre - raphaelites, shoujo manga, harajuku fashion, iridescent colors, detailed lineart, delicate glow accents, 8 k 3 d, arnold render', 'beautiful night goddess, in long flowy dress, closeup, porcelain highlighted skin, detailed face with anime style traits, detailed golden hair accessories, glowing pattern on skin, with beautiful horse, iridescent fractal whirls, passionate pose, intricate, elegant, sharp focus, highly detailed linework, trending on artstation, 3 d 8 k, by artgerm and greg rutkowski, mucha, giger, beksinski, ross tran', 'death is swallowed up in victory, very detailed and beautiful womans face, screaming with fear, artwork by artgerm, centered shot, wide angle, full body, elfpunk, artwork by naoto hattori, giger, landscape art by john howe', 'beautiful goddess, in silver flowy dress, porcelain highlighted skin, detailed face with big sad eyes, detailed golden hair accessories, glowing pattern on skin, iridescent fractal whirls in flowy hair, passionate pose, intricate, elegant, sharp focus, highly detailed linework, fantasy, concept art, cyberpunk elements, trending on artstation, 3 d 8 k, by artgerm and greg rutkowski, mucha, giger, beksinski, ross tran', 'ultra realistic mermaid princess closeup, gorgeous symmetric face and body, dramatic pose, glowing eyes, blush skin with freckles, long flowy hair, in the middle of arctic desert, sci - fi, fantasy, intricate, elegant, highly detailed, trending on artstation, concept art, smooth, sharp focus, octane render, dramatic volumetric lighting, inner glow, art by tian zi and artgerm and xiaoguang sun and giger, by wlop', 'beautiful moon goddess in long flowy dress, closeup, porcelain highlighted skin, detailed face with anime style traits, iridescent fractal whirls, passionate pose, intricate, elegant, sharp focus, highly detailed linework, trending on artstation, purple glow, 3 d 8 k, by artgerm and greg rutkowski, mucha, giger, beksinski, ross tran', 'a romantic scene of a young angel girl in love with beautiful long flower hair, flowers, 3 d render, hyper realistic, digital painting, fantasy art, beeple, peter mohrbacher, thomas kinkade', 'fractal lovers by giger, ethereal, passionate pose, ethereal, golden ratio, deep focus, d & d, dark fantasy, intricate purple green glow accents, elegant, highly detailed, digital painting, artstation, devianart, concept art, matte, sharp focus, octane render, hearthstone, art by artgerm and greg rutkowski and alphonse mucha', '3 / 4 view of a portrait of woman with flowy hair, bird wings, confident pose, pixie, genshin impact, intricate, elegant, sharp focus, illustration, highly detailed, concept art, matte, trending on artstation, bright colors, art by wlop and artgerm and greg rutkowski, mucha, marvel comics', 'ultra realistic nymph princess closeup, gorgeous symmetric face and body, dramatic combat pose, glowing eyes, blush skin with freckles, long red flowy hair, in the middle of arctic desert, sci - fi, fantasy, intricate, elegant, highly detailed, trending on artstation, concept art, smooth, sharp focus, octane render, dramatic volumetric lighting, inner glow, art by tian zi and artgerm and xiaoguang sun and giger, by wlop', 'botticelli eve and giger style adam dressed in fractal smoke in eden techno garden, beautiful porcelain faces, passionate poses, by kinkade, by giger, shepard fairey, botticelli, john singer sargent, pre - raphaelites, harajuku fashion, iridescent colors, detailed lineart, delicate neon glow accents, 8 k 3 d, arnold render', 'redhead muse by botticelli & giger, inspired by pre - raphaelites, shoujo manga and harajuku fashion, made of painted carved high - relief, semi - transparent marble, opaque glass, filament, polished mahoganny wood, intricate detail, dark - blue, light - blue, black, gold, silver, black background, kintsugi, realistic, cinematic lighting', 'demonic man and angelic woman in armour, passionate pose, pixie, renaissance impact, intricate, elegant, golden glow, sharp focus, soft bokeh, illustration, highly detailed, concept art, matte, trending on artstation, pastel colors, 3 d 8 k, art by wlop and artgerm and greg rutkowski, mucha, giger, marvel comics, beksinski,', 'ultra realistic cyborg knight princess portrait, gorgeous symmetric face and body, dramatic combat pose, glowing eyes, blush skin with freckles, flowy hair, in the middle of arctic desert, sci - fi, fantasy, intricate, elegant, highly detailed, trending on artstation, concept art, smooth, sharp focus, octane render, dramatic volumetric lighting, inner glow, art by tian zi and artgerm and xiaoguang sun and giger, by wlop', 'beautiful full body portrait of a female cyberpunk gnome black, wearing a fancy velvet tunic, by wlop and artgerm, steampunk! fiction, detailed deep black eyes, starry background, trending, on artstation.', 'photographic portrait by helen levitt of the woman who inspired gioconda, studio lighting, sigma 8 5 mm lens', 'a fully dressed!!! portrait of beautiful ornated hanuman!!!! god with flowing medium hair, soft facial features, kind appearence, digital art by alphonse mucha, inspired by krishen khanna and madhvi parekh, symmetrical body, artgerm, portrait, muted color scheme, highly detailed, outrun art style', 'portrait of sergio aguero with almost no beard by greg rutkowski, young, attractive, highly detailed portrait, scifi, digital painting, artstation, concept art, smooth, sharp foccus ilustration, artstation hq', 'very detailed portrait of a rugged man in his early thirties, strong jaw, deep black eyes, latino features, wearing a black!! t - shirt, earthy color scheme, by wlop and krenz cushart and artgerm, 9 0 s style, detailed eyes, starry background, trending, on artstation.', \"a sculpture of michelangelo's david, realistic charcoal portrait by frank auerbach\", \"a sculpture of a winged angel's torso, very detailed charcoal portrait by frank auerbach\", '! dream dakini as a modern fairy wearing a pink outfit, flying in the style of superman alongside penguins.', 'portrait human - giraffe hybrid, scaley black onyx skin', 'highly detailed, semi - realistic contemporary digital illustration, colored contours, well shaded, portrait of king arthur leaning over a table', 'high fantasy portrait of a red - headed early thirties female fairy, pixie, fae, imp, sprite in vctorian double bun hairstyle and sparkly, gleam baby pink coral outfit, elegant, intricate, highly detailed, smooth, sharp focus, ethereal, misty, fireflies in the backdrop. octane render, pastel color scheme, by hayao miyazaki.', 'award winning full body portrait of a beautiful ornated hanuman god, leaping!!!!!, intricate, elegant, highly detailed, digital painting, artstation, concept art, smooth, sharp focus, digital illustration, art by krenz cushart and artem demura and alphonse mucha', 'apteryx mantelli', 'portrait of bald sergio aguero in mid thirties with gray designer stubble!!!!!!! by greg rutkowski, attractive, highly detailed portrait, scifi, digital painting, artstation, concept art, smooth, sharp foccus ilustration, artstation hq', 'very detailed portrait of a rugged brazilian man in his early thirties, in profile, strong jaw, clean face, light stubble!!! ( ( deep black eyes ) ), detailed, ( very slight asian features ), ( ( ( strong latino features ) ) ), sharp nose pointing down, pastel color scheme, by tyler oulton, starry background, trending, on artstation.', 'if paris was built on the moon', 'a lone indigenous man overlooking a ledge towards the jungle below in dense amazon | highly detailed | very intricate | cinematic lighting | by asher brown durand and eddie mendoza | featured on artstation', 'award winning watercolor of a 3 0 year old auburn - headed fairy in short pigtails wearing a sparkly baby pink swimsuit with blue translucent dragonfly wings, against a cloudy blue sky backdrop, by hayao miyazaki', 'hanumanasana yoga posture, stretching one leg forward and the other straight back, arms stretched above the head, and the palms joined together', 'beautiful profile painting, highly detailed of lord hanuman, the monkey god, doing a front split', 'dwayne johnson playing yao ming in a biopic film, basketball scene, wide shot', 'Coach Belichick studying a football playbook while on vacation at the Eiffel tower', 'Dwayne Johnson as a crusty sea captain', \"film still from the new netflix fantasy adventure movie'chrono trigger'( 2 0 2 2 )\", 'The President of the United States shaking hands with a sinister grey space alien, official portrait', 'a large sea bird covered in trash and filth', 'charlemagne as a new street fighter character, screenshot, character select', \"comic book cover of'jfk meets the underground mole people ', art by alex ross\", 'grainy photo of erin esurance as a creepy monster in a closet, harsh flash', 'dwayne johnson as john madden', 'a photo of a house burning down in the background and mr. bean with an eerie expression in the foreground, strong depth of field', 'Coach Belichick in Edo Period Japan trying to teach samurai how to play football', 'peyton manning as a new street fighter character, screenshot, character select', 'andy reid as doctor who, 1 9 7 0 s, wide shot', 'Optimus Prime as a football coach', 'patrick stewart as usain bolt', 'john madden in interstellar', \"still image from the new studio ghibli animated film'coach tomlin wins the superbowl'\", \"still image from the new studio ghibli animated film'the crying of lot 4 9'\", 'john madden as ian malcom from jurassic park', 'beautiful coherent award-winning manga OVA DVD cover art of a mysterious lonely cyborg anime woman wearing a plugsuit, serial experiments lain, neon genesis evangelion, anime, animated, painted by tsutomu nihei', 'beautiful coherent award-winning manga cover art of a mysterious lonely anime woman wearing a plugsuit and traversing an endless concrete hallway, by tsutomu nihei', 'beautiful portrait of anthony fantano, theneedledrop, standing in desolate empty brutalist ruins desert wasteland, close to the camera, painted by zdzislaw beksinski', 'highly detailed professional seinen manga cover art of goth woman with red hair, red eyes, leather clothes, black makeup. chunibyo. horror action cyberpunk action manga cover promotional art. detailed intricate environment. pencils by ilya kuvshinov, painted by zdzislaw beksinski, inks & layouts by tsutomu nihei. blame!', 'highly detailed professional portrait of 9 0 s seinen manga art of goth woman with red hair, black makeup, and red eyes. chunibyo. drawn by', 'a desolate landscape with a lonely looming brutalist tower in the center, drawn by tsutomu nihei', 'highly detailed professional late 2 0 0 0 s shonen manga cover art of goth woman with red hair, red eyes, leather clothes, black makeup. chunibyo. horror cyberpunk action manga cover promotional art. detailed and intricate environment. pencils by ilya kuvshinov and painted by zdzislaw beksinski, inked by tsutomu nihei', 'a creepy cell phone camera picture of an alleyway in west philadelphia at night, with a woman in the distance.', 'beautiful! coherent! detailed! expert! professional portrait art of a goth clowngirl, painted by ilya kuvshinov!!! and zdzislaw beksinski', 'beautiful! coherent! detailed! expert! professional manga seinen portrait art of an emo goth jester clowngirl, painted by ilya kuvshinov!!! and designed by tsutomu nihei and zdzislaw beksinski', 'a portrait of a depressed girl made in a magazine clipping collage style, made by a depressed art student, art project', 'professionally drawn 9 0 s seinen seinen seinen mature cyberpunk horror action manga comic cover, full color, beautifully drawn coherent professional, drawn by ilya kuvshinov, ilya kuvshinov, and hiromu arakawa and tsutomu nihei. japanese script on the cover. stern woman in foreground. award - winning manga.', 'a desolate landscape dotted with brutalist complexes, drawn by tsutomu nihei,', 'a beautiful professional portrait of mc ride, painted by tsutomu nihei', 'professionally drawn seinen mature cyberpunk detective horror action manga comic cover, full color, beautifully drawn coherent professional, drawn by ilya kuvshinov!, satoshi kon, kentaro miura, dave mckean, tsutomu nihei. japanese script kanji hiragana on the cover. minimalist stylized cover art. indigo blue cel shaded', 'a portrait of dril painted by zdzislaw beksinski', '9 0 s seinen horror manga cover art', 'professionally drawn shoujo mature horror mystery romance manga comic cover, beautifully drawn museum portrait coherent professional, drawn by ilya kuvshinov, gustav klimt, alphonse mucha and tsutomu nihei. japanese script kanji hiragana on the cover. simplistic minimalist stylized cover art. pink & green & blue full color.', 'adorable film still of a piece of garlic, produced by studio ghibli', 'professionally drawn shoujo mature cyberpunk horror romance manga comic cover full color, beautifully drawn coherent professional, drawn by ilya kuvshinov, ilya kuvshinov!, dave mckean, alphonse mucha and tsutomu nihei. japanese script kanji hiragana on the cover. simplistic minimalist stylized cover art. pink & purple & blue full color.', 'close up of nicolas cage staring deep into the camera, tears pouring down his face, big toothy smile, fisheye lens', 'Abraham Lincoln wearing a flat brim cap backwards, taking a hit from a Juul e-cigarette, portrait photography by Lee Jeffries', 'elderly gene wilder playing gandalf, still from lord of the rings ( 2 0 0 3 )', 'still of gollum from lord of the rings devouring a juicy delicious burger', 'cinestill of a large blonde labradoodle skiing down a snowy mountain', 'cinestill of a blonde labradoodle working in a cubicle, wearing glasses and a dress, photograph by annie leibovitz', 'cinematic shot of adorable black kitten walking through neon cyberpunk neo - tokyo, studio ghibli, hayao miyazaki, anime, detailed', 'still of a woman riding on a giant dachshund, sports photography, action photography', 'cinestill of jfk putting on a helmet', 'still of scarlett johansson staring angrily into the camera, close - up shot of eyes, by annie leibovitz, kodak portra 4 0 0, 5 0 mm f / 1. 8', 'action shot of obese people swimming in gravy pool, hot brown thick gravy, sports photography, espn, summer olympics', 'colorful concept art of gandalf fighting the balrog, rodel gonzalez, marc davis, milt kahl, jim warren, don bluth, rob kaz, glen keane, jason deamer', 'movie still of evan rachel wood pointing gun at young harrison ford in blade runner ( 1 9 8 2 ), portrait, film grain, atmospheric lighting, action shot, low profile, wide angle', 'matte painting of epic fantasy landscape with golden hour lighting, colorful concept art by ted nasmith, john howe, alan lee, paul raymond gregory, inger edelfeldt, tim kirk, angus mcbride, jenny dolfen, high fantasy, trending on artstation', 'idyllic underwater scene with sunbeam shining through water, oil painting, Impressionism, in the style of Claude Monet, 4K, trending on ArtStation', 'faded daguerreotype portrait of disturbing haunted demonic abomination clown body horror', 'movie still of evan rachel wood talking to young harrison ford in blade runner ( 1 9 8 2 ) by ridley scott, portrait, film grain, atmospheric lighting, action shot', 'gandalf flying on a giant eagle over mount doom while it erupts lava, vivid concept art by ted nasmith, john howe, alan lee, paul raymond gregory, inger edelfeldt, tim kirk, angus mcbride, jenny dolfen, high fantasy, trending on artstation', 'still of jfk dodging bullets as neo in the matrix ( 1 9 9 9 )', 'faded daguerreotype of a creepy old doll, uncanny valley, disturbing', 'Evolve Squirtle into Charizard', 'Cat Women crouches on edge of building, in style of Mark Brooks and Artgerm', 'Hyperdetailed digital artwork concept art of Night island and the aurora borealis reflected in the dark sapphire water in style of Albert Bierstadt and Jim Burns, 4k resolution post-processing, Octane Render, Unreal Engine 5, Global Illumination, smooth, epic composition, cinematic shot', 'Poison Ivy entangled with Vines, in style of Mark Brooks and Artgerm', 'Beautiful Elven Princess Portrait, fantasy style, intricate', 'Cyberpunk Android design drawn by a manga artist, in style of Akira Toriyama and Gustave Dore, cinematic shot', 'a beautiful painting of a building in a serene landscape by Adonna Khare', 'a beautiful painting of a A paradisiacal landscape of a sea lagoon and city built on water, rays of light illuminating the water by John Howe, Trending on Artstation, Landscape vista', 'a beautiful painting of a building in a serene landscape by Alexander Milne Calder', 'a portrait of princess jasmine, in style of Bowater Charlie and Chausheva Katia', 'a portrait of princess jasmine, in style of Bowater Charlie and Krentz Cushart', 'plasma body, anime spectral female character, emerge from big old creepy tree, mist aura, black eyes melt, full body portrait, photorealistic, volumetric lighting, octane rendering, dark and mysterious, atmospheric, ominous, creepy, cinematic, real, concept art, Epic, 8k, 4k, ultra detail, ultra realistic, trading art station, rendered by awesomeness', 'Athene Roman Statue, Intricate Details, 8k resolution', 'A Beautiful digital artwork of the A bunch of goblins in the basket of a travel balloon, war and battle, in style by Dan Mumford, Cyril Rolando and M.W Kaluta, 8k resolution, Ultrafine details, Rendered in Unreal Engine 5, Cinematic Composition, Reimagined by industrial light and magic, smooth,4k, beautiful lighting, HDR, IMAX, Cinema 4D, shadow depth', 'a beautiful painting of a building in a serene landscape by Anton Pieck', 'multi dimension tesseract hybrids with impossible non Euclidian geometry', 'philippina woman tattoos on body dramatic beach cinematic photorealistic sunset', 'a beautiful painting of a building in a serene landscape by Anton Otto Fischer', \"a painting of a woman's face with a sky background, a comic book panel by makoto shinkai, featured on pixiv, crystal cubism, stained glass, made of glass, official art\", 'strange horror house by junji ito, hugh ferriss, lee madgwick, alex grey and gustave dore ; spiralled blood red and smoke black art nouveau architecture ; in the style of gothic art, elaborate horror house by wes benscoter, weird, beautiful, gorgeous, incredible depth, concept art, lifelike, photorealistic, imposing, evil, biblical hell, 8 k resolution, hyperrealism, detailed painting, deviantart, trending on artstation, unreal engine', 'jared leto in the style of junji ito', 'karkalicious', 'fingers in a blender', 'negative film portrait of a woman', 'goth girlfriend', 'short spear with a spike at the bottom of the handle', 'dave strider in the style of ancient egyptian artwork', 'photograph of a human heart laying in a forest full of dead trees', 'bloodstained sakura flowers', 'humanoid barn owl wearing a roman toga and holding a javelin', 'man covered in teeth', 'morbius in ancient rome', 'jared leto on the cover of the weezer blue album', 'eyes outside your window at night', 'lin manuel miranda in the style of junji ito', 'elder scrolls 6 new game leaked footage', 'jared leto in outer space', 'frog wedding', 'train wearing a suit', 'Biblically accurate muppet in stained glass at a church being worshipped', 'thief, dagger, leather armor, full body, hyper realistic, extremely detailed, dnd character art portrait, dark fantasy art, intricate fantasy painting, dramatic lighting, vivid colors, deviantart, artstation, by edgar maxence and caravaggio and michael whelan and delacroix.', 'fantasy illustration of a manticore at a tea party with a gnome wizard and and ogre knight of the realm.  Setting is a forest.  Table is a tree stump with a kettle on top', \"enchanted forest. rocky opening gaping hole in the ground ( 1 5'across, 2 0'deep ). misty waterfall ( stream cascades underground ). rocky sides ( lush patches of moss and ferns ). rough stone stairway ( cut into the hole's rocky side, leading down ). edgar maxence and caravaggio and michael whelan and delacroix style, artistic, intricate painting, cinematic lighting, hyper realistic, extremely detailed, vivid colors, establishing shot, dramatic lighting\", 'barbarian, full body, savage, realistic, dnd character art portrait, dark fantasy art, matte fantasy painting, deviantart artstation, by jason felix by steve argyle by tyler jacobson by edgar maxence and caravaggio and michael whelan and delacroix', 'pinhole black and white photo of a victorian living room with costumed people dancing', 'stone sarcophagus. dnd, dark fantasy art, intricate fantasy painting, dramatic lighting, vivid colors, deviantart, artstation, by edgar maxence and caravaggio and michael whelan and delacroix.', 'fantasy illustration of giant mutant frogs', 'a painting by edgar maxence and caravaggio and michael whelan and delacroix style, artistic, intricate painting, cinematic lighting, hyper realistic, extremely detailed, vivid colors, establishing shot, dramatic lighting', 'elf bard playing lute, full body, hyper realistic, extremely detailed, dnd character art portrait, dark fantasy art, intricate fantasy painting, dramatic lighting, vivid colors, deviantart, artstation, by edgar maxence and caravaggio and michael whelan and delacroix.', 'gelatinous cube, realistic, dnd monster, dark fantasy art, matte fantasy painting, deviantart artstation, by jason felix by steve argyle by tyler jacobson by peter mohrbacher by paul hedley, cinema,', 'wizard casting thunderwave, dnd character art portrait, dark fantasy art, deviantart, artstation, by edgar maxence and caravaggio and michael whelan and delacroix.', 'tiefling bard, full body, hyper realistic, extremely detailed, dnd character art portrait, dark fantasy art, intricate fantasy painting, dramatic lighting, vivid colors, deviantart, artstation, by edgar maxence and caravaggio and michael whelan.', \"the small gnome's garb is a riot of color, embroidered in gold thread. atop his head, a triangular hat of green felt sits at a rakish angle. emerald eyes gleam with mirth, and his smiling face is framed by a tangle of black hair. his brass instruments - - hand - bells and a panpipe\", \"jim henson's labyrinth. hedge maze. orchardlush orchard, packed with strange fruit trees. fairies flit from tree to tree, fighting with hummingbirds for nectar. 4 night trolls are beneath the largest tree. edgar maxence and caravaggio and michael whelan and delacroix style, artistic, intricate painting, cinematic lighting, hyper realistic, extremely detailed, vivid colors, establishing shot, dramatic lighting\", 'druid. young man with braided brown hair. wildflowers in his hair. amber eyes. leather armor, and a longbow. wooden staff carved with strange symbols. edgar maxence and caravaggio and michael whelan and delacroix style, artistic, intricate painting, cinematic lighting, hyper realistic, extremely detailed, vivid colors, establishing shot, dramatic lighting', 'pinhole camera photo of a witch flying on a broomstick', 'spelljammer galleon sailing the galaxy by clyde caldwell', 'gnome barbarian, full body, hyper realistic, extremely detailed, dnd character art portrait, dark fantasy art, intricate fantasy painting, dramatic lighting, vivid colors, deviantart, artstation, by clyde caldwell and krenz cushart and artem demura and john williams waterhouse', 'black and white illustration by erol otus the pack of kobolds is crouched in a circle.', 'harpy, dnd character art portrait, dramatic lighting, vivid colors by edgar maxence and caravaggio.', 'detailed hipster skinny man wearing big vr head set, long vibrant beard, dmt, by james gurney + intricate and vibrant work + portrait + trending on artstation + incredible gothic illustration + exquisite detail', 'green dragon detailed pixel art', 'arabian hassan as - sabbah on throne chair rising as a phenix + epic wide scene, cinematic lighting, artgerm, artstation, deviantart, 8 k, high detailed', 'rabbit groot as marble statue with sunglasses, blue sunglasses, in red background, soft blue texture, blue realistic 3 d render, high blue lights, 4 k, high detailed photography cape, 5 0 mm lens, rich blue colors, smooth gradients, depth of field, cinematic, hyper realism, high detail, octane render, unreal engine, 8 k very red colors, cape', 'detailed hipster skinny man wearing htc vive headset, long vibrant beard, dmt, by james gurney + intricate and vibrant work + portrait + trending on artstation + incredible gothic illustration + exquisite detail', 'detailed hipster skinny man with sunglasses, long beard with fires, dmt, by james gurney + intricate and vibrant work + portrait + trending on artstation + incredible gothic illustration + exquisite detail', 'detailed umm kulthum golden statue, by james gurney + intricate and vibrant gold line work + tarot card + mandelbulb fractal + full of black layers + portrait + trending on artstation + incredible gold and black gothic illustration + exquisite detail', 'detailed hipster skinny man with! vr headset!, long vibrant beard, dmt, by james gurney + intricate and vibrant work + portrait + trending on artstation + incredible gothic illustration + exquisite detail', 'rabbit groot as marble statue, red sunglasses, in red background, soft red texture, red realistic 3 d render, soft red lights, 4 k, high red photography red, 5 0 mm lens, rich red colors, smooth gradients, depth of field, cinematic, hyper realism, high detail, octane render, very red, 8 k, very red colors', 'detailed image of Anaximander by Ayami Kojima, Amano, Karol Bak, Greg Hildebrandt, and Mark Brooks, rich deep purple colors. Beksinski painting, part by Adrian Ghenie and Gerhard Richter. art by Takato Yamamoto. masterpiece . intricate artwork by Tooth Wu and wlop and beeple, greg rutkowski, very coherent symmetrical artwork, cinematic, hyper realism, high detail, octane render, unreal engine, 8k, Vibrant colors, Smooth gradients, High contrast, depth of field. by Katsuhiro Otomo, full body character drawing, inspired by Evangeleon, clean ink detailed line drawing, intricate detail, extremely detailed. painting by Arthur Rackham, Eugene de Blaas, Frederic Leighton', 'detailed mighty dmt goddess, by hokusai and james gurney + black paper with intricate and vibrant dmt line work + tarot card + mandelbulb fractal + full of silver layers + portrait + trending on artstation + incredible dmt and black gothic illustration + exquisite detail', 'unicorn wearing vr headset, vr headset in techno background, soft gradient texture, realistic 3 d render, high lights, 4 k, high detailed photography, 5 0 mm lens, rich vivid colors, smooth gradients, depth of field, cinematic, hyper realism, high detail, octane render, unreal engine, 8 k', 'cairo old streets + night life of 1 9 4 0, muizz street + egyptian muslim girl wearing egyptian hijab', 'a detailed fantasy character portrait of morgan freeman as saudi arab king by lauri blank, artgerm, evelyn de morgan, 8K, 50mm lens', 'detailed image of bots by Ayami Kojima, Amano, Karol Bak, Greg Hildebrandt, and Mark Brooks, rich deep universe colors. Beksinski painting, part by Adrian Ghenie and Gerhard Richter. art by Takato Yamamoto. masterpiece . intricate artwork by Tooth Wu and wlop and beeple, greg rutkowski, very coherent symmetrical artwork, cinematic, hyper realism, high detail, octane render, unreal engine, 8k, Vibrant colors, Smooth gradients, High contrast, depth of field. by Katsuhiro Otomo, full body character drawing, inspired by Evangeleon, clean ink detailed line drawing, intricate detail, extremely detailed. painting by Arthur Rackham, Eugene de Blaas, Frederic Leighton', 'detailed portrait of pirate cat as claw video game, hyper detailed, digital art, trending in artstation, cinematic lighting, studio quality, smooth render, unreal engine 5 rendered, octane rendered, art style by klimt and nixeu and ian sprigger and wlop and krenz cushart', 'rabbit groot as marble statue with sunglasses, blue sunglasses, in red background, soft blue texture, blue cape, blue realistic 3 d render, high blue lights, 4 k, high detailed photography, 5 0 mm lens, rich blue colors, smooth gradients, depth of field, cinematic, hyper realism, high detail, octane render, unreal engine, 8 k', 'very detailed portrait of skull with pearl spheres glowing eyes + melting face skin as candles + 4 k hyper details render + dramatic lighting + cinematography photography', 'face bleeding gold liquid, realistic 3 d render, gold tones, global illumination', 'under water bugs bunny, water light scattering, underwater photography, high details, 8 k, realistic shot, cinematic lighting', 'Movie still of Diane Kruger in The Expanse', 'Sheryl Sandberg in Got Milk? Ad', 'Mark Zuckerberg marble statue by Michelangelo, Metropolitan Museum of Fine Art, 24mm f/1.4', 'Film still of Sheryl Sandberg in SoulCycle (2017), directed by Steven Spielberg', 'Movie still of Mark Zuckerberg taking cover on Omaha Beach in Saving Private Ryan, establishing shot, smoke, gritty, action photo', 'CEO Tony Haile', 'Photo of Emma Watson in swimsuit, soft studio lighting, photo taken by Julia Margaret Cameron for Abercrombie and Fitch, award-winning photograph, 24mm f/1.4', 'Photo of Angelina Jolie wearing Warby Parker glasses, soft studio lighting, photo taken by Martin Schoeller for Abercrombie and Fitch, award-winning photo, 24mm f/1.4', 'Sheryl Sandberg at SoulCycle, Instagram photo', 'Portrait photo of dominatrix Sheryl Sandberg, 85mm f/1.4', 'taylor swift emoji', 'Sheryl Sandberg as Overly Attached Girlfriend', 'Photo of Joe Biden in swimsuit, soft studio lighting, photo taken by Anne Liebovitz for Abercrombie and Fitch, award-winning photograph, 24mm f/1.4', 'Photo of Sam Altman, OpenAI CEO, at Folsom Street Fair, by Anne Liebovitz, 85mm f/1.4', 'Portrait photo of Mark Zuckerberg, photographed by Anne Geddes, soft studio lighting, 85mm f/1.4', 'Portrait photo of a baby Mark Zuckerberg with flower, photographed by Anne Geddes', 'Photo of Ryan Gosling in swimsuit, soft studio lighting, photo taken by Anne Liebovitz for Abercrombie and Fitch, award-winning photograph, 24mm f/1.4', 'sheryl sandberg, medal of honor', 'Movie still of Sheryl Sandberg imprisoned in Supermax in Facebook The Movie (2017), directed by Steven Spielberg', 'Movie still of Emma Watson in The Matrix']\n",
        "\n",
        "test_y = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 18, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 21, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 38, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 41, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 49, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 51, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 69, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 78, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 84, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 85, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 88, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 89, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91, 91]\n",
        "\n",
        "# unique_authors = [ 2 10 15 18 21 25 31 38 41 45 49 51 69 72 78 84 85 88 89 91]\n",
        "unique_authors = list(set(test_y))\n",
        "color_map = {2: (0.12156862745098039, 0.4666666666666667, 0.7058823529411765, 1.0), 10: (0.6823529411764706, 0.7803921568627451, 0.9098039215686274, 1.0), 15: (1.0, 0.4980392156862745, 0.054901960784313725, 1.0), 18: (1.0, 0.7333333333333333, 0.47058823529411764, 1.0), 21: (0.17254901960784313, 0.6274509803921569, 0.17254901960784313, 1.0), 25: (0.596078431372549, 0.8745098039215686, 0.5411764705882353, 1.0), 31: (0.8392156862745098, 0.15294117647058825, 0.1568627450980392, 1.0), 38: (1.0, 0.596078431372549, 0.5882352941176471, 1.0), 41: (0.5803921568627451, 0.403921568627451, 0.7411764705882353, 1.0), 45: (0.7725490196078432, 0.6901960784313725, 0.8352941176470589, 1.0), 49: (0.5490196078431373, 0.33725490196078434, 0.29411764705882354, 1.0), 51: (0.7686274509803922, 0.611764705882353, 0.5803921568627451, 1.0), 69: (0.8901960784313725, 0.4666666666666667, 0.7607843137254902, 1.0), 72: (0.9686274509803922, 0.7137254901960784, 0.8235294117647058, 1.0), 78: (0.4980392156862745, 0.4980392156862745, 0.4980392156862745, 1.0), 84: (0.7803921568627451, 0.7803921568627451, 0.7803921568627451, 1.0), 85: (0.7372549019607844, 0.7411764705882353, 0.13333333333333333, 1.0), 88: (0.8588235294117647, 0.8588235294117647, 0.5529411764705883, 1.0), 89: (0.09019607843137255, 0.7450980392156863, 0.8117647058823529, 1.0), 91: (0.6196078431372549, 0.8549019607843137, 0.8980392156862745, 1.0)}\n",
        "\n",
        "\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "\n",
        "\n",
        "\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_try2/content_encoder_supcon_9.pt'\n",
        "\n",
        "import torch\n",
        "from transformers import BertGenerationEncoder, BertConfig\n",
        "\n",
        "# Load your model's state_dict from the .pt file\n",
        "state_dict = torch.load(ckpt_path)\n",
        "\n",
        "# Reinitialize the model with the same configuration used during training\n",
        "# You can load a default or custom configuration if needed\n",
        "config = BertConfig.from_pretrained('bert-base-cased')  # or use your specific configuration\n",
        "model = BertGenerationEncoder(config)\n",
        "\n",
        "# Load the weights into the model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# The model is now loaded and ready for inference or further training\n",
        "\n",
        "df = pd.DataFrame({'Target': test_y, 'content': test_x})\n",
        "\n",
        "# model = BertGenerationEncoder.from_pretrained(ckpt_path)\n",
        "# model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "# model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  # x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  outputs = model(x1, attention_mask=x3)\n",
        "  # all_feats.append(outputs.last_hidden_state.flatten(1).cpu().detach().numpy())\n",
        "  # all_labels.append(y.cpu().detach().numpy())\n",
        "  all_feats.append(outputs.last_hidden_state.flatten(1).detach().numpy())\n",
        "  all_labels.append(y.detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "\n",
        "# Create a color map for authors\n",
        "# unique_authors = df['Target'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "xvlL_kEC47L_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    EncoderDecoderModel,\n",
        "    BertGenerationDecoder,\n",
        "    BertGenerationEncoder,\n",
        "    BertTokenizer,\n",
        "    BertModel)\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "print(test_x)\n",
        "print(test_y)\n",
        "\n",
        "# from transformers import BertTokenizer, BertModel\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "# extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "# num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "# test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "# test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "#                              pin_memory=True)\n",
        "\n",
        "# pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "# ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "# num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "# model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "\n",
        "\n",
        "\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_try2/content_encoder_supcon_9.pt'\n",
        "\n",
        "import torch\n",
        "from transformers import BertGenerationEncoder, BertConfig\n",
        "\n",
        "# Load your model's state_dict from the .pt file\n",
        "state_dict = torch.load(ckpt_path)\n",
        "\n",
        "# Reinitialize the model with the same configuration used during training\n",
        "# You can load a default or custom configuration if needed\n",
        "config = BertConfig.from_pretrained('bert-base-cased')  # or use your specific configuration\n",
        "model = BertGenerationEncoder(config)\n",
        "\n",
        "# Load the weights into the model\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# The model is now loaded and ready for inference or further training\n",
        "\n",
        "\n",
        "# model = BertGenerationEncoder.from_pretrained(ckpt_path)\n",
        "# model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "# model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  # x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  outputs = model(x1, attention_mask=x3)\n",
        "  # all_feats.append(outputs.last_hidden_state.flatten(1).cpu().detach().numpy())\n",
        "  # all_labels.append(y.cpu().detach().numpy())\n",
        "  all_feats.append(outputs.last_hidden_state.flatten(1).numpy())\n",
        "  all_labels.append(y.numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "\n",
        "# Create a color map for authors\n",
        "# unique_authors = df['Target'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "HmuKi-P84o2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=20, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "# model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_supcon_para_bert-base-cased_coe1_temp0.1_unit2_epoch30/diffusiondb100_supcon_para_val0.72321_e29.pt'\n",
        "model = BertModel.from_pretrained('bert-base-cased')\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  feature = model(input_ids=x[0], attention_mask=x[2])\n",
        "  feats = feature.last_hidden_state.flatten(1)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# # Create a color map for authors\n",
        "# unique_authors = df['user_name'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# # Plot t-SNE results\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     indices = df['user_name'] == author\n",
        "#     plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "# plt.title(\"t-SNE Visualization of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# # Step 4: Calculate Cluster Centers\n",
        "# cluster_centers = df.groupby('user_name')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# # Plot cluster centers with the same color coding\n",
        "# plt.figure(figsize=(12, 8))\n",
        "# for author in unique_authors:\n",
        "#     plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "# plt.title(\"Cluster Centers of Prompts\")\n",
        "# plt.xlabel(\"t-SNE Dimension 1\")\n",
        "# plt.ylabel(\"t-SNE Dimension 2\")\n",
        "# # plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "# plt.show()\n",
        "\n",
        "# Create a color map for authors\n",
        "# unique_authors = df['Target'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "aIRYq0Io6pXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQM0lOyKEiMr"
      },
      "outputs": [],
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_cls_para_bert-base-cased_coe0.0_temp0.1_unit2_epoch30/diffusiondb100_cls_para_val0.72073_e24.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "\n",
        "# Create a color map for authors\n",
        "# unique_authors = df['Target'].unique()\n",
        "# colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "# color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## other tsne plot"
      ],
      "metadata": {
        "id": "guaZgXbO-buz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_para_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_para_val0.72073_e29.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "VboNG-R8bWdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe2_para_bert-base-cased_coe2.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe2_para_val0.73512_e24.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "JJBdRazdJR-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_supcon_para_maskn_bert-base-cased_coe1_temp0.1_unit2_epoch30/diffusiondb100_supcon_para_maskn_val0.67560_e26.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "SCnda6Uy07y6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lclonly_coe1_para_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lclonly_coe1_para_val0.73859_e27.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "Brd-amKVwQcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## tsne 2"
      ],
      "metadata": {
        "id": "pBOSJp0cqbGJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fix the color code of each author\n",
        "# find cluster center of training data\n",
        "# show the embeddings of text data - circle for correctly classified data, triangle for wrongly classified data\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "# Load train data\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/train_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "df1 = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "df1 = df1[['prompt', 'user_name']]\n",
        "df1.columns = ['content', 'Target']\n",
        "\n",
        "selected_authors = np.random.choice(df['Target'].unique(), size=50, replace=False)\n",
        "df = df[df['Target'].isin(selected_authors)]\n",
        "df1 = df1[df1['Target'].isin(selected_authors)]\n",
        "\n",
        "train_x, train_y = df['content'].tolist(), df['Target'].tolist()\n",
        "test_x, test_y = df1['content'].tolist(), df1['Target'].tolist()\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('hsv', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "print(color_map.keys())"
      ],
      "metadata": {
        "id": "DhFbIzSHyNai"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "train_set = BertDataset(train_x, train_y, tokenizer, num_tokens)\n",
        "train_loader = DataLoader(train_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe2_para_bert-base-cased_coe2.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe2_para_val0.73512_e24.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    pg = tqdm(train_loader, leave=False, total=len(train_loader))\n",
        "    all_feats = []\n",
        "    all_labels = []\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "      x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "      pred, feats = model(x, return_feat=True)\n",
        "      all_feats.append(feats.cpu().detach().numpy())\n",
        "      all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pg1 = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "    all_feats1 = []\n",
        "    all_labels1 = []\n",
        "    all_preds1 = []\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg1):  # for x1, x2, x3, y in train_set:\n",
        "      x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "      pred, feats = model(x, return_feat=True)\n",
        "      all_feats1.append(feats.cpu().detach().numpy())\n",
        "      all_labels1.append(y.cpu().detach().numpy())\n",
        "      pred = torch.argmax(pred, dim=1)\n",
        "      all_preds1.append(pred.cpu().numpy())\n",
        "\n",
        "all_feats1 = np.concatenate(all_feats1, axis=0)\n",
        "all_labels1 = np.concatenate(all_labels1, axis=0)\n",
        "all_preds1 = np.concatenate(all_preds1, axis=0)\n",
        "print(all_labels1)\n",
        "print(all_preds1)\n",
        "\n",
        "all_feats_all = np.vstack((all_feats, all_feats1))\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats_all)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "train_tsne = best_tsne[: len(all_feats)]\n",
        "test_tsne = best_tsne[len(all_feats): ]\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = train_tsne[:, 0]\n",
        "df['tsne_2'] = train_tsne[:, 1]\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min() - 5\n",
        "tsne_1_max = df['tsne_1'].max() + 5\n",
        "tsne_2_min = df['tsne_2'].min() - 5\n",
        "tsne_2_max = df['tsne_2'].max() + 5\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=30, color=color_map[author], marker='x')\n",
        "\n",
        "count = 0\n",
        "for i, (x, y_true, y_pred) in enumerate(zip(test_tsne, all_labels1, all_preds1)):\n",
        "    if y_true == y_pred:\n",
        "        count += 1\n",
        "        plt.scatter(x[0], x[1], color=color_map[y_true], s=30, marker='o')\n",
        "    else:\n",
        "        plt.scatter(x[0], x[1], color=color_map[y_true], s=30, marker='^')\n",
        "\n",
        "print('acc', count / len(all_labels1))\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pV1lMkmXqSiF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "train_set = BertDataset(train_x, train_y, tokenizer, num_tokens)\n",
        "train_loader = DataLoader(train_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = \"/content/drive/MyDrive/msc_project/model/contrastive/club/style_encoder_1.pt\"\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pg = tqdm(train_loader, leave=False, total=len(train_loader))\n",
        "    all_feats = []\n",
        "    all_labels = []\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "      x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "      pred, feats = model(x, return_feat=True)\n",
        "      all_feats.append(feats.cpu().detach().numpy())\n",
        "      all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pg1 = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "    all_feats1 = []\n",
        "    all_labels1 = []\n",
        "    all_preds1 = []\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg1):  # for x1, x2, x3, y in train_set:\n",
        "      x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "      pred, feats = model(x, return_feat=True)\n",
        "      all_feats1.append(feats.cpu().detach().numpy())\n",
        "      all_labels1.append(y.cpu().detach().numpy())\n",
        "      pred = torch.argmax(pred, dim=1)\n",
        "      all_preds1.append(pred.cpu().numpy())\n",
        "\n",
        "all_feats1 = np.concatenate(all_feats1, axis=0)\n",
        "all_labels1 = np.concatenate(all_labels1, axis=0)\n",
        "all_preds1 = np.concatenate(all_preds1, axis=0)\n",
        "print(all_labels1)\n",
        "print(all_preds1)\n",
        "\n",
        "all_feats_all = np.vstack((all_feats, all_feats1))\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats_all)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "train_tsne = best_tsne[: len(all_feats)]\n",
        "test_tsne = best_tsne[len(all_feats): ]\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = train_tsne[:, 0]\n",
        "df['tsne_2'] = train_tsne[:, 1]\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min() - 5\n",
        "tsne_1_max = df['tsne_1'].max() + 5\n",
        "tsne_2_min = df['tsne_2'].min() - 5\n",
        "tsne_2_max = df['tsne_2'].max() + 5\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=30, color=color_map[author], marker='x')\n",
        "\n",
        "count = 0\n",
        "for i, (x, y_true, y_pred) in enumerate(zip(test_tsne, all_labels1, all_preds1)):\n",
        "    if y_true == y_pred:\n",
        "        count += 1\n",
        "        plt.scatter(x[0], x[1], color=color_map[y_true], s=30, marker='o')\n",
        "    else:\n",
        "        plt.scatter(x[0], x[1], color=color_map[y_true], s=30, marker='^')\n",
        "\n",
        "print('acc', count / len(all_labels1))\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9js4x92B0rWv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "train_set = BertDataset(train_x, train_y, tokenizer, num_tokens)\n",
        "train_loader = DataLoader(train_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_supcon_para_bert-base-cased_coe1_temp0.1_unit2_epoch30/diffusiondb100_supcon_para_val0.72321_e29.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    pg = tqdm(train_loader, leave=False, total=len(train_loader))\n",
        "    all_feats = []\n",
        "    all_labels = []\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "      x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "      pred, feats = model(x, return_feat=True)\n",
        "      all_feats.append(feats.cpu().detach().numpy())\n",
        "      all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "with torch.no_grad():\n",
        "    pg1 = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "    all_feats1 = []\n",
        "    all_labels1 = []\n",
        "    all_preds1 = []\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg1):  # for x1, x2, x3, y in train_set:\n",
        "      x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "      pred, feats = model(x, return_feat=True)\n",
        "      all_feats1.append(feats.cpu().detach().numpy())\n",
        "      all_labels1.append(y.cpu().detach().numpy())\n",
        "      pred = torch.argmax(pred, dim=1)\n",
        "      all_preds1.append(pred.cpu().numpy())\n",
        "\n",
        "all_feats1 = np.concatenate(all_feats1, axis=0)\n",
        "all_labels1 = np.concatenate(all_labels1, axis=0)\n",
        "all_preds1 = np.concatenate(all_preds1, axis=0)\n",
        "print(all_labels1)\n",
        "print(all_preds1)\n",
        "\n",
        "all_feats_all = np.vstack((all_feats, all_feats1))\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats_all)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "train_tsne = best_tsne[: len(all_feats)]\n",
        "test_tsne = best_tsne[len(all_feats): ]\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = train_tsne[:, 0]\n",
        "df['tsne_2'] = train_tsne[:, 1]\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min() - 5\n",
        "tsne_1_max = df['tsne_1'].max() + 5\n",
        "tsne_2_min = df['tsne_2'].min() - 5\n",
        "tsne_2_max = df['tsne_2'].max() + 5\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=30, color=color_map[author], marker='x')\n",
        "\n",
        "count = 0\n",
        "for i, (x, y_true, y_pred) in enumerate(zip(test_tsne, all_labels1, all_preds1)):\n",
        "    if y_true == y_pred:\n",
        "        count += 1\n",
        "        plt.scatter(x[0], x[1], color=color_map[y_true], s=30, marker='o')\n",
        "    else:\n",
        "        plt.scatter(x[0], x[1], color=color_map[y_true], s=30, marker='^')\n",
        "\n",
        "print('acc', count / len(all_labels1))\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uxPcxb8Z9_5Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "# df = df[['prompt', 'user_name']]\n",
        "# df.columns = ['content', 'Target']\n",
        "\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_try/style_encoder_supcon_8.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_feats = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_feats.append(feats.cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_feats = np.concatenate(all_feats, axis=0)\n",
        "all_labels = np.concatenate(all_labels, axis=0)\n",
        "\n",
        "\n",
        "# Step 3: Visualize using t-SNE\n",
        "n_iter = 1  # Number of iterations\n",
        "best_kl_divergence = float('inf')\n",
        "best_tsne = None\n",
        "\n",
        "for i in range(n_iter):\n",
        "    tsne = TSNE(n_components=2, random_state=i, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(all_feats)\n",
        "    kl_divergence = tsne.kl_divergence_\n",
        "    print(f\"Iteration {i + 1}: KL-Divergence = {kl_divergence}\")\n",
        "\n",
        "    if kl_divergence < best_kl_divergence:\n",
        "        best_kl_divergence = kl_divergence\n",
        "        best_tsne = X_tsne\n",
        "\n",
        "# Step 6: Add the best t-SNE results to DataFrame\n",
        "df['tsne_1'] = best_tsne[:, 0]\n",
        "df['tsne_2'] = best_tsne[:, 1]\n",
        "\n",
        "\n",
        "# Create a color map for authors\n",
        "unique_authors = df['Target'].unique()\n",
        "colors = plt.cm.get_cmap('tab20', len(unique_authors))\n",
        "color_map = {author: colors(i) for i, author in enumerate(unique_authors)}\n",
        "\n",
        "# Calculate the range for the t-SNE plot\n",
        "tsne_1_min = df['tsne_1'].min()\n",
        "tsne_1_max = df['tsne_1'].max()\n",
        "tsne_2_min = df['tsne_2'].min()\n",
        "tsne_2_max = df['tsne_2'].max()\n",
        "\n",
        "# Plot t-SNE results\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    indices = df['Target'] == author\n",
        "    plt.scatter(df.loc[indices, 'tsne_1'], df.loc[indices, 'tsne_2'], label=author, s=10, color=color_map[author])\n",
        "\n",
        "plt.xlim(tsne_1_min, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"t-SNE Visualization of Prompts\", fontsize=16)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=14)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.savefig('tsne_visualization_of_prompts.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Calculate Cluster Centers\n",
        "cluster_centers = df.groupby('Target')[['tsne_1', 'tsne_2']].mean()\n",
        "\n",
        "# Plot cluster centers with the same color coding\n",
        "plt.figure(figsize=(10, 6))\n",
        "for author in unique_authors:\n",
        "    plt.scatter(cluster_centers.loc[author, 'tsne_1'], cluster_centers.loc[author, 'tsne_2'], label=author, s=100, color=color_map[author], marker='x')\n",
        "\n",
        "plt.xlim(-70, tsne_1_max)\n",
        "plt.ylim(tsne_2_min, tsne_2_max)\n",
        "plt.title(\"Cluster Centers of Prompts\", fontsize=20)\n",
        "plt.xlabel(\"t-SNE Dimension 1\", fontsize=18)\n",
        "plt.ylabel(\"t-SNE Dimension 2\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('cluster_centers_of_prompts.png', bbox_inches='tight')\n",
        "# plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
        "plt.show()\n",
        "\n",
        "# Step 5: Calculate Distances\n",
        "in_cluster_distances = []\n",
        "out_cluster_distances = []\n",
        "\n",
        "for author in df['Target'].unique():\n",
        "    author_indices = df['Target'] == author\n",
        "    author_points = df.loc[author_indices, ['tsne_1', 'tsne_2']]\n",
        "    other_points = df.loc[~author_indices, ['tsne_1', 'tsne_2']]\n",
        "\n",
        "    # In-cluster distances\n",
        "    distances_to_center = pairwise_distances(author_points, cluster_centers.loc[author].values.reshape(1, -1))\n",
        "    in_cluster_distances.extend(distances_to_center)\n",
        "\n",
        "    # Out-cluster distances\n",
        "    distances_to_others = pairwise_distances(author_points, other_points)\n",
        "    out_cluster_distances.extend(distances_to_others.mean(axis=1))\n",
        "\n",
        "# Convert to numpy arrays for visualization\n",
        "in_cluster_distances = np.array(in_cluster_distances)\n",
        "out_cluster_distances = np.array(out_cluster_distances)\n",
        "\n",
        "# Step 6: Visualize Distances\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(in_cluster_distances, bins=30, alpha=0.5, label='In-Cluster Distances')\n",
        "plt.hist(out_cluster_distances, bins=30, alpha=0.5, label='Out-Cluster Distances')\n",
        "plt.legend(fontsize=20)\n",
        "plt.title(\"Distribution of In-Cluster and Out-Cluster Distances\", fontsize=20)\n",
        "plt.xlabel(\"Distance\", fontsize=18)\n",
        "plt.ylabel(\"Frequency\", fontsize=18)\n",
        "plt.xticks(fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.savefig('distribution_of_distances.png', bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# Calculate and display statistics\n",
        "def calculate_statistics(distances):\n",
        "    return {\n",
        "        'mean': np.mean(distances),\n",
        "        'median': np.median(distances),\n",
        "        'std': np.std(distances),\n",
        "        'min': np.min(distances),\n",
        "        'max': np.max(distances),\n",
        "        '25th_percentile': np.percentile(distances, 25),\n",
        "        '50th_percentile': np.percentile(distances, 50),\n",
        "        '75th_percentile': np.percentile(distances, 75),\n",
        "        '90th_percentile': np.percentile(distances, 90),\n",
        "    }\n",
        "\n",
        "# Calculate statistics for in-cluster and out-cluster distances\n",
        "in_cluster_stats = calculate_statistics(in_cluster_distances)\n",
        "out_cluster_stats = calculate_statistics(out_cluster_distances)\n",
        "\n",
        "# Convert the statistics to a DataFrame for better visualization\n",
        "stats_df = pd.DataFrame({\n",
        "    'In-Cluster Distances': in_cluster_stats,\n",
        "    'Out-Cluster Distances': out_cluster_stats\n",
        "})\n",
        "\n",
        "# Display the statistics\n",
        "print(stats_df)\n"
      ],
      "metadata": {
        "id": "FPStfDIPe_0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## accuracy (tested using paraphrase dataset)"
      ],
      "metadata": {
        "id": "9qujt9hSjvoV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'paraphrase', 'user_name']]\n",
        "df.columns = ['content', 'paraphrase', 'Target']\n",
        "\n",
        "# Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['paraphrase'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_supcon_para_bert-base-cased_coe1_temp0.1_unit2_epoch30/diffusiondb100_supcon_para_val0.72321_e29.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_preds.append(pred.argmax(1).cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_preds = np.concatenate(all_preds, axis=0).tolist()\n",
        "all_labels = np.concatenate(all_labels, axis=0).tolist()\n",
        "print(all_preds)\n",
        "print(all_labels)\n",
        "# Compare predictions to labels\n",
        "correct_predictions = sum(p == l for p, l in zip(all_preds, all_labels))\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / len(all_preds)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "u0DnFRBwjz5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tfidf - perplexit 30, iteration 10\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "# from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import pairwise_distances\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "# df = pd.read_csv('/content/drive/MyDrive/dataset_topic_analysis/final_50_authors_50_prompts.csv', encoding='utf-8')\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'paraphrase', 'user_name']]\n",
        "df.columns = ['content', 'paraphrase', 'Target']\n",
        "\n",
        "# Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['paraphrase'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_cls_para_bert-base-cased_coe0.0_temp0.1_unit2_epoch30/diffusiondb100_cls_para_val0.72073_e24.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "for i, (x1, x2, x3, y) in enumerate(pg):  # for x1, x2, x3, y in train_set:\n",
        "  x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "  pred, feats = model(x, return_feat=True)\n",
        "  all_preds.append(pred.argmax(1).cpu().detach().numpy())\n",
        "  all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_preds = np.concatenate(all_preds, axis=0).tolist()\n",
        "all_labels = np.concatenate(all_labels, axis=0).tolist()\n",
        "print(all_preds)\n",
        "print(all_labels)\n",
        "# Compare predictions to labels\n",
        "correct_predictions = sum(p == l for p, l in zip(all_preds, all_labels))\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / len(all_preds)\n",
        "print(accuracy)"
      ],
      "metadata": {
        "id": "UdFT-SZbpsIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## prediction accuracy for each of the author - para"
      ],
      "metadata": {
        "id": "h6igtquAUiyP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_topicseparate100_label_1.csv')\n",
        "df = df[['prompt', 'user_label']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe1_para_topic_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe1_para_topic_val0.48859_e29.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "# Lists to store the true labels and predicted labels\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "# testing\n",
        "model.eval()\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader), disable=False)\n",
        "with torch.no_grad():\n",
        "    test_acc = AverageMeter()\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg):\n",
        "        x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "        pred, feats = model(x, return_feat=True)\n",
        "        # logger\n",
        "        test_acc.update((pred.argmax(1) == y).sum().item() / len(y))\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        pg.set_postfix({\n",
        "            'test acc': '{:.6f}'.format(test_acc.avg),\n",
        "        })\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "\n",
        "# Create a DataFrame to hold the results\n",
        "results_df = pd.DataFrame({\n",
        "    'author': all_labels,    # True labels (author)\n",
        "    'pred_author': all_preds # Predicted labels (author)\n",
        "})\n",
        "\n",
        "# Group by author and calculate accuracy for each author\n",
        "author_accuracy_cls_contra = results_df.groupby('author').apply(lambda x: (x['author'] == x['pred_author']).mean())\n",
        "\n",
        "# Display the accuracy for each author\n",
        "# print(author_accuracy_cls_contra)\n"
      ],
      "metadata": {
        "id": "n1fLE2JIlvaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "author_accuracy_cls_contra.sum()"
      ],
      "metadata": {
        "id": "mxyVJkQBMSAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_supcon_para_bert-base-cased_coe1_temp0.1_unit2_epoch30/diffusiondb100_supcon_para_val0.72321_e29.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "# Lists to store the true labels and predicted labels\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "# testing\n",
        "model.eval()\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader), disable=False)\n",
        "with torch.no_grad():\n",
        "    test_acc = AverageMeter()\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg):\n",
        "        x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "        pred, feats = model(x, return_feat=True)\n",
        "        # logger\n",
        "        test_acc.update((pred.argmax(1) == y).sum().item() / len(y))\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        pg.set_postfix({\n",
        "            'test acc': '{:.6f}'.format(test_acc.avg),\n",
        "        })\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "\n",
        "# Create a DataFrame to hold the results\n",
        "results_df = pd.DataFrame({\n",
        "    'author': all_labels,    # True labels (author)\n",
        "    'pred_author': all_preds # Predicted labels (author)\n",
        "})\n",
        "\n",
        "# Group by author and calculate accuracy for each author\n",
        "author_accuracy_cls_contra = results_df.groupby('author').apply(lambda x: (x['author'] == x['pred_author']).mean())\n",
        "\n",
        "# Display the accuracy for each author\n",
        "# print(author_accuracy_cls_contra)\n"
      ],
      "metadata": {
        "id": "bTFIAPxaUrdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_cls_para_bert-base-cased_coe0.0_temp0.1_unit2_epoch30/diffusiondb100_cls_para_val0.72073_e24.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "# Lists to store the true labels and predicted labels\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "# testing\n",
        "model.eval()\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader), disable=False)\n",
        "with torch.no_grad():\n",
        "    test_acc = AverageMeter()\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg):\n",
        "        x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "        pred, feats = model(x, return_feat=True)\n",
        "        # logger\n",
        "        test_acc.update((pred.argmax(1) == y).sum().item() / len(y))\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        pg.set_postfix({\n",
        "            'test acc': '{:.6f}'.format(test_acc.avg),\n",
        "        })\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "\n",
        "# Create a DataFrame to hold the results\n",
        "results_df = pd.DataFrame({\n",
        "    'author': all_labels,    # True labels (author)\n",
        "    'pred_author': all_preds # Predicted labels (author)\n",
        "})\n",
        "\n",
        "# Group by author and calculate accuracy for each author\n",
        "author_accuracy_cls = results_df.groupby('author').apply(lambda x: (x['author'] == x['pred_author']).mean())\n",
        "\n",
        "# Display the accuracy for each author\n",
        "# print(author_accuracy_cls)\n"
      ],
      "metadata": {
        "id": "2Sv48WrtUtk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe2_para_bert-base-cased_coe2.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe2_para_val0.73512_e24.pt'\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe1_para_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe1_para_val0.73264_e16.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "# Lists to store the true labels and predicted labels\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "# testing\n",
        "model.eval()\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader), disable=False)\n",
        "with torch.no_grad():\n",
        "    test_acc = AverageMeter()\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg):\n",
        "        x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "        pred, feats = model(x, return_feat=True)\n",
        "        # logger\n",
        "        test_acc.update((pred.argmax(1) == y).sum().item() / len(y))\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        pg.set_postfix({\n",
        "            'test acc': '{:.6f}'.format(test_acc.avg),\n",
        "        })\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "\n",
        "# Create a DataFrame to hold the results\n",
        "results_df = pd.DataFrame({\n",
        "    'author': all_labels,    # True labels (author)\n",
        "    'pred_author': all_preds # Predicted labels (author)\n",
        "})\n",
        "\n",
        "# Group by author and calculate accuracy for each author\n",
        "author_accuracy_lcl = results_df.groupby('author').apply(lambda x: (x['author'] == x['pred_author']).mean())\n",
        "\n",
        "# Display the accuracy for each author\n",
        "# print(author_accuracy_cls)\n"
      ],
      "metadata": {
        "id": "GXtxXMapVKux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe2_para_bert-base-cased_coe2.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe2_para_val0.73512_e24.pt'\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_try2/style_encoder_supcon_9.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "# Lists to store the true labels and predicted labels\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "# testing\n",
        "model.eval()\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader), disable=False)\n",
        "with torch.no_grad():\n",
        "    test_acc = AverageMeter()\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg):\n",
        "        x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "        pred, feats = model(x, return_feat=True)\n",
        "        # logger\n",
        "        test_acc.update((pred.argmax(1) == y).sum().item() / len(y))\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        pg.set_postfix({\n",
        "            'test acc': '{:.6f}'.format(test_acc.avg),\n",
        "        })\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "\n",
        "# Create a DataFrame to hold the results\n",
        "results_df = pd.DataFrame({\n",
        "    'author': all_labels,    # True labels (author)\n",
        "    'pred_author': all_preds # Predicted labels (author)\n",
        "})\n",
        "\n",
        "# Group by author and calculate accuracy for each author\n",
        "author_accuracy_mi = results_df.groupby('author').apply(lambda x: (x['author'] == x['pred_author']).mean())\n",
        "\n",
        "# Display the accuracy for each author\n",
        "# print(author_accuracy_cls)\n"
      ],
      "metadata": {
        "id": "dZoST6GVZ7I5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_result = pd.concat([author_accuracy_cls_contra, author_accuracy_cls], axis=1)\n",
        "accuracy_result = pd.concat([accuracy_result, author_accuracy_lcl], axis=1)\n",
        "accuracy_result = pd.concat([accuracy_result, author_accuracy_mi], axis=1)\n",
        "accuracy_result.columns = ['cls_contra', 'cls', 'lcl', 'mi']\n",
        "accuracy_result"
      ],
      "metadata": {
        "id": "YJj0WGMMVDWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy_result['mi_minus_cls'] = accuracy_result['mi'] - accuracy_result['cls']\n",
        "accuracy_result['mi_minus_cls_contra'] = accuracy_result['mi'] - accuracy_result['cls_contra']\n",
        "accuracy_result['mi_minus_lcl'] = accuracy_result['mi'] - accuracy_result['lcl']\n",
        "accuracy_result"
      ],
      "metadata": {
        "id": "ihUnimmWciGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the number of positive values\n",
        "num_positive = (accuracy_result['mi_minus_cls'] > 0).sum()\n",
        "\n",
        "# Count the number of zero values\n",
        "num_zero = (accuracy_result['mi_minus_cls'] == 0).sum()\n",
        "\n",
        "# Count the number of negative values\n",
        "num_negative = (accuracy_result['mi_minus_cls'] < 0).sum()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of positive values: {num_positive}\")\n",
        "print(f\"Number of zero values: {num_zero}\")\n",
        "print(f\"Number of negative values: {num_negative}\")\n",
        "\n",
        "# Count the number of positive values\n",
        "num_positive = (accuracy_result['mi_minus_cls_contra'] > 0).sum()\n",
        "\n",
        "# Count the number of zero values\n",
        "num_zero = (accuracy_result['mi_minus_cls_contra'] == 0).sum()\n",
        "\n",
        "# Count the number of negative values\n",
        "num_negative = (accuracy_result['mi_minus_cls_contra'] < 0).sum()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of positive values: {num_positive}\")\n",
        "print(f\"Number of zero values: {num_zero}\")\n",
        "print(f\"Number of negative values: {num_negative}\")\n",
        "\n",
        "# Count the number of positive values\n",
        "num_positive = (accuracy_result['mi_minus_lcl'] > 0).sum()\n",
        "\n",
        "# Count the number of zero values\n",
        "num_zero = (accuracy_result['mi_minus_lcl'] == 0).sum()\n",
        "\n",
        "# Count the number of negative values\n",
        "num_negative = (accuracy_result['mi_minus_lcl'] < 0).sum()\n",
        "\n",
        "# Print the results\n",
        "print(f\"Number of positive values: {num_positive}\")\n",
        "print(f\"Number of zero values: {num_zero}\")\n",
        "print(f\"Number of negative values: {num_negative}\")"
      ],
      "metadata": {
        "id": "MveocIUwdEQd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Filter the DataFrame for rows where mi_minus_cls is negative\n",
        "# df_negative = accuracy_result\n",
        "df_negative = accuracy_result[accuracy_result['mi_minus_cls'] < 0]\n",
        "# Set the figure size\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Set up the x-axis to be the index (row number) of the filtered DataFrame\n",
        "x = df_negative.index\n",
        "\n",
        "# Plot 'mi' values as bars in blue\n",
        "plt.bar(x - 0.2, df_negative['mi'], width=0.4, label='mi', color='blue')\n",
        "\n",
        "# Plot 'cls' values as bars in red\n",
        "plt.bar(x + 0.2, df_negative['cls'], width=0.4, label='cls', color='red')\n",
        "\n",
        "# Set labels and title\n",
        "plt.xlabel('Row Number')\n",
        "plt.ylabel('Value')\n",
        "plt.title('mi and cls Values for Rows with Negative mi_minus_cls')\n",
        "\n",
        "# Add legend to differentiate between mi and cls\n",
        "plt.legend()\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "qKiC-kHLdyfD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "accuracy_result.sum()"
      ],
      "metadata": {
        "id": "uhe7a4uSVdN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## quantify model confidence"
      ],
      "metadata": {
        "id": "UOv21bEK6sPy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compute_topk_entropy(pred, k):\n",
        "    # Step 1: Convert logits to probabilities\n",
        "    probs = F.softmax(pred, dim=1)\n",
        "\n",
        "    # Step 2: Get the top-k probabilities\n",
        "    topk_probs, _ = torch.topk(probs, k=k, dim=1)\n",
        "\n",
        "    # Step 3: Normalize top-k probabilities to sum to 1\n",
        "    normalized_topk_probs = topk_probs / topk_probs.sum(dim=1, keepdim=True)\n",
        "\n",
        "    # Step 4: Calculate entropy for the top-k probabilities\n",
        "    entropy = -torch.sum(normalized_topk_probs * torch.log2(normalized_topk_probs), dim=1)\n",
        "\n",
        "    return entropy.mean().item()\n",
        "\n",
        "\n",
        "def get_predictions(ckpt_path, test_loader):\n",
        "\n",
        "    ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "    num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "    model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "    model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "    model = nn.DataParallel(model).cuda()\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "    with torch.no_grad():\n",
        "        for i, (x1, x2, x3, y) in enumerate(pg):\n",
        "            x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "            pred, feats = model(x, return_feat=True)\n",
        "            all_preds.append(pred)\n",
        "\n",
        "    all_preds = torch.cat(all_preds, dim=0)\n",
        "\n",
        "    return all_preds\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "ckpt_path1 = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe1_para_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe1_para_val0.73264_e16.pt'\n",
        "# ckpt_path1 = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe2_para_bert-base-cased_coe2.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe2_para_val0.73512_e24.pt'\n",
        "ckpt_path2 = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_cls_para_bert-base-cased_coe0.0_temp0.1_unit2_epoch30/diffusiondb100_cls_para_val0.72073_e24.pt'\n",
        "ckpt_path3 = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_supcon_para_bert-base-cased_coe1_temp0.1_unit2_epoch30/diffusiondb100_supcon_para_val0.72321_e29.pt'\n",
        "\n",
        "all_preds1 = get_predictions(ckpt_path1, test_loader)\n",
        "all_preds2 = get_predictions(ckpt_path2, test_loader)\n",
        "all_preds3 = get_predictions(ckpt_path3, test_loader)\n"
      ],
      "metadata": {
        "id": "7Z-DPohb2qoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k_values = [1, 2, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]\n",
        "avg_entropies1 = []\n",
        "avg_entropies2 = []\n",
        "avg_entropies3 = []\n",
        "for k in k_values:\n",
        "    avg_entropy1 = compute_topk_entropy(all_preds1, k)\n",
        "    avg_entropies1.append(avg_entropy1)\n",
        "\n",
        "    avg_entropy2 = compute_topk_entropy(all_preds2, k)\n",
        "    avg_entropies2.append(avg_entropy2)\n",
        "\n",
        "    avg_entropy3 = compute_topk_entropy(all_preds3, k)\n",
        "    avg_entropies3.append(avg_entropy3)\n",
        "\n",
        "print(avg_entropies1)\n",
        "print(avg_entropies2)\n",
        "print(avg_entropies3)\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, avg_entropies1, label=\"SUP+LCL+CE\")\n",
        "plt.plot(k_values, avg_entropies2, label=\"CE\")\n",
        "plt.plot(k_values, avg_entropies3, label=\"SUP+CE\")\n",
        "plt.xlabel(\"k\")\n",
        "plt.ylabel(\"Entropy (bits)\")\n",
        "plt.title(\"Averaged Entropy of the Prediction Score Distributions\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TYrakg4v9Wqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(all_preds1)\n",
        "print(all_preds2)\n",
        "print(all_preds3)"
      ],
      "metadata": {
        "id": "z5u2LYesA7Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## acuracy"
      ],
      "metadata": {
        "id": "PbkGlp2VFg7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe1_para_albert-base-v2_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe1_para_val0.68056_e17.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_cls_para_albert-base-v2_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_cls_para_val0.66964_e24.pt'\n",
        "\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe1_para_roberta-base_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe1_para_val0.73065_e22.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_cls_para_roberta-base_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_cls_para_val0.71627_e26.pt'\n",
        "\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lclonly_coe1_para_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lclonly_coe1_para_val0.73859_e27.pt'\n",
        "\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_lcl_coe1_para_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_lcl_coe1_para_val0.73264_e16.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_cls_para_bert-base-cased_coe0.0_temp0.1_unit2_epoch30/diffusiondb100_cls_para_val0.72073_e24.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_supcon_para_bert-base-cased_coe1_temp0.1_unit2_epoch30/diffusiondb100_supcon_para_val0.72321_e29.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_try2/style_encoder_supcon_9.pt'\n",
        "\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/blogs50_cls_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/blogs50_cls_val0.82588_e29.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/blogs50_lcl_coe1_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/blogs50_lcl_coe1_val0.83203_e28.pt'\n",
        "\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/imdb62_cls_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/imdb62_cls_val0.97917_e27.pt'\n",
        "\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_clean_cls_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_clean_cls_val0.41518_e23.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_clean_lcl_coe1_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_clean_lcl_coe1_val0.43056_e22.pt'\n",
        "\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb120_lcl_coe1_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb120_lcl_coe1_val0.79117_e26.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb80_lcl_coe1_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb80_lcl_coe1_val0.68899_e22.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb60_lcl_coe1_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb60_lcl_coe1_val0.61954_e28.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_200_lcl_coe1_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_200_lcl_coe1_val0.68975_e27.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/lcl/diffusiondb100_150_lcl_coe1_bert-base-cased_coe1.0_temp0.1_unit2_epoch30/diffusiondb100_150_lcl_coe1_val0.73867_e29.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_80/style_encoder_supcon1_14.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_60/style_encoder_supcon1_17.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_clean/style_encoder_supcon1_8.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_imdb621/style_encoder_supcon1_6.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_blogs501/style_encoder_supcon1_10.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_120/style_encoder_supcon1_12.pt'\n",
        "# ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_100_150/style_encoder_supcon1_6.pt'\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_100_2001/style_encoder_supcon1_7.pt'\n",
        "\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs50/processed/blogs50_AA_test.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/clean/test_random100_label_1.csv')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random100_200_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "# df = df[['text', 'author_id']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, AlbertTokenizer, AlbertModel, RobertaTokenizer, RobertaModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "# tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')\n",
        "# extractor = AlbertModel.from_pretrained('albert-base-v2')\n",
        "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "# extractor = RobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 200\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "# num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "\n",
        "model.eval()\n",
        "\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i, (x1, x2, x3, y) in enumerate(test_loader):  # for x1, x2, x3, y in train_set:\n",
        "        x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "        pred, feats = model(x, return_feat=True)\n",
        "        all_preds.append(pred.argmax(1).cpu().detach().numpy())\n",
        "        all_labels.append(y.cpu().detach().numpy())\n",
        "\n",
        "all_preds = np.concatenate(all_preds, axis=0).tolist()\n",
        "all_labels = np.concatenate(all_labels, axis=0).tolist()\n",
        "# print(all_preds)\n",
        "# print(all_labels)\n",
        "# Compare predictions to labels\n",
        "correct_predictions = sum(p == l for p, l in zip(all_preds, all_labels))\n",
        "# Calculate accuracy\n",
        "accuracy = correct_predictions / len(all_preds)\n",
        "print('accuracy', accuracy)\n",
        "\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate precision, recall, and F1 score\n",
        "# Macro average (unweighted mean, treats all classes equally):\n",
        "macro_precision = precision_score(all_labels, all_preds, average='macro')\n",
        "macro_recall = recall_score(all_labels, all_preds, average='macro')\n",
        "macro_f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "# Weighted average (weighted by class frequencies):\n",
        "weighted_precision = precision_score(all_labels, all_preds, average='weighted')\n",
        "weighted_recall = recall_score(all_labels, all_preds, average='weighted')\n",
        "weighted_f1 = f1_score(all_labels, all_preds, average='weighted')\n",
        "\n",
        "# Print the results\n",
        "print(f\"Macro Precision: {macro_precision}\")\n",
        "print(f\"Macro Recall: {macro_recall}\")\n",
        "print(f\"Macro F1: {macro_f1}\")\n",
        "\n",
        "print(f\"Weighted Precision: {weighted_precision}\")\n",
        "print(f\"Weighted Recall: {weighted_recall}\")\n",
        "print(f\"Weighted F1: {weighted_f1}\")\n"
      ],
      "metadata": {
        "id": "WRSaFohdFjo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCcC6-aos5kH"
      },
      "source": [
        "## prediction accuracy for each of the author"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4E0jICoisqiQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_bert-base-cased_coe1_temp0.1_unit2_epoch20/diffusiondb100_val0.77108_e17.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "# Lists to store the true labels and predicted labels\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "# testing\n",
        "model.eval()\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader), disable=False)\n",
        "with torch.no_grad():\n",
        "    test_acc = AverageMeter()\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg):\n",
        "        x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "        pred, feats = model(x, return_feat=True)\n",
        "        # logger\n",
        "        test_acc.update((pred.argmax(1) == y).sum().item() / len(y))\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        pg.set_postfix({\n",
        "            'test acc': '{:.6f}'.format(test_acc.avg),\n",
        "        })\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "\n",
        "# Create a DataFrame to hold the results\n",
        "results_df = pd.DataFrame({\n",
        "    'author': all_labels,    # True labels (author)\n",
        "    'pred_author': all_preds # Predicted labels (author)\n",
        "})\n",
        "\n",
        "# Group by author and calculate accuracy for each author\n",
        "author_accuracy_cls_contra = results_df.groupby('author').apply(lambda x: (x['author'] == x['pred_author']).mean())\n",
        "\n",
        "# Display the accuracy for each author\n",
        "# print(author_accuracy_cls_contra)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSNx08jn7BDG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_cls_bert-base-cased_coe1_temp0.1_unit2_epoch20/diffusiondb100_cls_val0.77510_e7.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "# Lists to store the true labels and predicted labels\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "# testing\n",
        "model.eval()\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader), disable=False)\n",
        "with torch.no_grad():\n",
        "    test_acc = AverageMeter()\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg):\n",
        "        x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "        pred, feats = model(x, return_feat=True)\n",
        "        # logger\n",
        "        test_acc.update((pred.argmax(1) == y).sum().item() / len(y))\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        pg.set_postfix({\n",
        "            'test acc': '{:.6f}'.format(test_acc.avg),\n",
        "        })\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "\n",
        "# Create a DataFrame to hold the results\n",
        "results_df = pd.DataFrame({\n",
        "    'author': all_labels,    # True labels (author)\n",
        "    'pred_author': all_preds # Predicted labels (author)\n",
        "})\n",
        "\n",
        "# Group by author and calculate accuracy for each author\n",
        "author_accuracy_cls = results_df.groupby('author').apply(lambda x: (x['author'] == x['pred_author']).mean())\n",
        "\n",
        "# Display the accuracy for each author\n",
        "# print(author_accuracy_cls)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZLtKfId55G5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n",
        "df = df[['prompt', 'user_name']]\n",
        "df.columns = ['content', 'Target']\n",
        "\n",
        "# # Randomly select 10 authors\n",
        "# selected_authors = np.random.choice(df['Target'].unique(), size=10, replace=False)\n",
        "# df = df[df['Target'].isin(selected_authors)]\n",
        "\n",
        "test_x, test_y = df['content'].tolist(), df['Target'].tolist()\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "test_set = BertDataset(test_x, test_y, tokenizer, num_tokens)\n",
        "test_loader = DataLoader(test_set, batch_size=24, shuffle=False, num_workers=4,\n",
        "                             pin_memory=True)\n",
        "\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader))\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100\n",
        "model = BertClassifier(extractor, LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout))\n",
        "ckpt_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_supcon_cls_bert-base-cased_coe1_temp0.1_unit6_epoch30/diffusiondb100_supcon_cls_val0.79216_e24.pt'\n",
        "model = load_model_dic(model, ckpt_path, verbose=True, strict=True)\n",
        "model = nn.DataParallel(model).cuda()\n",
        "# Lists to store the true labels and predicted labels\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "# testing\n",
        "model.eval()\n",
        "pg = tqdm(test_loader, leave=False, total=len(test_loader), disable=False)\n",
        "with torch.no_grad():\n",
        "    test_acc = AverageMeter()\n",
        "    for i, (x1, x2, x3, y) in enumerate(pg):\n",
        "        x, y = (x1.cuda(), x2.cuda(), x3.cuda()), y.cuda()\n",
        "        pred, feats = model(x, return_feat=True)\n",
        "        # logger\n",
        "        test_acc.update((pred.argmax(1) == y).sum().item() / len(y))\n",
        "        pred = torch.argmax(pred, dim=1)\n",
        "        all_labels.extend(y.cpu().numpy())\n",
        "        all_preds.extend(pred.cpu().numpy())\n",
        "        pg.set_postfix({\n",
        "            'test acc': '{:.6f}'.format(test_acc.avg),\n",
        "        })\n",
        "\n",
        "all_labels = np.array(all_labels)\n",
        "all_preds = np.array(all_preds)\n",
        "\n",
        "# Create a DataFrame to hold the results\n",
        "results_df = pd.DataFrame({\n",
        "    'author': all_labels,    # True labels (author)\n",
        "    'pred_author': all_preds # Predicted labels (author)\n",
        "})\n",
        "\n",
        "# Group by author and calculate accuracy for each author\n",
        "author_accuracy_cls_supcon = results_df.groupby('author').apply(lambda x: (x['author'] == x['pred_author']).mean())\n",
        "\n",
        "# Display the accuracy for each author\n",
        "# print(author_accuracy_cls_contra)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9v9sfrs_7UKH"
      },
      "outputs": [],
      "source": [
        "accuracy_result = pd.concat([author_accuracy_cls_contra, author_accuracy_cls], axis=1)\n",
        "accuracy_result = pd.concat([author_accuracy_cls_contra, author_accuracy_cls], axis=1)\n",
        "accuracy_result.columns = ['cls_contra', 'cls']\n",
        "accuracy_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZdLtY5cj8cIX"
      },
      "outputs": [],
      "source": [
        "accuracy_result.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ikMS_3NYnYF"
      },
      "source": [
        "## Dataset-level"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIOU0bWW-Fjc"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    datasets = ['imdb62', 'blog', 'turing', 'diffusiondb']\n",
        "    parser = argparse.ArgumentParser(description=f'Training models for datasets {datasets}')\n",
        "    parser.add_argument('--dataset', type=str, help='dataset used for training', choices=datasets)\n",
        "    parser.add_argument('--id', type=str, default='0', help='experiment id')\n",
        "    parser.add_argument('--gpu', type=str, help='the cuda devices used for training', default=\"0,1,2,3\")\n",
        "    parser.add_argument('--tqdm', type=bool, help='whether tqdm is on', default=False)\n",
        "    parser.add_argument('--authors', type=int, help='number of authors', default=None)\n",
        "    parser.add_argument('--samples-per-auth', type=int, help='number of samples per author', default=None)\n",
        "    parser.add_argument('--epochs', type=int, default=5)\n",
        "    parser.add_argument('--model', type=str, default='microsoft/deberta-base')\n",
        "    parser.add_argument('--coe', type=float, default=1)\n",
        "\n",
        "    # dataset - num of authors mapping\n",
        "    default_num_authors = {\n",
        "        'imdb62': 62,\n",
        "        'blog': 50,\n",
        "        'turing': 20,\n",
        "        'diffusiondb': 100,\n",
        "    }\n",
        "\n",
        "    training_args = [\n",
        "    '--dataset', 'blog',\n",
        "    '--id', 'blog10',\n",
        "    '--gpu', '0',\n",
        "    '--tqdm', 'True',\n",
        "    '--authors', '10',\n",
        "    '--epochs', '8',\n",
        "    '--model', 'bert-base-cased'\n",
        "    ]\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args(training_args)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "    source = args.dataset\n",
        "    num_authors = args.authors if args.authors is not None else default_num_authors[args.dataset]\n",
        "    print(' '.join(f'{k}={v}' for k, v in vars(args).items()))  # print all args\n",
        "\n",
        "    # masked classes\n",
        "    mask_classes = {\n",
        "        'blog': [],\n",
        "        'imdb62': [],\n",
        "        'turing': [],\n",
        "        'diffusiondb': [],\n",
        "    }\n",
        "\n",
        "    # load data and remove emails containing the sender's name\n",
        "    df = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "    df = df[['prompt', 'user_name']]\n",
        "    df.columns = ['content', 'From']\n",
        "\n",
        "    if args.authors is not default_num_authors[args.dataset]:\n",
        "        warnings.warn(f\"Number of authors for dataset {args.dataset} is {default_num_authors[args.dataset]}, \"\n",
        "                      f\"but got {args.authors} instead. \")\n",
        "\n",
        "    if args.samples_per_auth is not None:\n",
        "        warnings.warn(f\"Number of samples per author specified as {args.samples_per_auth}, which is a \"\n",
        "                      f\"dangerous argument. \")\n",
        "\n",
        "    limit = num_authors\n",
        "    print(\"Number of authors: \", limit)\n",
        "\n",
        "    # select top N senders and build train and test\n",
        "    nlp_train, nlp_val, nlp_test_paraphrased = build_train_test(df, source, limit, per_author=args.samples_per_auth, seed=0)\n",
        "\n",
        "    # print(df.head(5))\n",
        "    print(nlp_test_paraphrased.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uD39iVSZCMqB"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    datasets = ['imdb62', 'blog', 'turing', 'diffusiondb']\n",
        "    parser = argparse.ArgumentParser(description=f'Training models for datasets {datasets}')\n",
        "    parser.add_argument('--dataset', type=str, help='dataset used for training', choices=datasets)\n",
        "    parser.add_argument('--id', type=str, default='0', help='experiment id')\n",
        "    parser.add_argument('--gpu', type=str, help='the cuda devices used for training', default=\"0,1,2,3\")\n",
        "    parser.add_argument('--tqdm', type=bool, help='whether tqdm is on', default=False)\n",
        "    parser.add_argument('--authors', type=int, help='number of authors', default=None)\n",
        "    parser.add_argument('--samples-per-auth', type=int, help='number of samples per author', default=None)\n",
        "    parser.add_argument('--epochs', type=int, default=5)\n",
        "    parser.add_argument('--model', type=str, default='microsoft/deberta-base')\n",
        "    parser.add_argument('--coe', type=float, default=1)\n",
        "\n",
        "    # dataset - num of authors mapping\n",
        "    default_num_authors = {\n",
        "        'imdb62': 62,\n",
        "        'blog': 50,\n",
        "        'turing': 20,\n",
        "        'diffusiondb': 100,\n",
        "    }\n",
        "\n",
        "    training_args = [\n",
        "    '--dataset', 'blog',\n",
        "    '--id', 'blog50',\n",
        "    '--gpu', '0',\n",
        "    '--tqdm', 'True',\n",
        "    '--authors', '50',\n",
        "    '--epochs', '8',\n",
        "    '--model', 'bert-base-cased'\n",
        "    ]\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args(training_args)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "    source = args.dataset\n",
        "    num_authors = args.authors if args.authors is not None else default_num_authors[args.dataset]\n",
        "    print(' '.join(f'{k}={v}' for k, v in vars(args).items()))  # print all args\n",
        "\n",
        "    # masked classes\n",
        "    mask_classes = {\n",
        "        'blog': [],\n",
        "        'imdb62': [],\n",
        "        'turing': [],\n",
        "        'diffusiondb': [],\n",
        "    }\n",
        "\n",
        "    # load data and remove emails containing the sender's name\n",
        "    df = load_dataset_dataframe(source)\n",
        "\n",
        "    if args.authors is not default_num_authors[args.dataset]:\n",
        "        warnings.warn(f\"Number of authors for dataset {args.dataset} is {default_num_authors[args.dataset]}, \"\n",
        "                      f\"but got {args.authors} instead. \")\n",
        "\n",
        "    if args.samples_per_auth is not None:\n",
        "        warnings.warn(f\"Number of samples per author specified as {args.samples_per_auth}, which is a \"\n",
        "                      f\"dangerous argument. \")\n",
        "\n",
        "    limit = num_authors\n",
        "    print(\"Number of authors: \", limit)\n",
        "\n",
        "    # select top N senders and build train and test\n",
        "    nlp_train, nlp_val, nlp_test_50 = build_train_test(df, source, limit, per_author=args.samples_per_auth, seed=0)\n",
        "\n",
        "    # print(df.head(5))\n",
        "    print(nlp_test_50.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVy-uNJWCslK"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    datasets = ['imdb62', 'blog', 'turing', 'diffusiondb']\n",
        "    parser = argparse.ArgumentParser(description=f'Training models for datasets {datasets}')\n",
        "    parser.add_argument('--dataset', type=str, help='dataset used for training', choices=datasets)\n",
        "    parser.add_argument('--id', type=str, default='0', help='experiment id')\n",
        "    parser.add_argument('--gpu', type=str, help='the cuda devices used for training', default=\"0,1,2,3\")\n",
        "    parser.add_argument('--tqdm', type=bool, help='whether tqdm is on', default=False)\n",
        "    parser.add_argument('--authors', type=int, help='number of authors', default=None)\n",
        "    parser.add_argument('--samples-per-auth', type=int, help='number of samples per author', default=None)\n",
        "    parser.add_argument('--epochs', type=int, default=5)\n",
        "    parser.add_argument('--model', type=str, default='microsoft/deberta-base')\n",
        "    parser.add_argument('--coe', type=float, default=1)\n",
        "\n",
        "    # dataset - num of authors mapping\n",
        "    default_num_authors = {\n",
        "        'imdb62': 62,\n",
        "        'blog': 50,\n",
        "        'turing': 20,\n",
        "        'diffusiondb': 100,\n",
        "    }\n",
        "\n",
        "    training_args = [\n",
        "    '--dataset', 'turing',\n",
        "    '--id', 'turing',\n",
        "    '--gpu', '0',\n",
        "    '--tqdm', 'True',\n",
        "    '--authors', '20',\n",
        "    '--epochs', '8',\n",
        "    '--model', 'bert-base-cased'\n",
        "    ]\n",
        "\n",
        "    # parse args\n",
        "    args = parser.parse_args(training_args)\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.gpu\n",
        "    source = args.dataset\n",
        "    num_authors = args.authors if args.authors is not None else default_num_authors[args.dataset]\n",
        "    print(' '.join(f'{k}={v}' for k, v in vars(args).items()))  # print all args\n",
        "\n",
        "    # masked classes\n",
        "    mask_classes = {\n",
        "        'blog': [],\n",
        "        'imdb62': [],\n",
        "        'turing': [],\n",
        "        'diffusiondb': [],\n",
        "    }\n",
        "\n",
        "    # load data and remove emails containing the sender's name\n",
        "    df = load_dataset_dataframe(source)\n",
        "\n",
        "    if args.authors is not default_num_authors[args.dataset]:\n",
        "        warnings.warn(f\"Number of authors for dataset {args.dataset} is {default_num_authors[args.dataset]}, \"\n",
        "                      f\"but got {args.authors} instead. \")\n",
        "\n",
        "    if args.samples_per_auth is not None:\n",
        "        warnings.warn(f\"Number of samples per author specified as {args.samples_per_auth}, which is a \"\n",
        "                      f\"dangerous argument. \")\n",
        "\n",
        "    limit = num_authors\n",
        "    print(\"Number of authors: \", limit)\n",
        "\n",
        "    # select top N senders and build train and test\n",
        "    nlp_train, nlp_val, nlp_test_turing = build_train_test(df, source, limit, per_author=args.samples_per_auth, seed=0)\n",
        "\n",
        "    # print(df.head(5))\n",
        "    print(nlp_test_turing.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKxOsHjuV7Yf"
      },
      "outputs": [],
      "source": [
        "print(nlp_test_10.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "450pOa1AETzv"
      },
      "outputs": [],
      "source": [
        "!pip install lda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH0ISn8iDmFq"
      },
      "outputs": [],
      "source": [
        "grouped_by_author = nlp_test_10.groupby('Target')\n",
        "blog10_documents_by_author = {author: group for author, group in grouped_by_author}\n",
        "\n",
        "grouped_by_author = nlp_test_50.groupby('Target')\n",
        "blog50_documents_by_author = {author: group for author, group in grouped_by_author}\n",
        "\n",
        "grouped_by_author = nlp_test_turing.groupby('Target')\n",
        "turing_documents_by_author = {author: group for author, group in grouped_by_author}\n",
        "\n",
        "# print(blog10_documents_by_author)\n",
        "# print(blog50_documents_by_author)\n",
        "# print(turing_documents_by_author)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_test_paraphased = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "nlp_test_paraphased = nlp_test_paraphased[['prompt', 'user_name']]\n",
        "nlp_test_paraphased.columns = ['content', 'Target']\n",
        "\n",
        "nlp_test_clean = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/clean/test_random100_label_1.csv')\n",
        "nlp_test_clean = nlp_test_clean[['prompt', 'user_name']]\n",
        "nlp_test_clean.columns = ['content', 'Target']\n",
        "\n",
        "nlp_test_blogs50 = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs/processed/blogs50_AA_test.csv')\n",
        "nlp_test_blogs50 = nlp_test_blogs50[['text', 'author_id']]\n",
        "nlp_test_blogs50.columns = ['content', 'Target']\n",
        "\n",
        "nlp_test_imdb62 = pd.read_csv('/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_test.csv')\n",
        "nlp_test_imdb62 = nlp_test_imdb62[['text', 'author_id']]\n",
        "nlp_test_imdb62.columns = ['content', 'Target']"
      ],
      "metadata": {
        "id": "2VWl_YPCv77Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nlp_test_paraphased['cleaned_text'] = nlp_test_paraphased['content'].apply(fil_sent)\n",
        "\n",
        "# 2. Apply the 'extract_style' function to extract features and add them to the DataFrame\n",
        "style_columns = [avg_len, len_text, len_words, num_short_w, per_digit, per_cap, f_a, f_b, f_c, f_d, f_e, f_f, f_g, f_h, f_i,\n",
        "         f_j, f_k, f_l, f_m, f_n, f_o, f_p, f_q, f_r, f_s, f_t, f_u, f_v, f_w, f_x, f_y, f_z, f_0, f_1, f_2, f_3,\n",
        "         f_4, f_5, f_6, f_7, f_8, f_9, f_e_0, f_e_1, f_e_2, f_e_3, f_e_4, f_e_5, f_e_6, f_e_7, f_e_8, f_e_9, f_e_10,\n",
        "         f_e_11, richness]\n",
        "nlp_test_paraphased[style_columns] = nlp_test_paraphased['cleaned_text'].apply(extract_style)\n"
      ],
      "metadata": {
        "id": "cDBpNJ7OYCVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grouped_by_author = nlp_test_paraphased.groupby('Target')\n",
        "paraphrased_documents_by_author = {author: group for author, group in grouped_by_author}\n",
        "\n",
        "grouped_by_author = nlp_test_clean.groupby('Target')\n",
        "clean_documents_by_author = {author: group for author, group in grouped_by_author}\n",
        "\n",
        "grouped_by_author = nlp_test_blogs50.groupby('Target')\n",
        "blogs50_documents_by_author = {author: group for author, group in grouped_by_author}\n",
        "\n",
        "grouped_by_author = nlp_test_imdb62.groupby('Target')\n",
        "imdb62_documents_by_author = {author: group for author, group in grouped_by_author}"
      ],
      "metadata": {
        "id": "xpFjn3_ww7Jl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R-9GuyPduZe"
      },
      "outputs": [],
      "source": [
        "from __future__ import division, print_function\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import map_tag\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "import lda\n",
        "import re\n",
        "import lda.datasets\n",
        "from collections import defaultdict\n",
        "from numpy import sum\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import entropy\n",
        "from numpy.linalg import norm\n",
        "from collections import Counter\n",
        "import argparse\n",
        "import warnings\n",
        "\n",
        "np.random.seed(1337)\n",
        "\n",
        "def char_bigram(text, x_train):\n",
        "    vec = CountVectorizer(analyzer=\"char\", ngram_range=(2, 2), max_df=0.95, min_df=2, max_features=100)\n",
        "    vec.fit_transform(x_train)\n",
        "    vocab = vec.vocabulary_\n",
        "    vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(2, 2), vocabulary=vocab, max_features=100)\n",
        "    vectorizer.fit_transform(x_train)\n",
        "    feature_bigrams = vectorizer.transform(text)\n",
        "    return feature_bigrams.toarray()\n",
        "\n",
        "def char_trigram(text, x_train):\n",
        "    vec = CountVectorizer(analyzer=\"char\", ngram_range=(3, 3), max_df=0.95, min_df=2, max_features=100)\n",
        "    vec.fit_transform(x_train)\n",
        "    vocab = vec.vocabulary_\n",
        "    vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(3, 3), vocabulary=vocab, max_features=100)\n",
        "    vectorizer.fit_transform(x_train)\n",
        "    feature_trigrams = vectorizer.transform(text)\n",
        "    return feature_trigrams.toarray()\n",
        "\n",
        "def word_unigram(text, x_train):\n",
        "    # print(text)\n",
        "    vec = CountVectorizer(analyzer=\"word\", ngram_range=(1, 1), max_df=0.95, min_df=2, max_features=100, stop_words=\"english\")\n",
        "    vec.fit_transform(x_train)\n",
        "    vocab = vec.vocabulary_\n",
        "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1, 1), vocabulary=vocab, max_features=100)\n",
        "    vectorizer.fit_transform(x_train)\n",
        "    feature_wunigrams = vectorizer.transform(text)\n",
        "    return feature_wunigrams.toarray()\n",
        "\n",
        "def word_bigram(text, x_train):\n",
        "    vec = CountVectorizer(analyzer=\"word\", ngram_range=(2, 2), max_df=0.95, min_df=2, max_features=100, stop_words=\"english\")\n",
        "    vec.fit_transform(x_train)\n",
        "    vocab = vec.vocabulary_\n",
        "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(2, 2), vocabulary=vocab, max_features=100)\n",
        "    vectorizer.fit_transform(x_train)\n",
        "    feature_wbigrams = vectorizer.transform(text)\n",
        "    return feature_wbigrams.toarray()\n",
        "\n",
        "def word_trigram(text, x_train):\n",
        "    vec = CountVectorizer(analyzer=\"word\", ngram_range=(3, 3), max_df=0.95, min_df=2, max_features=100, stop_words=\"english\")\n",
        "    vec.fit_transform(x_train)\n",
        "    vocab = vec.vocabulary_\n",
        "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(3, 3), vocabulary=vocab, max_features=100)\n",
        "    vectorizer.fit_transform(x_train)\n",
        "    feature_wtrigrams = vectorizer.transform(text)\n",
        "    return feature_wtrigrams.toarray()\n",
        "\n",
        "\n",
        "# Topic analysis\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\d+\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip()\n",
        "\n",
        "def sample_prompts(group, n=100):\n",
        "    return group.sample(n=n, random_state=42) if len(group) > n else group\n",
        "\n",
        "def load_diffusiondb(df):\n",
        "    # load data (can be used if data already split into train and test set)\n",
        "    df = df[['content', 'From']]\n",
        "    # print(df)\n",
        "    df.columns = ['prompt', 'user_name']\n",
        "\n",
        "    x = df['prompt'].tolist()\n",
        "    y = df['user_name'].tolist()\n",
        "    dict_author = {}    # id doc: author_name\n",
        "    X = []\n",
        "    for i in range(len(x)):\n",
        "        X.append(clean_str(x[i]))\n",
        "        dict_author[i] = y[i]\n",
        "    return X, dict_author\n",
        "\n",
        "\n",
        "def JSD(P, Q):\n",
        "    _P = P / norm(P, ord=1)\n",
        "    _Q = Q / norm(Q, ord=1)\n",
        "    _M = 0.5 * (_P + _Q)\n",
        "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
        "\n",
        "\n",
        "def topics_analysis(df, number_of_topics):\n",
        "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "    X, dict_author = load_diffusiondb(df)\n",
        "\n",
        "    # print(X)\n",
        "    # create vocabulary\n",
        "    print (\"creating vocabulary..\")\n",
        "    print (\"---------------------------\")\n",
        "\n",
        "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=5, stop_words='english')\n",
        "    X_tf = tf_vectorizer.fit_transform(X)\n",
        "    vocab = tf_vectorizer.get_feature_names_out()\n",
        "    print(\"shape: {}\\n\".format(X_tf.shape))\n",
        "    # print(vocab)\n",
        "\n",
        "\n",
        "    # building topic model using LDA\n",
        "    print (\"building model..\")\n",
        "    print (\"---------------------------\")\n",
        "    model = lda.LDA(n_topics=number_of_topics, n_iter=500, random_state=1000)\n",
        "    model.fit(X_tf)\n",
        "    topic_word = model.topic_word_\n",
        "    print(\"shape: {}\".format(topic_word.shape))\n",
        "\n",
        "    # show detail of topic\n",
        "    n = 10\n",
        "    for i, topic_dist in enumerate(topic_word):\n",
        "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]\n",
        "        print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))\n",
        "\n",
        "    print (\"document topic model..\")\n",
        "    print (\"---------------------------\")\n",
        "    doc_topic = model.doc_topic_\n",
        "    topic_most = {}\n",
        "    for n in range(len(doc_topic)):\n",
        "        topic_most_pr = doc_topic[n].argmax()\n",
        "        author = dict_author[n]\n",
        "        if author in topic_most:\n",
        "            tp_most.append(topic_most_pr)\n",
        "        else:\n",
        "            tp_most = []\n",
        "            tp_most.append(topic_most_pr)\n",
        "        topic_most[author] = tp_most\n",
        "\n",
        "    i = 0\n",
        "    for author_p, topic_p in topic_most.items():\n",
        "        print (i, author_p, Counter(topic_p))\n",
        "        i += 1\n",
        "\n",
        "    new_dict = defaultdict(list)\n",
        "    for k, v in dict_author.items():\n",
        "        new_dict[v].append(k)\n",
        "\n",
        "    new_dict_2 = defaultdict(list)\n",
        "    for k, v in new_dict.items():\n",
        "        sum_per_author = np.zeros(number_of_topics)\n",
        "        n_doc = len(v)\n",
        "        for i in range(len(v)):\n",
        "            sum_per_author = sum([sum_per_author, doc_topic[v[i]]], axis=0)\n",
        "        mean_prob = sum_per_author/n_doc\n",
        "        new_dict_2[k].append(mean_prob)\n",
        "\n",
        "    # print(new_dict_2)\n",
        "    return dict(new_dict_2)\n",
        "\n",
        "def extract_content_features(doc, df):\n",
        "    # doc = [row['content']]\n",
        "    word_unigrams = word_unigram(doc, df['content'].tolist())\n",
        "    word_bigrams = word_bigram(doc, df['content'].tolist())\n",
        "    word_trigrams = word_trigram(doc, df['content'].tolist())\n",
        "    return np.concatenate([word_unigrams, word_bigrams, word_trigrams])\n",
        "\n",
        "def extract_style_features(row, df):\n",
        "    # print(row)\n",
        "    columns_to_select = ['avg_len',\n",
        "       'num_short_w', 'per_digit', 'per_cap', 'f_a', 'f_b', 'f_c', 'f_d',\n",
        "       'f_e', 'f_f', 'f_g', 'f_h', 'f_i', 'f_j', 'f_k', 'f_l', 'f_m', 'f_n',\n",
        "       'f_o', 'f_p', 'f_q', 'f_r', 'f_s', 'f_t', 'f_u', 'f_v', 'f_w', 'f_x',\n",
        "       'f_y', 'f_z', 'f_0', 'f_1', 'f_2', 'f_3', 'f_4', 'f_5', 'f_6', 'f_7',\n",
        "       'f_8', 'f_9', 'f_e_0', 'f_e_1', 'f_e_2', 'f_e_3', 'f_e_4', 'f_e_5',\n",
        "       'f_e_6', 'f_e_7', 'f_e_8', 'f_e_9', 'f_e_10', 'f_e_11', 'richness']\n",
        "    # print(row[columns_to_select].to_numpy())\n",
        "    return row[columns_to_select].to_numpy()\n",
        "\n",
        "def extract_hybrid_features(row, df):\n",
        "    # doc = row['content']\n",
        "    char_bigrams = char_bigram(row, df['content'].tolist())\n",
        "    char_trigrams = char_trigram(row, df['content'].tolist())\n",
        "    return np.concatenate([char_bigrams, char_trigrams])\n",
        "\n",
        "def extract_topic_features(row, df):\n",
        "    pass\n",
        "\n",
        "def compute_author_representation(documents, feature_extractor, df):\n",
        "    feature_vectors = feature_extractor(documents['content'].to_numpy(), df)\n",
        "    return np.mean(feature_vectors, axis=0)\n",
        "\n",
        "def compute_author_representation_style(documents, feature_extractor, df):\n",
        "    feature_vectors = np.array([feature_extractor(row, df) for idx, row in documents.iterrows()])\n",
        "    # print(feature_vectors)\n",
        "    return np.mean(feature_vectors, axis=0)\n",
        "\n",
        "def compute_inter_author_dissimilarity(dfs, feature_extractor, df, feature_type, distance_metric='cosine'):\n",
        "    author_representations = {}\n",
        "\n",
        "    if feature_type == 'Style':\n",
        "      for author, docs in dfs.items():\n",
        "        # print(compute_author_representation_style(docs, feature_extractor, df))\n",
        "        author_representations[author] = compute_author_representation_style(docs, feature_extractor, df)\n",
        "    else:\n",
        "      for author, docs in dfs.items():\n",
        "          author_representations[author] = compute_author_representation(docs, feature_extractor, df)\n",
        "\n",
        "    authors = list(author_representations.keys())\n",
        "    num_authors = len(authors)\n",
        "\n",
        "    # Calculate pairwise dissimilarity\n",
        "    dissimilarities = []\n",
        "\n",
        "    for i in range(num_authors):\n",
        "        for j in range(num_authors):\n",
        "            vec_i = author_representations[authors[i]]\n",
        "            vec_j = author_representations[authors[j]]\n",
        "\n",
        "            if distance_metric == 'cosine':\n",
        "                dissimilarity = 1 - cosine(vec_i, vec_j)\n",
        "            elif distance_metric == 'jsd':\n",
        "                dissimilarity = jensenshannon(vec_i, vec_j) ** 2\n",
        "            else:\n",
        "                raise ValueError(\"Unknown distance metric\")\n",
        "\n",
        "            dissimilarities.append(dissimilarity)\n",
        "\n",
        "    # Return the average dissimilarity\n",
        "    return np.sum(dissimilarities) / (num_authors ** 2)\n",
        "\n",
        "def compute_inter_author_dissimilarity_topics(dfs, feature_extractor, df, distance_metric='cosine'):\n",
        "    author_representations = topics_analysis(df, 20)\n",
        "\n",
        "    authors = list(author_representations.keys())\n",
        "    num_authors = len(authors)\n",
        "\n",
        "    # print(author_representations)\n",
        "    # print(authors)\n",
        "    # Calculate pairwise dissimilarity\n",
        "    dissimilarities = []\n",
        "\n",
        "    for i in range(num_authors):\n",
        "        for j in range(num_authors):\n",
        "            vec_i = author_representations[authors[i]][0]\n",
        "            vec_j = author_representations[authors[j]][0]\n",
        "            # print(vec_i[0])\n",
        "            # print(vec_j)\n",
        "\n",
        "            if distance_metric == 'cosine':\n",
        "                dissimilarity = 1 - cosine(vec_i, vec_j)\n",
        "            elif distance_metric == 'jsd':\n",
        "                vec_i = vec_i / np.sum(vec_i)\n",
        "                vec_j = vec_j / np.sum(vec_j)\n",
        "                dissimilarity = jensenshannon(vec_i, vec_j) ** 2\n",
        "            else:\n",
        "                raise ValueError(\"Unknown distance metric\")\n",
        "\n",
        "            dissimilarities.append(dissimilarity)\n",
        "\n",
        "    # Return the average dissimilarity\n",
        "    return np.sum(dissimilarities) / (num_authors ** 2)\n",
        "\n",
        "\n",
        "# datasets = {\n",
        "#     'Blog10': blog10_documents_by_author,  # Replace with actual data\n",
        "#     'Blog50': blog50_documents_by_author,\n",
        "#     # 'TuringBench': turing_documents_by_author\n",
        "# }\n",
        "\n",
        "# datasets1 = {\n",
        "#     'Blog10': nlp_test_10,  # Replace with actual data\n",
        "#     'Blog50': nlp_test_50,\n",
        "#     # 'TuringBench': turing_documents_by_author\n",
        "# }\n",
        "\n",
        "datasets = {\n",
        "    'paraphrased': paraphrased_documents_by_author,  # Replace with actual data\n",
        "    'clean': clean_documents_by_author,\n",
        "    'blogs50':blogs50_documents_by_author,\n",
        "    'imdb62':imdb62_documents_by_author,\n",
        "    # 'TuringBench': turing_documents_by_author\n",
        "}\n",
        "\n",
        "datasets1 = {\n",
        "    'paraphrased': nlp_test_paraphased,  # Replace with actual data\n",
        "    'clean': nlp_test_clean,\n",
        "    'blogs50':nlp_test_blogs50,\n",
        "    'imdb62':nlp_test_imdb62,\n",
        "   # 'TuringBench': turing_documents_by_author\n",
        "}\n",
        "\n",
        "# feature_extractors = {\n",
        "#     'Topic': extract_topic_features,\n",
        "#     'Hybrid': extract_hybrid_features,\n",
        "#     #'Content': extract_content_features,\n",
        "#     #'Style': extract_style_features,\n",
        "\n",
        "# }\n",
        "\n",
        "feature_extractors = {\n",
        "    # 'Topic': extract_topic_features,\n",
        "    'Hybrid': extract_hybrid_features,\n",
        "    # 'Content': extract_content_features,\n",
        "    # 'Style': extract_style_features,\n",
        "\n",
        "}\n",
        "\n",
        "distance_metrics = {\n",
        "    'Content': 'cosine',\n",
        "    'Style': 'cosine',\n",
        "    'Hybrid': 'cosine',\n",
        "    'Topic': 'jsd'\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for dataset_name, dfs in datasets.items():\n",
        "    results[dataset_name] = {}\n",
        "    for feature_type, extractor in feature_extractors.items():\n",
        "        print(feature_type)\n",
        "        if feature_type != 'Topic':\n",
        "          dissimilarity = compute_inter_author_dissimilarity(\n",
        "              dfs, extractor, datasets1[dataset_name], feature_type, distance_metric=distance_metrics[feature_type]\n",
        "          )\n",
        "        else:\n",
        "          dissimilarity = compute_inter_author_dissimilarity_topics(\n",
        "              dfs, extractor, datasets1[dataset_name], distance_metric=distance_metrics[feature_type]\n",
        "          )\n",
        "        print(dissimilarity)\n",
        "        results[dataset_name][feature_type] = dissimilarity\n",
        "\n",
        "for feature_type in feature_extractors.keys():\n",
        "    max_value = max(results[dataset_name][feature_type] for dataset_name in datasets.keys())\n",
        "\n",
        "    for dataset_name in datasets.keys():\n",
        "        results[dataset_name][feature_type] /= max_value\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJ4buBUU5eAw"
      },
      "outputs": [],
      "source": [
        "from __future__ import division, print_function\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.tag import map_tag\n",
        "from collections import Counter\n",
        "import nltk\n",
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from scipy.spatial.distance import cosine\n",
        "from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "import lda\n",
        "import re\n",
        "import lda.datasets\n",
        "from collections import defaultdict\n",
        "from numpy import sum\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import entropy\n",
        "from numpy.linalg import norm\n",
        "from collections import Counter\n",
        "import argparse\n",
        "import warnings\n",
        "\n",
        "np.random.seed(1337)\n",
        "\n",
        "def char_bigram(text, x_train):\n",
        "    vec = CountVectorizer(analyzer=\"char\", ngram_range=(2, 2), max_df=0.95, min_df=2, max_features=100)\n",
        "    vec.fit_transform(x_train)\n",
        "    vocab = vec.vocabulary_\n",
        "    vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(2, 2), vocabulary=vocab, max_features=100)\n",
        "    vectorizer.fit_transform(x_train)\n",
        "    feature_bigrams = vectorizer.transform(text)\n",
        "    return feature_bigrams.toarray()\n",
        "\n",
        "def char_trigram(text, x_train):\n",
        "    vec = CountVectorizer(analyzer=\"char\", ngram_range=(3, 3), max_df=0.95, min_df=2, max_features=100)\n",
        "    vec.fit_transform(x_train)\n",
        "    vocab = vec.vocabulary_\n",
        "    vectorizer = CountVectorizer(analyzer=\"char\", ngram_range=(3, 3), vocabulary=vocab, max_features=100)\n",
        "    vectorizer.fit_transform(x_train)\n",
        "    feature_trigrams = vectorizer.transform(text)\n",
        "    return feature_trigrams.toarray()\n",
        "\n",
        "def word_unigram(text, x_train):\n",
        "    # print(text)\n",
        "    vec = CountVectorizer(analyzer=\"word\", ngram_range=(1, 1), max_df=0.95, min_df=2, max_features=100, stop_words=\"english\")\n",
        "    vec.fit_transform(x_train)\n",
        "    vocab = vec.vocabulary_\n",
        "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(1, 1), vocabulary=vocab, max_features=100)\n",
        "    vectorizer.fit_transform(x_train)\n",
        "    feature_wunigrams = vectorizer.transform(text)\n",
        "    return feature_wunigrams.toarray()\n",
        "\n",
        "def word_bigram(text, x_train):\n",
        "    vec = CountVectorizer(analyzer=\"word\", ngram_range=(2, 2), max_df=0.95, min_df=2, max_features=100, stop_words=\"english\")\n",
        "    vec.fit_transform(x_train)\n",
        "    vocab = vec.vocabulary_\n",
        "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(2, 2), vocabulary=vocab, max_features=100)\n",
        "    vectorizer.fit_transform(x_train)\n",
        "    feature_wbigrams = vectorizer.transform(text)\n",
        "    return feature_wbigrams.toarray()\n",
        "\n",
        "def word_trigram(text, x_train):\n",
        "    vec = CountVectorizer(analyzer=\"word\", ngram_range=(3, 3), max_df=0.95, min_df=2, max_features=100, stop_words=\"english\")\n",
        "    vec.fit_transform(x_train)\n",
        "    vocab = vec.vocabulary_\n",
        "    vectorizer = CountVectorizer(analyzer=\"word\", ngram_range=(3, 3), vocabulary=vocab, max_features=100)\n",
        "    vectorizer.fit_transform(x_train)\n",
        "    feature_wtrigrams = vectorizer.transform(text)\n",
        "    return feature_wtrigrams.toarray()\n",
        "\n",
        "\n",
        "# Topic analysis\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\d+\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip()\n",
        "\n",
        "def sample_prompts(group, n=100):\n",
        "    return group.sample(n=n, random_state=42) if len(group) > n else group\n",
        "\n",
        "def load_diffusiondb(df):\n",
        "    # load data (can be used if data already split into train and test set)\n",
        "    df = df[['content', 'Target']]\n",
        "    # print(df)\n",
        "    df.columns = ['prompt', 'user_name']\n",
        "\n",
        "    x = df['prompt'].tolist()\n",
        "    y = df['user_name'].tolist()\n",
        "    dict_author = {}    # id doc: author_name\n",
        "    X = []\n",
        "    for i in range(len(x)):\n",
        "        X.append(clean_str(x[i]))\n",
        "        dict_author[i] = y[i]\n",
        "    return X, dict_author\n",
        "\n",
        "\n",
        "def JSD(P, Q):\n",
        "    _P = P / norm(P, ord=1)\n",
        "    _Q = Q / norm(Q, ord=1)\n",
        "    _M = 0.5 * (_P + _Q)\n",
        "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
        "\n",
        "\n",
        "def topics_analysis(df, number_of_topics):\n",
        "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "    X, dict_author = load_diffusiondb(df)\n",
        "\n",
        "    # print(X)\n",
        "    # create vocabulary\n",
        "    print (\"creating vocabulary..\")\n",
        "    print (\"---------------------------\")\n",
        "\n",
        "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=5, stop_words='english')\n",
        "    X_tf = tf_vectorizer.fit_transform(X)\n",
        "    vocab = tf_vectorizer.get_feature_names_out()\n",
        "    print(\"shape: {}\\n\".format(X_tf.shape))\n",
        "    # print(vocab)\n",
        "\n",
        "\n",
        "    # building topic model using LDA\n",
        "    print (\"building model..\")\n",
        "    print (\"---------------------------\")\n",
        "    model = lda.LDA(n_topics=number_of_topics, n_iter=500, random_state=1000)\n",
        "    model.fit(X_tf)\n",
        "    topic_word = model.topic_word_\n",
        "    print(\"shape: {}\".format(topic_word.shape))\n",
        "\n",
        "    # show detail of topic\n",
        "    n = 10\n",
        "    for i, topic_dist in enumerate(topic_word):\n",
        "        topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]\n",
        "        print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))\n",
        "\n",
        "    print (\"document topic model..\")\n",
        "    print (\"---------------------------\")\n",
        "    doc_topic = model.doc_topic_\n",
        "    topic_most = {}\n",
        "    for n in range(len(doc_topic)):\n",
        "        topic_most_pr = doc_topic[n].argmax()\n",
        "        author = dict_author[n]\n",
        "        if author in topic_most:\n",
        "            tp_most.append(topic_most_pr)\n",
        "        else:\n",
        "            tp_most = []\n",
        "            tp_most.append(topic_most_pr)\n",
        "        topic_most[author] = tp_most\n",
        "\n",
        "    i = 0\n",
        "    for author_p, topic_p in topic_most.items():\n",
        "        print (i, author_p, Counter(topic_p))\n",
        "        i += 1\n",
        "\n",
        "    new_dict = defaultdict(list)\n",
        "    for k, v in dict_author.items():\n",
        "        new_dict[v].append(k)\n",
        "\n",
        "    new_dict_2 = defaultdict(list)\n",
        "    for k, v in new_dict.items():\n",
        "        sum_per_author = np.zeros(number_of_topics)\n",
        "        n_doc = len(v)\n",
        "        for i in range(len(v)):\n",
        "            sum_per_author = sum([sum_per_author, doc_topic[v[i]]], axis=0)\n",
        "        mean_prob = sum_per_author/n_doc\n",
        "        new_dict_2[k].append(mean_prob)\n",
        "\n",
        "    # print(new_dict_2)\n",
        "    return dict(new_dict_2)\n",
        "\n",
        "def extract_content_features(doc, df):\n",
        "    # doc = [row['content']]\n",
        "    word_unigrams = word_unigram(doc, df['content'].tolist())\n",
        "    word_bigrams = word_bigram(doc, df['content'].tolist())\n",
        "    word_trigrams = word_trigram(doc, df['content'].tolist())\n",
        "    return np.concatenate([word_unigrams, word_bigrams, word_trigrams])\n",
        "\n",
        "def extract_style_features(row, df):\n",
        "    # print(row)\n",
        "    columns_to_select = ['avg_len',\n",
        "       'num_short_w', 'per_digit', 'per_cap', 'f_a', 'f_b', 'f_c', 'f_d',\n",
        "       'f_e', 'f_f', 'f_g', 'f_h', 'f_i', 'f_j', 'f_k', 'f_l', 'f_m', 'f_n',\n",
        "       'f_o', 'f_p', 'f_q', 'f_r', 'f_s', 'f_t', 'f_u', 'f_v', 'f_w', 'f_x',\n",
        "       'f_y', 'f_z', 'f_0', 'f_1', 'f_2', 'f_3', 'f_4', 'f_5', 'f_6', 'f_7',\n",
        "       'f_8', 'f_9', 'f_e_0', 'f_e_1', 'f_e_2', 'f_e_3', 'f_e_4', 'f_e_5',\n",
        "       'f_e_6', 'f_e_7', 'f_e_8', 'f_e_9', 'f_e_10', 'f_e_11', 'richness']\n",
        "    # print(row[columns_to_select].to_numpy())\n",
        "    return row[columns_to_select].to_numpy()\n",
        "\n",
        "def extract_hybrid_features(row, df):\n",
        "    # doc = row['content']\n",
        "    char_bigrams = char_bigram(row, df['content'].tolist())\n",
        "    char_trigrams = char_trigram(row, df['content'].tolist())\n",
        "    return np.concatenate([char_bigrams, char_trigrams])\n",
        "\n",
        "def extract_topic_features(row, df):\n",
        "    pass\n",
        "\n",
        "def compute_author_representation(documents, feature_extractor, df):\n",
        "    feature_vectors = feature_extractor(documents['content'].to_numpy(), df)\n",
        "    return np.mean(feature_vectors, axis=0)\n",
        "\n",
        "def compute_author_representation_style(documents, feature_extractor, df):\n",
        "    feature_vectors = np.array([feature_extractor(row, df) for idx, row in documents.iterrows()])\n",
        "    # print(feature_vectors)\n",
        "    return np.mean(feature_vectors, axis=0)\n",
        "\n",
        "def compute_inter_author_dissimilarity(dfs, feature_extractor, df, feature_type, distance_metric='cosine'):\n",
        "    author_representations = {}\n",
        "\n",
        "    if feature_type == 'Style':\n",
        "      for author, docs in dfs.items():\n",
        "        # print(compute_author_representation_style(docs, feature_extractor, df))\n",
        "        author_representations[author] = compute_author_representation_style(docs, feature_extractor, df)\n",
        "    else:\n",
        "      for author, docs in dfs.items():\n",
        "          author_representations[author] = compute_author_representation(docs, feature_extractor, df)\n",
        "\n",
        "    authors = list(author_representations.keys())\n",
        "    num_authors = len(authors)\n",
        "\n",
        "    # Calculate pairwise dissimilarity\n",
        "    dissimilarities = []\n",
        "\n",
        "    for i in range(num_authors):\n",
        "        for j in range(num_authors):\n",
        "            vec_i = author_representations[authors[i]]\n",
        "            vec_j = author_representations[authors[j]]\n",
        "\n",
        "            if distance_metric == 'cosine':\n",
        "                dissimilarity = 1 - cosine(vec_i, vec_j)\n",
        "            elif distance_metric == 'jsd':\n",
        "                dissimilarity = jensenshannon(vec_i, vec_j) ** 2\n",
        "            else:\n",
        "                raise ValueError(\"Unknown distance metric\")\n",
        "\n",
        "            dissimilarities.append(dissimilarity)\n",
        "\n",
        "    # Return the average dissimilarity\n",
        "    return np.sum(dissimilarities) / (num_authors ** 2)\n",
        "\n",
        "def compute_inter_author_dissimilarity_topics(dfs, feature_extractor, df, distance_metric='cosine'):\n",
        "    author_representations = topics_analysis(df, 20)\n",
        "\n",
        "    authors = list(author_representations.keys())\n",
        "    num_authors = len(authors)\n",
        "\n",
        "    # print(author_representations)\n",
        "    # print(authors)\n",
        "    # Calculate pairwise dissimilarity\n",
        "    dissimilarities = []\n",
        "\n",
        "    for i in range(num_authors):\n",
        "        for j in range(num_authors):\n",
        "            vec_i = author_representations[authors[i]][0]\n",
        "            vec_j = author_representations[authors[j]][0]\n",
        "            # print(vec_i[0])\n",
        "            # print(vec_j)\n",
        "\n",
        "            if distance_metric == 'cosine':\n",
        "                dissimilarity = 1 - cosine(vec_i, vec_j)\n",
        "            elif distance_metric == 'jsd':\n",
        "                vec_i = vec_i / np.sum(vec_i)\n",
        "                vec_j = vec_j / np.sum(vec_j)\n",
        "                dissimilarity = jensenshannon(vec_i, vec_j) ** 2\n",
        "            else:\n",
        "                raise ValueError(\"Unknown distance metric\")\n",
        "\n",
        "            dissimilarities.append(dissimilarity)\n",
        "\n",
        "    # Return the average dissimilarity\n",
        "    return np.sum(dissimilarities) / (num_authors ** 2)\n",
        "\n",
        "\n",
        "# datasets = {\n",
        "#     'Blog10': blog10_documents_by_author,  # Replace with actual data\n",
        "#     'Blog50': blog50_documents_by_author,\n",
        "#     # 'TuringBench': turing_documents_by_author\n",
        "# }\n",
        "\n",
        "# datasets1 = {\n",
        "#     'Blog10': nlp_test_10,  # Replace with actual data\n",
        "#     'Blog50': nlp_test_50,\n",
        "#     # 'TuringBench': turing_documents_by_author\n",
        "# }\n",
        "\n",
        "datasets = {\n",
        "    'paraphrased': paraphrased_documents_by_author,  # Replace with actual data\n",
        "    'clean': clean_documents_by_author,\n",
        "    'blogs50':blogs50_documents_by_author,\n",
        "    'imdb62':imdb62_documents_by_author,\n",
        "    # 'TuringBench': turing_documents_by_author\n",
        "}\n",
        "\n",
        "datasets1 = {\n",
        "    'paraphrased': nlp_test_paraphased,  # Replace with actual data\n",
        "    'clean': nlp_test_clean,\n",
        "    'blogs50':nlp_test_blogs50,\n",
        "    'imdb62':nlp_test_imdb62,\n",
        "   # 'TuringBench': turing_documents_by_author\n",
        "}\n",
        "\n",
        "# feature_extractors = {\n",
        "#     'Topic': extract_topic_features,\n",
        "#     'Hybrid': extract_hybrid_features,\n",
        "#     #'Content': extract_content_features,\n",
        "#     #'Style': extract_style_features,\n",
        "\n",
        "# }\n",
        "\n",
        "feature_extractors = {\n",
        "    'Topic': extract_topic_features,\n",
        "    # 'Hybrid': extract_hybrid_features,\n",
        "    'Content': extract_content_features,\n",
        "    # 'Style': extract_style_features,\n",
        "\n",
        "}\n",
        "\n",
        "distance_metrics = {\n",
        "    'Content': 'cosine',\n",
        "    'Style': 'cosine',\n",
        "    'Hybrid': 'cosine',\n",
        "    'Topic': 'jsd'\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for dataset_name, dfs in datasets.items():\n",
        "    results[dataset_name] = {}\n",
        "    for feature_type, extractor in feature_extractors.items():\n",
        "        print(feature_type)\n",
        "        if feature_type != 'Topic':\n",
        "          dissimilarity = compute_inter_author_dissimilarity(\n",
        "              dfs, extractor, datasets1[dataset_name], feature_type, distance_metric=distance_metrics[feature_type]\n",
        "          )\n",
        "        else:\n",
        "          dissimilarity = compute_inter_author_dissimilarity_topics(\n",
        "              dfs, extractor, datasets1[dataset_name], distance_metric=distance_metrics[feature_type]\n",
        "          )\n",
        "        print(dissimilarity)\n",
        "        results[dataset_name][feature_type] = dissimilarity\n",
        "\n",
        "for feature_type in feature_extractors.keys():\n",
        "    max_value = max(results[dataset_name][feature_type] for dataset_name in datasets.keys())\n",
        "\n",
        "    for dataset_name in datasets.keys():\n",
        "        results[dataset_name][feature_type] /= max_value\n",
        "\n",
        "print(results)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q8snT8kl_dUg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "hpW7_8AtrpS6",
        "p9AzA-KfsFEj",
        "0jkIU-yvqGgS",
        "GBuLHq2em9lz",
        "GmCODjLet87P",
        "nvbgIxe7n0_F",
        "pBOSJp0cqbGJ",
        "9qujt9hSjvoV",
        "h6igtquAUiyP",
        "UOv21bEK6sPy",
        "dCcC6-aos5kH"
      ],
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}