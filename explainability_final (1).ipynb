{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install captum"
      ],
      "metadata": {
        "id": "PnhPUoy1BXMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ace_tools"
      ],
      "metadata": {
        "id": "2vv0BACdJ43J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import tarfile\n",
        "import gdown\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset, Sampler\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import matplotlib.font_manager as fm\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.collections import QuadMesh\n",
        "import seaborn as sn\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from transformers import BertModel, BertForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "metadata": {
        "id": "EIFDFlvmKFUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "45MQsn6-B5HG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from captum.attr import LayerIntegratedGradients, visualization as viz\n",
        "import joblib\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Logistic Regression Classifier from the image\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0):\n",
        "        super().__init__()\n",
        "        print(f'Logistic Regression classifier of dim ({in_dim} {hid_dim} {out_dim})')\n",
        "\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_dim, hid_dim, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hid_dim, out_dim, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_feat=False):\n",
        "        out = self.nn(x)\n",
        "        if return_feat:\n",
        "            return out, x\n",
        "        return out\n",
        "\n",
        "\n",
        "# BertClassifier class from the image\n",
        "class BertClassifier(nn.Module):\n",
        "    FEAT_LEN = 768\n",
        "\n",
        "    def __init__(self, raw_bert, classifier):\n",
        "        super().__init__()\n",
        "        self.bert = raw_bert\n",
        "        self.fc = classifier\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # BERT model forward pass\n",
        "        feature = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # print(feature.last_hidden_state.shape)\n",
        "        out = self.fc(feature.last_hidden_state.flatten(1))  # Flatten [CLS] token representation\n",
        "        return out, feature.last_hidden_state.flatten(1)\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100  # Adjust out_dim as necessary\n",
        "\n",
        "# Use the LogisticRegression and BertClassifier from the image\n",
        "classifier = LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout)\n",
        "model = BertClassifier(extractor, classifier)\n",
        "\n",
        "# Load the model weights (assuming they are for the updated model)\n",
        "model_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_try2/style_encoder_supcon_9.pt'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Define the forward function for prediction\n",
        "def predict(inputs, attention_mask=None):\n",
        "    output, _ = model(inputs, attention_mask=attention_mask)\n",
        "    return output\n",
        "\n",
        "# Forward function for classification used by LayerIntegratedGradients\n",
        "def classify_forward_func(inputs, attention_mask=None):\n",
        "    logits = predict(inputs, attention_mask=attention_mask)\n",
        "    return logits\n",
        "\n",
        "# Reference token IDs\n",
        "ref_token_id = tokenizer.pad_token_id\n",
        "sep_token_id = tokenizer.sep_token_id\n",
        "cls_token_id = tokenizer.cls_token_id\n",
        "\n",
        "# def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
        "#     text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "#     input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
        "#     ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
        "#     return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device)\n",
        "\n",
        "# def construct_attention_mask(input_ids):\n",
        "#     return torch.ones_like(input_ids)\n",
        "\n",
        "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, max_length=256):\n",
        "    # Tokenize the text with padding\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',  # Pad to max length\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # Get input_ids and attention_mask\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Create reference input filled with the pad token but with [CLS] and [SEP] tokens in their places\n",
        "    ref_input_ids = input_ids.clone()\n",
        "    ref_input_ids[:, 1:-1] = ref_token_id  # Fill everything except [CLS] and [SEP] with the pad token\n",
        "    ref_input_ids[:, 0] = cls_token_id  # [CLS] at the beginning\n",
        "    ref_input_ids[:, -1] = sep_token_id  # [SEP] at the end\n",
        "\n",
        "    return input_ids, ref_input_ids, attention_mask\n",
        "\n",
        "\n",
        "# Load the test dataframe\n",
        "nlp_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "nlp_test = nlp_test[['prompt', 'user_name']]\n",
        "nlp_test.columns = ['content', 'Target']\n",
        "# encoder_path = '/content/drive/MyDrive/label_encoder_80.pkl'\n",
        "# label_encoder = joblib.load(encoder_path)\n",
        "\n",
        "delta_all = []\n",
        "attributions_sum_all = []\n",
        "\n",
        "# Run Layer Integrated Gradients on a sample\n",
        "for idx, row in tqdm(nlp_test.iterrows(), total=len(nlp_test)):\n",
        "    # if idx == 10:\n",
        "    #     break\n",
        "    text = row['content']\n",
        "    label = row['Target']\n",
        "    # label_encoded = label_encoder.transform([label])\n",
        "\n",
        "    input_ids, ref_input_ids, attention_mask = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
        "    # attention_mask = construct_attention_mask(input_ids)\n",
        "\n",
        "    # Get predictions\n",
        "    logits = predict(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Target label index\n",
        "    target_label_idx = torch.argmax(logits).item()\n",
        "\n",
        "    # Layer Integrated Gradients\n",
        "    lig = LayerIntegratedGradients(classify_forward_func, model.bert.embeddings)\n",
        "\n",
        "    # Compute attributions with respect to the BERT embeddings\n",
        "    attributions, delta = lig.attribute(\n",
        "        inputs=input_ids.long(),\n",
        "        baselines=ref_input_ids,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        target=target_label_idx,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "    # Summarize attributions\n",
        "    def summarize_attributions(attributions):\n",
        "        attributions = attributions.sum(dim=-1).squeeze(0)\n",
        "        attributions = attributions / torch.norm(attributions)\n",
        "        return attributions\n",
        "\n",
        "    attributions_sum = summarize_attributions(attributions)\n",
        "    # print(attributions_sum.shape)\n",
        "\n",
        "    delta_all.append(delta)\n",
        "\n",
        "\n",
        "    def remove_pad_tokens(tokens, attributions, pad_token_id):\n",
        "        filtered_tokens = []\n",
        "        filtered_attributions = []\n",
        "        for token, attr in zip(tokens, attributions):\n",
        "            if token != tokenizer.pad_token:  # Check if the token is not a pad token\n",
        "                filtered_tokens.append(token)\n",
        "                filtered_attributions.append(attr)  # Convert tensor to value\n",
        "        return filtered_tokens, filtered_attributions\n",
        "\n",
        "    # Visualize\n",
        "    indices = input_ids[0].detach().tolist()\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
        "\n",
        "    filtered_tokens, filtered_attributions = remove_pad_tokens(all_tokens, attributions_sum.tolist(), tokenizer.pad_token_id)\n",
        "    attributions_sum_all.append(filtered_attributions)\n",
        "\n",
        "    # Create a visualization record without pad tokens\n",
        "    classification_vis = viz.VisualizationDataRecord(\n",
        "        torch.tensor(filtered_attributions),\n",
        "        torch.max(torch.softmax(logits[0], dim=0)),\n",
        "        torch.argmax(logits),\n",
        "        label,\n",
        "        str(label),\n",
        "        torch.tensor(filtered_attributions).sum(),\n",
        "        filtered_tokens,\n",
        "        delta\n",
        "    )\n",
        "    # classification_vis = viz.VisualizationDataRecord(\n",
        "    #     attributions_sum,\n",
        "    #     torch.max(torch.softmax(logits[0], dim=0)),\n",
        "    #     torch.argmax(logits),\n",
        "    #     label_encoded,\n",
        "    #     str(label_encoded),\n",
        "    #     attributions_sum.sum(),\n",
        "    #     all_tokens,\n",
        "    #     delta\n",
        "    # )\n",
        "\n",
        "    print(f'\\033[1mVisualization For Sample {idx}\\033[0m')\n",
        "    viz.visualize_text([classification_vis])\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "CJrMbD1t2Lpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from captum.attr import LayerIntegratedGradients, visualization as viz\n",
        "import joblib\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Logistic Regression Classifier from the image\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0):\n",
        "        super().__init__()\n",
        "        print(f'Logistic Regression classifier of dim ({in_dim} {hid_dim} {out_dim})')\n",
        "\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_dim, hid_dim, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hid_dim, out_dim, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_feat=False):\n",
        "        out = self.nn(x)\n",
        "        if return_feat:\n",
        "            return out, x\n",
        "        return out\n",
        "\n",
        "\n",
        "# BertClassifier class from the image\n",
        "class BertClassifier(nn.Module):\n",
        "    FEAT_LEN = 768\n",
        "\n",
        "    def __init__(self, raw_bert, classifier):\n",
        "        super().__init__()\n",
        "        self.bert = raw_bert\n",
        "        self.fc = classifier\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # BERT model forward pass\n",
        "        feature = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # print(feature.last_hidden_state.shape)\n",
        "        out = self.fc(feature.last_hidden_state.flatten(1))  # Flatten [CLS] token representation\n",
        "        return out, feature.last_hidden_state.flatten(1)\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100  # Adjust out_dim as necessary\n",
        "\n",
        "# Use the LogisticRegression and BertClassifier from the image\n",
        "classifier = LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout)\n",
        "model = BertClassifier(extractor, classifier)\n",
        "\n",
        "# Load the model weights (assuming they are for the updated model)\n",
        "model_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_try2/style_encoder_supcon_9.pt'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Define the forward function for prediction\n",
        "def predict(inputs, attention_mask=None):\n",
        "    output, _ = model(inputs, attention_mask=attention_mask)\n",
        "    return output\n",
        "\n",
        "# Forward function for classification used by LayerIntegratedGradients\n",
        "def classify_forward_func(inputs, attention_mask=None):\n",
        "    logits = predict(inputs, attention_mask=attention_mask)\n",
        "    return logits\n",
        "\n",
        "# Reference token IDs\n",
        "ref_token_id = tokenizer.pad_token_id\n",
        "sep_token_id = tokenizer.sep_token_id\n",
        "cls_token_id = tokenizer.cls_token_id\n",
        "\n",
        "# def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
        "#     text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "#     input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
        "#     ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
        "#     return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device)\n",
        "\n",
        "# def construct_attention_mask(input_ids):\n",
        "#     return torch.ones_like(input_ids)\n",
        "\n",
        "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, max_length=256):\n",
        "    # Tokenize the text with padding\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',  # Pad to max length\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # Get input_ids and attention_mask\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Create reference input filled with the pad token but with [CLS] and [SEP] tokens in their places\n",
        "    ref_input_ids = input_ids.clone()\n",
        "    ref_input_ids[:, 1:-1] = ref_token_id  # Fill everything except [CLS] and [SEP] with the pad token\n",
        "    ref_input_ids[:, 0] = cls_token_id  # [CLS] at the beginning\n",
        "    ref_input_ids[:, -1] = sep_token_id  # [SEP] at the end\n",
        "\n",
        "    return input_ids, ref_input_ids, attention_mask\n",
        "\n",
        "\n",
        "# Load the test dataframe\n",
        "nlp_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "nlp_test = nlp_test[['prompt', 'user_name']]\n",
        "nlp_test.columns = ['content', 'Target']\n",
        "# encoder_path = '/content/drive/MyDrive/label_encoder_80.pkl'\n",
        "# label_encoder = joblib.load(encoder_path)\n",
        "\n",
        "delta_all = []\n",
        "attributions_sum_all = []\n",
        "\n",
        "# Run Layer Integrated Gradients on a sample\n",
        "for idx, row in tqdm(nlp_test.iterrows(), total=len(nlp_test)):\n",
        "    text = row['content']\n",
        "    label = row['Target']\n",
        "\n",
        "    input_ids, ref_input_ids, attention_mask = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
        "\n",
        "    # Get predictions\n",
        "    logits = predict(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Target label index\n",
        "    target_label_idx = torch.argmax(logits).item()\n",
        "\n",
        "    # Layer Integrated Gradients\n",
        "    lig = LayerIntegratedGradients(classify_forward_func, model.bert.embeddings)\n",
        "\n",
        "    # Compute attributions with respect to the BERT embeddings\n",
        "    attributions, delta = lig.attribute(\n",
        "        inputs=input_ids.long(),\n",
        "        baselines=ref_input_ids,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        target=target_label_idx,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "    # Summarize attributions\n",
        "    def summarize_attributions(attributions):\n",
        "        attributions = attributions.sum(dim=-1).squeeze(0)\n",
        "        attributions = attributions / torch.norm(attributions)\n",
        "        return attributions\n",
        "\n",
        "    attributions_sum = summarize_attributions(attributions)\n",
        "\n",
        "    # Move delta to CPU and detach it\n",
        "    delta_cpu = delta.cpu().detach().numpy()\n",
        "    delta_all.append(delta_cpu)\n",
        "\n",
        "    def remove_pad_tokens(tokens, attributions, pad_token_id):\n",
        "        filtered_tokens = []\n",
        "        filtered_attributions = []\n",
        "        for token, attr in zip(tokens, attributions):\n",
        "            if token != tokenizer.pad_token:  # Check if the token is not a pad token\n",
        "                filtered_tokens.append(token)\n",
        "                filtered_attributions.append(attr)  # Convert tensor to value\n",
        "        return filtered_tokens, filtered_attributions\n",
        "\n",
        "    # Convert tokens and attributions to CPU-friendly format\n",
        "    indices = input_ids[0].detach().tolist()\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
        "\n",
        "    filtered_tokens, filtered_attributions = remove_pad_tokens(all_tokens, attributions_sum.tolist(), tokenizer.pad_token_id)\n",
        "\n",
        "    # Convert filtered_attributions to CPU-friendly format\n",
        "    filtered_attributions = torch.tensor(filtered_attributions).cpu().detach().numpy().tolist()\n",
        "    attributions_sum_all.append(filtered_attributions)\n",
        "\n",
        "    # Create a visualization record without pad tokens\n",
        "    classification_vis = viz.VisualizationDataRecord(\n",
        "        torch.tensor(filtered_attributions),\n",
        "        torch.max(torch.softmax(logits[0], dim=0)),\n",
        "        torch.argmax(logits),\n",
        "        label,\n",
        "        str(label),\n",
        "        torch.tensor(filtered_attributions).sum(),\n",
        "        filtered_tokens,\n",
        "        delta_cpu\n",
        "    )\n",
        "\n",
        "    print(f'\\033[1mVisualization For Sample {idx}\\033[0m')\n",
        "    viz.visualize_text([classification_vis])\n",
        "    print()\n",
        "\n",
        "# Save the delta and attributions_sum_all locally\n",
        "import joblib\n",
        "\n",
        "joblib.dump(delta_all, 'delta_all.pkl')\n",
        "joblib.dump(attributions_sum_all, 'attributions_sum_all.pkl')\n",
        "\n"
      ],
      "metadata": {
        "id": "gwiwQUi1_hm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from captum.attr import LayerIntegratedGradients, visualization as viz\n",
        "import joblib\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Logistic Regression Classifier from the image\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0):\n",
        "        super().__init__()\n",
        "        print(f'Logistic Regression classifier of dim ({in_dim} {hid_dim} {out_dim})')\n",
        "\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_dim, hid_dim, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hid_dim, out_dim, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_feat=False):\n",
        "        out = self.nn(x)\n",
        "        if return_feat:\n",
        "            return out, x\n",
        "        return out\n",
        "\n",
        "\n",
        "# BertClassifier class from the image\n",
        "class BertClassifier(nn.Module):\n",
        "    FEAT_LEN = 768\n",
        "\n",
        "    def __init__(self, raw_bert, classifier):\n",
        "        super().__init__()\n",
        "        self.bert = raw_bert\n",
        "        self.fc = classifier\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # BERT model forward pass\n",
        "        feature = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # print(feature.last_hidden_state.shape)\n",
        "        out = self.fc(feature.last_hidden_state.flatten(1))  # Flatten [CLS] token representation\n",
        "        return out, feature.last_hidden_state.flatten(1)\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100  # Adjust out_dim as necessary\n",
        "\n",
        "# Use the LogisticRegression and BertClassifier from the image\n",
        "classifier = LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout)\n",
        "model = BertClassifier(extractor, classifier)\n",
        "\n",
        "# Load the model weights (assuming they are for the updated model)\n",
        "model_path = '/content/drive/MyDrive/msc_project/model/contrastive/club_try2/style_encoder_supcon_9.pt'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Define the forward function for prediction\n",
        "def predict(inputs, attention_mask=None):\n",
        "    output, _ = model(inputs, attention_mask=attention_mask)\n",
        "    return output\n",
        "\n",
        "# Forward function for classification used by LayerIntegratedGradients\n",
        "def classify_forward_func(inputs, attention_mask=None):\n",
        "    logits = predict(inputs, attention_mask=attention_mask)\n",
        "    return logits\n",
        "\n",
        "# Reference token IDs\n",
        "ref_token_id = tokenizer.pad_token_id\n",
        "sep_token_id = tokenizer.sep_token_id\n",
        "cls_token_id = tokenizer.cls_token_id\n",
        "\n",
        "# def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
        "#     text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "#     input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
        "#     ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
        "#     return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device)\n",
        "\n",
        "# def construct_attention_mask(input_ids):\n",
        "#     return torch.ones_like(input_ids)\n",
        "\n",
        "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, max_length=256):\n",
        "    # Tokenize the text with padding\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',  # Pad to max length\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # Get input_ids and attention_mask\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Create reference input filled with the pad token but with [CLS] and [SEP] tokens in their places\n",
        "    ref_input_ids = input_ids.clone()\n",
        "    ref_input_ids[:, 1:-1] = ref_token_id  # Fill everything except [CLS] and [SEP] with the pad token\n",
        "    ref_input_ids[:, 0] = cls_token_id  # [CLS] at the beginning\n",
        "    ref_input_ids[:, -1] = sep_token_id  # [SEP] at the end\n",
        "\n",
        "    return input_ids, ref_input_ids, attention_mask\n",
        "\n",
        "\n",
        "# Load the test dataframe\n",
        "nlp_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "nlp_test = nlp_test[['prompt', 'user_name']]\n",
        "nlp_test.columns = ['content', 'Target']\n",
        "# encoder_path = '/content/drive/MyDrive/label_encoder_80.pkl'\n",
        "# label_encoder = joblib.load(encoder_path)\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Dictionary to hold attribution scores per author\n",
        "# Structure: {author_name: {token: [list_of_attributions]}}\n",
        "author_attributions = defaultdict(lambda: defaultdict(list))\n",
        "delta_all = []\n",
        "attributions_sum_all = []\n",
        "\n",
        "# Run Layer Integrated Gradients on a sample and accumulate attributions\n",
        "for idx, row in tqdm(nlp_test.iterrows(), total=len(nlp_test)):\n",
        "    text = row['content']\n",
        "    author = row['Target']  # 'Target' corresponds to the author\n",
        "\n",
        "    input_ids, ref_input_ids, attention_mask = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
        "\n",
        "    # Get predictions\n",
        "    logits = predict(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Target label index\n",
        "    target_label_idx = torch.argmax(logits).item()\n",
        "\n",
        "    # Layer Integrated Gradients\n",
        "    lig = LayerIntegratedGradients(classify_forward_func, model.bert.embeddings)\n",
        "\n",
        "    # Compute attributions with respect to the BERT embeddings\n",
        "    attributions, delta = lig.attribute(\n",
        "        inputs=input_ids.long(),\n",
        "        baselines=ref_input_ids,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        target=target_label_idx,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "    # Summarize attributions\n",
        "    def summarize_attributions(attributions):\n",
        "        attributions = attributions.sum(dim=-1).squeeze(0)\n",
        "        attributions = attributions / torch.norm(attributions)\n",
        "        return attributions\n",
        "\n",
        "    attributions_sum = summarize_attributions(attributions)\n",
        "\n",
        "    # Move delta to CPU and detach it\n",
        "    delta_cpu = delta.cpu().detach().numpy()\n",
        "    delta_all.append(delta_cpu)\n",
        "\n",
        "    def remove_pad_tokens(tokens, attributions, pad_token_id):\n",
        "        filtered_tokens = []\n",
        "        filtered_attributions = []\n",
        "        for token, attr in zip(tokens, attributions):\n",
        "            if token != tokenizer.pad_token:  # Check if the token is not a pad token\n",
        "                filtered_tokens.append(token)\n",
        "                filtered_attributions.append(attr)  # Convert tensor to value\n",
        "        return filtered_tokens, filtered_attributions\n",
        "\n",
        "    # Convert tokens and attributions to CPU-friendly format\n",
        "    indices = input_ids[0].detach().tolist()\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
        "\n",
        "    filtered_tokens, filtered_attributions = remove_pad_tokens(all_tokens, attributions_sum.tolist(), tokenizer.pad_token_id)\n",
        "\n",
        "    # Convert filtered_attributions to CPU-friendly format\n",
        "    filtered_attributions = torch.tensor(filtered_attributions).cpu().detach().numpy().tolist()\n",
        "    attributions_sum_all.append(filtered_attributions)\n",
        "\n",
        "    # Accumulate attribution scores for each token for the current author\n",
        "    for token, attribution in zip(filtered_tokens, filtered_attributions):\n",
        "        author_attributions[author][token].append(attribution)\n",
        "\n",
        "# Now calculate the average attribution score for each token per author\n",
        "author_avg_attributions = {}\n",
        "\n",
        "for author, token_attributions in author_attributions.items():\n",
        "    author_avg_attributions[author] = {}\n",
        "    for token, attributions in token_attributions.items():\n",
        "        # Calculate the average attribution score for each token\n",
        "        avg_attribution = np.mean(attributions)\n",
        "        author_avg_attributions[author][token] = avg_attribution\n",
        "\n",
        "# # Example output: print the average attributions for each author\n",
        "# for author, token_avg_attributions in author_avg_attributions.items():\n",
        "#     print(f\"\\nAuthor: {author}\")\n",
        "#     for token, avg_attribution in token_avg_attributions.items():\n",
        "#         print(f\"Token: {token}, Average Attribution: {avg_attribution}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "SY0CMQ-eMn-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "author_avg_attributions_sorted = {}\n",
        "\n",
        "for author, token_attributions in author_attributions.items():\n",
        "    author_avg_attributions = {}\n",
        "    for token, attributions in token_attributions.items():\n",
        "        # Calculate the average attribution score for each token\n",
        "        avg_attribution = np.mean(attributions)\n",
        "        author_avg_attributions[token] = avg_attribution\n",
        "\n",
        "    # Sort tokens by average attribution score in descending order\n",
        "    sorted_tokens = sorted(author_avg_attributions.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Store sorted tokens for the author\n",
        "    author_avg_attributions_sorted[author] = sorted_tokens\n",
        "\n",
        "# # Example output: print the sorted average attributions for each author\n",
        "# for author, sorted_tokens in author_avg_attributions_sorted.items():\n",
        "#     print(f\"\\nAuthor: {author}\")\n",
        "#     for token, avg_attribution in sorted_tokens:\n",
        "#         print(f\"Token: {token}, Average Attribution: {avg_attribution}\")"
      ],
      "metadata": {
        "id": "nhO4yHFLReGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example output: print the sorted average attributions for each author\n",
        "for author, sorted_tokens in author_avg_attributions_sorted.items():\n",
        "    print(f\"\\nAuthor: {author}\")\n",
        "    count = 0\n",
        "    for token, avg_attribution in sorted_tokens:\n",
        "        print(f\"Token: {token}, Average Attribution: {avg_attribution}\")\n",
        "        count += 1\n",
        "        if count == 10:\n",
        "          break"
      ],
      "metadata": {
        "id": "RK9g2PJUqqft"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from captum.attr import LayerIntegratedGradients, visualization as viz\n",
        "import joblib\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Logistic Regression Classifier from the image\n",
        "class LogisticRegression(nn.Module):\n",
        "    def __init__(self, in_dim, hid_dim, out_dim, dropout=0):\n",
        "        super().__init__()\n",
        "        print(f'Logistic Regression classifier of dim ({in_dim} {hid_dim} {out_dim})')\n",
        "\n",
        "        self.nn = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_dim, hid_dim, bias=True),\n",
        "            nn.LeakyReLU(negative_slope=0.2, inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(hid_dim, out_dim, bias=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, return_feat=False):\n",
        "        out = self.nn(x)\n",
        "        if return_feat:\n",
        "            return out, x\n",
        "        return out\n",
        "\n",
        "\n",
        "# BertClassifier class from the image\n",
        "class BertClassifier(nn.Module):\n",
        "    FEAT_LEN = 768\n",
        "\n",
        "    def __init__(self, raw_bert, classifier):\n",
        "        super().__init__()\n",
        "        self.bert = raw_bert\n",
        "        self.fc = classifier\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        # BERT model forward pass\n",
        "        feature = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        # print(feature.last_hidden_state.shape)\n",
        "        out = self.fc(feature.last_hidden_state.flatten(1))  # Flatten [CLS] token representation\n",
        "        return out, feature.last_hidden_state.flatten(1)\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "extractor = BertModel.from_pretrained('bert-base-cased')\n",
        "ngpus, dropout = torch.cuda.device_count(), 0.35\n",
        "num_tokens, hidden_dim, out_dim = 256, 512, 100  # Adjust out_dim as necessary\n",
        "\n",
        "# Use the LogisticRegression and BertClassifier from the image\n",
        "classifier = LogisticRegression(768 * num_tokens, hidden_dim, out_dim, dropout=dropout)\n",
        "model = BertClassifier(extractor, classifier)\n",
        "\n",
        "# Load the model weights (assuming they are for the updated model)\n",
        "model_path = '/content/drive/MyDrive/msc_project/model/contrastive/contrax/exp_data/diffusiondb100_cls_para_bert-base-cased_coe0.0_temp0.1_unit2_epoch30/diffusiondb100_cls_para_val0.72073_e24.pt'\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Define the forward function for prediction\n",
        "def predict(inputs, attention_mask=None):\n",
        "    output, _ = model(inputs, attention_mask=attention_mask)\n",
        "    return output\n",
        "\n",
        "# Forward function for classification used by LayerIntegratedGradients\n",
        "def classify_forward_func(inputs, attention_mask=None):\n",
        "    logits = predict(inputs, attention_mask=attention_mask)\n",
        "    return logits\n",
        "\n",
        "# Reference token IDs\n",
        "ref_token_id = tokenizer.pad_token_id\n",
        "sep_token_id = tokenizer.sep_token_id\n",
        "cls_token_id = tokenizer.cls_token_id\n",
        "\n",
        "# def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
        "#     text_ids = tokenizer.encode(text, add_special_tokens=False)\n",
        "#     input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
        "#     ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
        "#     return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device)\n",
        "\n",
        "# def construct_attention_mask(input_ids):\n",
        "#     return torch.ones_like(input_ids)\n",
        "\n",
        "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id, max_length=256):\n",
        "    # Tokenize the text with padding\n",
        "    inputs = tokenizer.encode_plus(\n",
        "        text,\n",
        "        add_special_tokens=True,\n",
        "        max_length=max_length,\n",
        "        padding='max_length',  # Pad to max length\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\",\n",
        "        return_attention_mask=True\n",
        "    )\n",
        "\n",
        "    # Get input_ids and attention_mask\n",
        "    input_ids = inputs['input_ids'].to(device)\n",
        "    attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "    # Create reference input filled with the pad token but with [CLS] and [SEP] tokens in their places\n",
        "    ref_input_ids = input_ids.clone()\n",
        "    ref_input_ids[:, 1:-1] = ref_token_id  # Fill everything except [CLS] and [SEP] with the pad token\n",
        "    ref_input_ids[:, 0] = cls_token_id  # [CLS] at the beginning\n",
        "    ref_input_ids[:, -1] = sep_token_id  # [SEP] at the end\n",
        "\n",
        "    return input_ids, ref_input_ids, attention_mask\n",
        "\n",
        "\n",
        "# Load the test dataframe\n",
        "nlp_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n",
        "nlp_test = nlp_test[['prompt', 'user_name']]\n",
        "nlp_test.columns = ['content', 'Target']\n",
        "# encoder_path = '/content/drive/MyDrive/label_encoder_80.pkl'\n",
        "# label_encoder = joblib.load(encoder_path)\n",
        "\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Dictionary to hold attribution scores per author\n",
        "# Structure: {author_name: {token: [list_of_attributions]}}\n",
        "author_attributions = defaultdict(lambda: defaultdict(list))\n",
        "delta_all = []\n",
        "attributions_sum_all = []\n",
        "\n",
        "# Run Layer Integrated Gradients on a sample and accumulate attributions\n",
        "for idx, row in tqdm(nlp_test.iterrows(), total=len(nlp_test)):\n",
        "    text = row['content']\n",
        "    author = row['Target']  # 'Target' corresponds to the author\n",
        "\n",
        "    input_ids, ref_input_ids, attention_mask = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
        "\n",
        "    # Get predictions\n",
        "    logits = predict(input_ids, attention_mask=attention_mask)\n",
        "\n",
        "    # Target label index\n",
        "    target_label_idx = torch.argmax(logits).item()\n",
        "\n",
        "    # Layer Integrated Gradients\n",
        "    lig = LayerIntegratedGradients(classify_forward_func, model.bert.embeddings)\n",
        "\n",
        "    # Compute attributions with respect to the BERT embeddings\n",
        "    attributions, delta = lig.attribute(\n",
        "        inputs=input_ids.long(),\n",
        "        baselines=ref_input_ids,\n",
        "        additional_forward_args=(attention_mask,),\n",
        "        target=target_label_idx,\n",
        "        return_convergence_delta=True\n",
        "    )\n",
        "\n",
        "    # Summarize attributions\n",
        "    def summarize_attributions(attributions):\n",
        "        attributions = attributions.sum(dim=-1).squeeze(0)\n",
        "        attributions = attributions / torch.norm(attributions)\n",
        "        return attributions\n",
        "\n",
        "    attributions_sum = summarize_attributions(attributions)\n",
        "\n",
        "    # Move delta to CPU and detach it\n",
        "    delta_cpu = delta.cpu().detach().numpy()\n",
        "    delta_all.append(delta_cpu)\n",
        "\n",
        "    def remove_pad_tokens(tokens, attributions, pad_token_id):\n",
        "        filtered_tokens = []\n",
        "        filtered_attributions = []\n",
        "        for token, attr in zip(tokens, attributions):\n",
        "            if token != tokenizer.pad_token:  # Check if the token is not a pad token\n",
        "                filtered_tokens.append(token)\n",
        "                filtered_attributions.append(attr)  # Convert tensor to value\n",
        "        return filtered_tokens, filtered_attributions\n",
        "\n",
        "    # Convert tokens and attributions to CPU-friendly format\n",
        "    indices = input_ids[0].detach().tolist()\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
        "\n",
        "    filtered_tokens, filtered_attributions = remove_pad_tokens(all_tokens, attributions_sum.tolist(), tokenizer.pad_token_id)\n",
        "\n",
        "    # Convert filtered_attributions to CPU-friendly format\n",
        "    filtered_attributions = torch.tensor(filtered_attributions).cpu().detach().numpy().tolist()\n",
        "    attributions_sum_all.append(filtered_attributions)\n",
        "\n",
        "    # Accumulate attribution scores for each token for the current author\n",
        "    for token, attribution in zip(filtered_tokens, filtered_attributions):\n",
        "        author_attributions[author][token].append(attribution)\n",
        "\n",
        "author_avg_attributions_sorted = {}\n",
        "\n",
        "for author, token_attributions in author_attributions.items():\n",
        "    author_avg_attributions = {}\n",
        "    for token, attributions in token_attributions.items():\n",
        "        # Calculate the average attribution score for each token\n",
        "        avg_attribution = np.mean(attributions)\n",
        "        author_avg_attributions[token] = avg_attribution\n",
        "\n",
        "    # Sort tokens by average attribution score in descending order\n",
        "    sorted_tokens = sorted(author_avg_attributions.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Store sorted tokens for the author\n",
        "    author_avg_attributions_sorted[author] = sorted_tokens\n",
        "\n",
        "# Example output: print the sorted average attributions for each author\n",
        "for author, sorted_tokens in author_avg_attributions_sorted.items():\n",
        "    print(f\"\\nAuthor: {author}\")\n",
        "    for token, avg_attribution in sorted_tokens:\n",
        "        print(f\"Token: {token}, Average Attribution: {avg_attribution}\")\n"
      ],
      "metadata": {
        "id": "gjpJ9jBPTReg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'delta': delta_all,\n",
        "    'attributions_sum': attributions_sum_all\n",
        "}\n",
        "\n",
        "# Convert the dictionary into a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Display the DataFrame (optional)\n",
        "print(df)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv('output.csv', index=False)"
      ],
      "metadata": {
        "id": "kM7fvTRiGa3K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dr3S-KCZRccW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ],
      "metadata": {
        "id": "68y_PQiZD4RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load spaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "def get_pos_tags(text):\n",
        "    # Apply spaCy POS tagging on the original text\n",
        "    doc = nlp(text)\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "    return pos_tags\n",
        "\n",
        "# Example text\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Get POS tags for the original sentence\n",
        "pos_tags = get_pos_tags(text)\n",
        "print(\"Original POS tags:\", pos_tags)\n",
        "\n",
        "\n",
        "def map_pos_to_bert_tokens(text, tokenizer):\n",
        "    # Get POS tags using spaCy\n",
        "    doc = nlp(text)\n",
        "    original_words = [token.text for token in doc]\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "    # Tokenize using BERT\n",
        "    bert_tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    mapped_tokens = []\n",
        "    pos_pointer = 0\n",
        "\n",
        "    # Map the POS tags to the BERT tokens\n",
        "    for token in bert_tokens:\n",
        "        if token.startswith(\"##\"):\n",
        "            # For subword tokens, we keep the same POS tag as the previous word\n",
        "            mapped_tokens.append((token, pos_tags[pos_pointer - 1]))\n",
        "        else:\n",
        "            # Assign the current word's POS tag\n",
        "            mapped_tokens.append((token, pos_tags[pos_pointer]))\n",
        "            pos_pointer += 1\n",
        "\n",
        "        # Ensure we don't exceed the original word list\n",
        "        if pos_pointer >= len(pos_tags):\n",
        "            break\n",
        "\n",
        "    return mapped_tokens\n",
        "\n",
        "# Example sentence\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "\n",
        "# Get BERT tokens and their corresponding POS tags\n",
        "bert_pos_mapping = map_pos_to_bert_tokens(text, tokenizer)\n",
        "print(\"BERT tokens with POS tags:\", bert_pos_mapping)\n",
        "\n",
        "\n",
        "# Assuming you have a list of BERT tokens and attributions\n",
        "def analyze_pos_attributions(text, attributions, tokenizer):\n",
        "    bert_pos_mapping = map_pos_to_bert_tokens(text, tokenizer)\n",
        "\n",
        "    for (token, pos), attribution in zip(bert_pos_mapping, attributions):\n",
        "        print(f\"Token: {token}, POS: {pos}, Attribution: {attribution}\")\n",
        "\n",
        "# Example: Assuming you have a text and the attributions from your model\n",
        "text = \"The quick brown fox jumps over the lazy dog.\"\n",
        "attributions = [0.1, 0.05, 0.03, 0.12, 0.5, 0.2, 0.1, 0.07, 0.08]\n",
        "analyze_pos_attributions(text, attributions, tokenizer)\n"
      ],
      "metadata": {
        "id": "PwwI4AmPHrF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load spaCy model for POS tagging\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Load BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "\n",
        "# Function to get POS tags from original text using spaCy\n",
        "def get_pos_tags(text):\n",
        "    doc = nlp(text)\n",
        "    pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "    return pos_tags\n",
        "\n",
        "# Function to map POS tags to BERT tokens\n",
        "def map_pos_to_bert_tokens(text, tokenizer):\n",
        "    doc = nlp(text)\n",
        "    original_words = [token.text for token in doc]\n",
        "    pos_tags = [token.pos_ for token in doc]\n",
        "\n",
        "    # Tokenize using BERT\n",
        "    bert_tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    mapped_tokens = []\n",
        "    pos_pointer = 0\n",
        "\n",
        "    # Map the POS tags to the BERT tokens\n",
        "    for token in bert_tokens:\n",
        "        if token.startswith(\"##\"):\n",
        "            # For subword tokens, we keep the same POS tag as the previous word\n",
        "            mapped_tokens.append((token, pos_tags[pos_pointer - 1]))\n",
        "        else:\n",
        "            # Assign the current word's POS tag\n",
        "            mapped_tokens.append((token, pos_tags[pos_pointer]))\n",
        "            pos_pointer += 1\n",
        "\n",
        "        # Ensure we don't exceed the original word list\n",
        "        if pos_pointer >= len(pos_tags):\n",
        "            break\n",
        "\n",
        "    return mapped_tokens\n",
        "\n",
        "# Initialize dictionary to store attributions by POS tag\n",
        "def calculate_average_attributions_by_pos(df):\n",
        "    pos_attributions = {}\n",
        "\n",
        "    # Loop over the rows of the dataframe\n",
        "    for idx, row in df.iterrows():\n",
        "        text = row['content']\n",
        "        attributions_sum = eval(row['attributions_sum'])  # Assuming this is a list of attribution values\n",
        "        print(len(attributions_sum))\n",
        "\n",
        "        # Get BERT tokens and POS tags\n",
        "        bert_pos_mapping = map_pos_to_bert_tokens(text, tokenizer)\n",
        "        print(len(bert_pos_mapping))\n",
        "\n",
        "        # Ensure we have the same length for BERT tokens and attributions\n",
        "        if len(bert_pos_mapping) != len(attributions_sum):\n",
        "            print(f\"Warning: Mismatch in token count and attributions at row {idx}\")\n",
        "            continue\n",
        "\n",
        "        # Accumulate attribution scores by POS tags\n",
        "        for (token, pos), attribution in zip(bert_pos_mapping, attributions_sum):\n",
        "            if pos not in pos_attributions:\n",
        "                pos_attributions[pos] = {'total_attr': 0, 'count': 0}\n",
        "\n",
        "            pos_attributions[pos]['total_attr'] += attribution\n",
        "            pos_attributions[pos]['count'] += 1\n",
        "\n",
        "    # Calculate average attribution score for each POS tag\n",
        "    average_pos_attributions = {pos: pos_data['total_attr'] / pos_data['count'] for pos, pos_data in pos_attributions.items()}\n",
        "\n",
        "    return average_pos_attributions\n",
        "\n",
        "# Now apply the function to calculate average attributions for each POS tag\n",
        "average_attributions_by_pos = calculate_average_attributions_by_pos(df)\n",
        "\n",
        "# Print the results\n",
        "print(\"Average attribution scores by POS tags:\")\n",
        "for pos, avg_attr in average_attributions_by_pos.items():\n",
        "    print(f\"{pos}: {avg_attr}\")\n"
      ],
      "metadata": {
        "id": "8AVm93jlGScu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from captum.attr import IntegratedGradients\n",
        "import pandas as pd\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load your trained content encoder and tokenizer\n",
        "text_encoder = BertModel.from_pretrained('your_text_encoder').to(device)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Load precomputed author embeddings (from training) and move to GPU\n",
        "author_embeddings = torch.load('author_embeddings.pth').to(device)\n",
        "\n",
        "# Switch model to evaluation mode\n",
        "text_encoder.eval()\n",
        "\n",
        "# Function to compute text embeddings using your encoder\n",
        "def get_text_embedding(inputs):\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}  # Move inputs to GPU\n",
        "    outputs = text_encoder(**inputs)\n",
        "    text_embedding = outputs.last_hidden_state.mean(dim=1)  # Averaging token embeddings\n",
        "    return text_embedding\n",
        "\n",
        "# Cosine similarity function\n",
        "def cosine_similarity(text_embedding, author_id):\n",
        "    author_embedding = author_embeddings[author_id].unsqueeze(0)\n",
        "    cosine_sim = F.cosine_similarity(text_embedding, author_embedding)\n",
        "    return cosine_sim\n",
        "\n",
        "# Forward function for computing cosine similarity\n",
        "def forward_func(inputs, author_id):\n",
        "    text_embedding = get_text_embedding(inputs)\n",
        "    cosine_sim = cosine_similarity(text_embedding, author_id)\n",
        "    return cosine_sim\n",
        "\n",
        "# Integrated Gradients setup\n",
        "ig = IntegratedGradients(lambda inputs: forward_func(inputs, author_id))\n",
        "\n",
        "# Example test set with ground truth authors\n",
        "test_set = [\n",
        "    {\"text\": \"Sample sentence 1.\", \"author_id\": 0},\n",
        "    {\"text\": \"Sample sentence 2.\", \"author_id\": 1},\n",
        "    # Add more samples from your test set\n",
        "]\n",
        "\n",
        "# Store the results in a list of dictionaries\n",
        "results = []\n",
        "\n",
        "for sample in test_set:\n",
        "    text = sample[\"text\"]\n",
        "    author_id = sample[\"author_id\"]\n",
        "\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "    # Move inputs to GPU\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Compute Integrated Gradients attributions\n",
        "    attributions = ig.attribute(inputs['input_ids'], target=0)  # Cosine similarity doesn't have specific classes\n",
        "\n",
        "    # Move attributions back to CPU for processing\n",
        "    attributions = attributions.cpu()\n",
        "\n",
        "    # Convert input IDs to tokens\n",
        "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0].cpu())\n",
        "\n",
        "    # Store the tokens and their corresponding attributions\n",
        "    attributions_list = attributions.detach().numpy().tolist()[0]\n",
        "\n",
        "    # Append the results\n",
        "    results.append({\n",
        "        \"text\": text,\n",
        "        \"author_id\": author_id,\n",
        "        \"tokens\": tokens,\n",
        "        \"attributions\": attributions_list\n",
        "    })\n",
        "\n",
        "# Convert the results to a DataFrame for better visualization\n",
        "df = pd.DataFrame(results)\n",
        "tools.display_dataframe_to_user(name=\"Test Set Integrated Gradients\", dataframe=df)\n",
        "\n",
        "# You can also save it to a CSV file\n",
        "df.to_csv('integrated_gradients_test_set_stage2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "dZ5Z8MXNBTkd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}