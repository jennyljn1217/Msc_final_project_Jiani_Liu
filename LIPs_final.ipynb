{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1gl0dwJqGoFn9s2uMrOe9nn03iJTJYAuz","authorship_tag":"ABX9TyMfom6WkN4IYyRw6jE+oq3D"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# https://github.com/baixianghuang/authorship-llm/tree/main"],"metadata":{"id":"g9VtZWmQpfxg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install openai"],"metadata":{"id":"fuEhC_aLFHw_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tiktoken"],"metadata":{"id":"KyWxvtmtFNDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6FDVNWetn0W-"},"outputs":[],"source":["import os\n","import csv\n","import json\n","import time\n","import torch\n","import openai\n","import pickle\n","import random\n","import tiktoken\n","# import py3langid\n","import numpy as np\n","import pandas as pd\n","import torch.nn.functional as F\n","\n","from random import shuffle\n","from sklearn import metrics\n","from ast import literal_eval\n","from openai import AzureOpenAI\n","from sklearn.metrics.pairwise import cosine_similarity\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from transformers import AutoModel, AutoTokenizer, AutoConfig, AutoModelForSequenceClassification"]},{"cell_type":"code","source":["N_EVAL=10"],"metadata":{"id":"gGabHooRBVDF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def num_tokens_from_string(string, encoding_name):\n","    encoding = tiktoken.encoding_for_model(encoding_name)\n","    num_tokens = len(encoding.encode(string))\n","    return num_tokens\n","\n","\n","def eval_fn(y_test, y_pred):\n","    acc = round(metrics.accuracy_score(y_test, y_pred)*100, 2)\n","    f1_w = round(metrics.f1_score(y_test, y_pred, average='weighted')*100, 2)\n","    f1_micro = round(metrics.f1_score(y_test, y_pred, average='micro')*100, 2)\n","    f1_macro = round(metrics.f1_score(y_test, y_pred, average='macro')*100, 2)\n","    # Macro Precision\n","    macro_precision = round(metrics.precision_score(y_test, y_pred, average='macro') * 100, 2)\n","    # Macro Recall\n","    macro_recall = round(metrics.recall_score(y_test, y_pred, average='macro') * 100, 2)\n","    print(f\"Macro Precision: {macro_precision}%\")\n","    print(f\"Macro Recall: {macro_recall}%\")\n","    return acc, f1_w, f1_micro, f1_macro\n","\n","\n","def embed_fn(model_name, texts, baseline_type):\n","    if baseline_type == 'bert':\n","        model = AutoModel.from_pretrained(model_name)\n","        tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        tokenized_texts = tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=512)\n","        embedding = model(tokenized_texts.input_ids.to(model.device), tokenized_texts.attention_mask.to(model.device)).last_hidden_state.mean(dim=1)\n","    elif baseline_type == 'tf-idf':\n","        vectorizer = TfidfVectorizer(max_features=3000, analyzer='char', ngram_range=(4, 4))\n","        embedding = torch.from_numpy(vectorizer.fit_transform(texts).toarray())\n","    elif baseline_type == 'ada':\n","        ada_client = AzureOpenAI(api_key = \"replace_this\", api_version = \"2023-05-15\", azure_endpoint = \"replace_this\")\n","        ada_response = ada_client.embeddings.create(input = texts, model = \"replace_this\")\n","        embedding = torch.Tensor([e.embedding for e in ada_response.data])\n","    return embedding\n","\n","\n","def run_aa_baseline(df_sub, model_name, baseline_type='bert'):\n","    ls_acc, ls_f1_w, ls_f1_micro, ls_f1_macro = [], [], [], []\n","\n","    for i in df_sub.index:\n","        ls_query_text, ls_potential_text = df_sub.loc[i, 'query_text'], df_sub.loc[i, 'potential_text']\n","        embed_query_texts = F.normalize(embed_fn(model_name, ls_query_text, baseline_type))\n","        embed_potential_texts = F.normalize(embed_fn(model_name, ls_potential_text, baseline_type))\n","\n","        preds = embed_query_texts @ embed_potential_texts.T\n","        preds = F.softmax(preds, dim=-1)\n","        labels = np.arange(0, len(ls_query_text))\n","\n","        acc, f1_w, f1_micro, f1_macro = eval_fn(labels, preds.argmax(-1).numpy())\n","        ls_acc.append(acc)\n","        ls_f1_w.append(f1_w)\n","        ls_f1_micro.append(f1_micro)\n","        ls_f1_macro.append(f1_macro)\n","\n","    muti_avg = (round(np.mean(ls_acc), 2), round(np.mean(ls_f1_w), 2), round(np.mean(ls_f1_micro), 2), round(np.mean(ls_f1_macro), 2))\n","    muti_std = (round(np.std(ls_acc), 2), round(np.std(ls_f1_w), 2), round(np.std(ls_f1_micro), 2), round(np.std(ls_f1_macro), 2))\n","    return muti_avg, muti_std"],"metadata":{"id":"RDz4YkR2BaXF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# data prep"],"metadata":{"id":"JAc573shDCmF"}},{"cell_type":"code","source":["df = pd.read_csv(\"blogtext.csv\")\n","df.drop(['gender', 'age', 'topic', 'sign', 'date'], axis=1, inplace=True)\n","df.shape\n","\n","\n","# Finding and removing duplicate rows\n","df[df[['text']].duplicated(keep=False)].sort_values('text')\n","\n","\n","print('Before removing duplicates, df.shape:', df.shape)\n","df = df.drop_duplicates(subset=['text'], keep='first').reset_index(drop=True)\n","print('New df.shape:', df.shape)\n","\n","\n","%%time\n","print(f\"{df.shape[0]:,}\")\n","df['lang'] = df['text'].apply(lambda x: py3langid.classify(x)[0])\n","print('% of English text:', f\"{df[df.lang=='en'].shape[0] / df.shape[0]}\")\n","\n","df = df[df.lang=='en']\n","df.drop('lang', axis=1, inplace=True)\n","print(f\"{df.shape[0]:,}\")\n","\n","\n","# check # of tokens\n","for i in range(10):\n","    text1, text2 = df.sample(2).text.values\n","    print(num_tokens_from_string(text1 + text2, \"gpt-3.5-turbo\"))\n","\n","\n","%%time\n","df = df[df[\"text\"].apply(lambda x: num_tokens_from_string(x, \"gpt-3.5-turbo\") < 512)]\n","print(f\"{df.shape[0]:,}\")\n","\n","\n","%%time\n","df = df[df[\"text\"].apply(lambda x: num_tokens_from_string(x, \"gpt-3.5-turbo\") > 56)]\n","print(f\"{df.shape[0]:,}\")\n","\n","\n","v = df.id.value_counts()\n","df = df[df.id.isin(v[v >= 2].index)]\n","print('# unique authors:', df.id.nunique())\n","print('New df.shape:', df.shape)\n","\n","\n","def sampler_aa_fn_pro(df, n, reps):\n","    \"\"\"\n","    Sample a new list of authors every time, use each of author as a query author so that the number of labels = n.\n","    Then, compute evaluaion metric for this set of authors and repeat this for multiple times (repetitions) to compute mean F1 etc.\n","    All the authors are unique as long as n is less than the number of unique authors.\n","    n: number of candidate authors.\n","    reps: number of repetitions.\n","    \"\"\"\n","    dict_to_df = []\n","    ls_unique_author = df.id.unique().tolist()\n","    for _ in range(reps):\n","        candidate_authors = random.sample(ls_unique_author, n)\n","        ls_unique_author = [e for e in ls_unique_author if e not in candidate_authors]\n","        ls_queries, ls_potential_texts = [], []\n","        dict_row = {}\n","\n","        for author_id in candidate_authors:\n","            # each text in these 2 lists are from unique authors, texts at same index are from the same author\n","            text, text_same_author = df.loc[author_id == df.id].text.sample(2)\n","            ls_queries.append(text)\n","            ls_potential_texts.append(text_same_author)\n","\n","        dict_row[\"query_text\"] = ls_queries\n","        dict_row[\"potential_text\"] = ls_potential_texts\n","        dict_to_df.append(dict_row)\n","\n","    df_sub = pd.DataFrame(dict_to_df)\n","    return df_sub\n","\n","\n"],"metadata":{"id":"HNnZkI8bBgnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def sampler_aa_fn_pro(df, n, reps):\n","    \"\"\"\n","    Sample a new list of authors every time, use each of author as a query author so that the number of labels = n.\n","    Then, compute evaluaion metric for this set of authors and repeat this for multiple times (repetitions) to compute mean F1 etc.\n","    All the authors are unique as long as n is less than the number of unique authors.\n","    n: number of candidate authors.\n","    reps: number of repetitions.\n","    \"\"\"\n","    dict_to_df = []\n","    ls_unique_author = df.id.unique().tolist()\n","    for _ in range(reps):\n","        candidate_authors = random.sample(ls_unique_author, n)\n","        ls_unique_author = [e for e in ls_unique_author if e not in candidate_authors]\n","        ls_queries, ls_potential_texts = [], []\n","        dict_row = {}\n","\n","        for author_id in candidate_authors:\n","            # each text in these 2 lists are from unique authors, texts at same index are from the same author\n","            text, text_same_author = df.loc[author_id == df.id].text.sample(2)\n","            ls_queries.append(text)\n","            ls_potential_texts.append(text_same_author)\n","\n","        dict_row[\"query_text\"] = ls_queries\n","        dict_row[\"potential_text\"] = ls_potential_texts\n","        dict_to_df.append(dict_row)\n","\n","    df_sub = pd.DataFrame(dict_to_df)\n","    return df_sub\n","\n","# nlp_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_test.csv')\n","nlp_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n","nlp_test = nlp_test[['prompt', 'user_name']]\n","nlp_test.columns = ['text', 'id']\n","# nlp_test.columns = ['id', 'text']\n","print(nlp_test)\n","df_sub = sampler_aa_fn_pro(nlp_test, 10, 3)"],"metadata":{"id":"Q4TXB89kucxn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_sub"],"metadata":{"id":"jovebzPzwsGa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# exp"],"metadata":{"id":"FmUZJEqfDW5f"}},{"cell_type":"code","source":["def run_aa(df, method, model_name, prompt_input, system_msg, ls_df, ls_model, ls_method, n_eval=N_EVAL):\n","    \"\"\"randomly select a subset of query texts\"\"\"\n","    start_time = time.time()\n","    df_res_all = pd.DataFrame()\n","    print(\"\\n++++++++++ \", method, model_name, n_eval, \" ++++++++++\")\n","\n","    for i in df.index:\n","        ls_reps = []\n","        text_label_map = {}\n","        sampled_queries = []  # select a subset for evaluation (e.g, n_eval out of 10)\n","        ls_query_text, ls_potential_text = df.loc[i, 'query_text'], df.loc[i, 'potential_text']\n","        random.seed(0)\n","        for idx, val in random.sample(list(enumerate(ls_query_text)), n_eval):\n","            text_label_map[val] = idx\n","            sampled_queries.append(val)\n","\n","        for query_text in sampled_queries:\n","            example_texts = json.dumps(dict(enumerate(ls_potential_text)))\n","            prompt = prompt_input+f\"\"\"The input texts are delimited with triple backticks. ```\\n\\nQuery text: {query_text} \\n\\nTexts from potential authors: {example_texts}\\n\\n```\"\"\"\n","            # List of potential author IDs: {list(dict(enumerate(ls_potential_text)).keys())}\n","\n","            raw_response = openai.chat.completions.create(\n","                model=model_name,\n","                response_format={\"type\": \"json_object\"},\n","                messages=[\n","                    {\"role\": \"system\", \"content\": system_msg},\n","                    {\"role\": \"user\", \"content\": prompt}\n","                ],\n","                temperature=0\n","            )\n","\n","            response_str = raw_response.choices[0].message.content\n","            print(prompt)\n","            print('\\nRaw response content:\\n', response_str, '\\nLabel:', text_label_map[query_text])\n","            try:\n","                response = json.loads(response_str, strict=False)\n","            except json.JSONDecodeError:\n","                print(f\"++++++++++ JSONDecodeError ++++++++++\")\n","                response = json.loads(\"{}\")\n","                response['analysis'] = response_str\n","                response['answer'] = -1\n","\n","            response[\"query_text\"], response[\"example_texts\"] = query_text, example_texts\n","            response[\"tokens\"] = raw_response.usage.total_tokens\n","            response[\"label\"] = text_label_map[query_text]\n","            ls_reps.append(response)\n","            response = None\n","\n","        df_reps = pd.DataFrame(ls_reps)\n","        df_reps['answer'] = pd.to_numeric(df_reps['answer'], errors='coerce')\n","        df_reps['answer'] = df_reps['answer'].fillna(-1)\n","        df_res_all = pd.concat([df_res_all, df_reps]).reset_index(drop=True)\n","\n","    ls_df.append(df_res_all)\n","    ls_method.append(method)\n","    ls_model.append(model_name)\n","    print(\"--- Execution Time: %s seconds ---\" % round(time.time() - start_time, 2))\n","    return df_res_all"],"metadata":{"id":"ldqHj9ulDXs4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dict_baseline = {'TF-IDF':'TF-IDF', 'BERT':'bert-base-uncased',\n","                 'RoBERTa':'roberta-base', 'ELECTRA':'google/electra-base-discriminator',\n","                 'DeBERTa':'microsoft/deberta-base'}\n","dict_embed_type = {'TF-IDF':'tf-idf', 'BERT':'bert', 'RoBERTa':'bert',\n","                   'ELECTRA':'bert', 'DeBERTa':'bert'}\n","\n","def compare_baseline_mod(df_sub, ls_df, ls_model, ls_method, n_eval=N_EVAL, std_flag=False, baseline_idx=len(dict_baseline)):\n","    ls_res_avg, ls_res_std = [], []\n","\n","    # for key, val in list(dict_baseline.items())[:baseline_idx]:\n","    #     muti_avg, muti_std = run_aa_baseline(df_sub, val, dict_embed_type[key])\n","    #     ls_res_avg.append((key, val)+muti_avg+(0,))\n","    #     ls_res_std.append((key, val)+muti_std+(0,))\n","\n","    for i, df_tmp in enumerate(ls_df):\n","        muti_avg, muti_std = eval_all_fn(df_tmp, n_eval)\n","        answer_tmp = df_tmp.copy()\n","\n","        ls_res_avg.append((ls_method[i], ls_model[i])+muti_avg+(abs(answer_tmp[answer_tmp.answer==-1]['answer'].astype('int').sum()),))\n","        ls_res_std.append((ls_method[i], ls_model[i])+muti_std+(None,))\n","\n","    res_avg = pd.DataFrame(ls_res_avg, columns=ls_col)\n","    res_std = pd.DataFrame(ls_res_std, columns=ls_col)\n","    if std_flag:\n","        return res_avg, res_std\n","    else:\n","        return res_avg\n","\n","\n","def eval_all_fn(df_res_all, n_eval):\n","    \"\"\"evaluate the entire df of multiple repetitions, take avg of each rep.\n","    The null or -1 answers are counted as false\n","    Make sure n_eval is same in run_aa()\"\"\"\n","    ls_acc, ls_f1_w, ls_f1_micro, ls_f1_macro = [], [], [], []\n","    for i in range(0, len(df_res_all.index), n_eval):\n","        df_reps = df_res_all[i: i+n_eval]\n","        acc, f1_w, f1_micro, f1_macro = eval_fn(df_reps[\"label\"], df_reps[\"answer\"])\n","        ls_acc.append(acc)\n","        ls_f1_w.append(f1_w)\n","        ls_f1_micro.append(f1_micro)\n","        ls_f1_macro.append(f1_macro)\n","\n","    muti_avg = (round(np.mean(ls_acc), 2), round(np.mean(ls_f1_w), 2), round(np.mean(ls_f1_micro), 2), round(np.mean(ls_f1_macro), 2))\n","    muti_std = (round(np.std(ls_acc), 2), round(np.std(ls_f1_w), 2), round(np.std(ls_f1_micro), 2), round(np.std(ls_f1_macro), 2))\n","    return muti_avg, muti_std"],"metadata":{"id":"8HWvx2W3DbVd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# api_version = \"2023-12-01-preview\"  # \"2023-05-15\"\n","ls_col = ['Prompt', 'Model', 'Accuracy', 'Weighted F1', 'Micro F1', 'Macro F1', 'Unsure']\n","\n","openai.api_key = ''"],"metadata":{"id":"4W7UFZ-5DdJh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# n=10"],"metadata":{"id":"p2gNZ2KsDga-"}},{"cell_type":"code","source":["# m1, m2 = \"gpt-35-turbo\", \"gpt-4-turbo\"\n","# m1, m2 = 'gpt-3.5-turbo', 'gpt-4-turbo'\n","m1 = 'gpt-4-turbo'\n","v1, v2, v3, v4 = 'no_guidance', 'little_guidance', 'grammar', 'LIP'\n","\n","prompt1 = \"Given a set of texts with known authors and a query text, determine the author of the query text. \"\n","prompt2 = prompt1+\"Do not consider topic differences. \"\n","prompt3 = prompt1+\"Focus on grammatical styles. \"\n","prompt4 = prompt1+\"Analyze the writing styles of the input texts, disregarding the differences in topic and content. Focus on linguistic features such as phrasal verbs, modal verbs, punctuation, rare words, affixes, quantities, humor, sarcasm, typographical errors, and misspellings. \"\n","system_msg = \"\"\"Respond with a JSON object including two key elements:\n","{\n","  \"analysis\": Reasoning behind your answer.\n","  \"answer\": The query text's author ID.\n","}\"\"\""],"metadata":{"id":"sZ4emuFYDfG5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%time\n","\n","# imdb62\n","ls_df_10, ls_model_10, ls_method_10 = [], [], []\n","\n","# df1_gpt35 = run_aa(df_10, v1, m1, prompt1, system_msg, ls_df_10, ls_model_10, ls_method_10)\n","# df2_gpt35 = run_aa(df_10, v2, m1, prompt2, system_msg, ls_df_10, ls_model_10, ls_method_10)\n","# df3_gpt35 = run_aa(df_10, v3, m1, prompt3, system_msg, ls_df_10, ls_model_10, ls_method_10)\n","# df4_gpt35 = run_aa(df_10, v4, m1, prompt4, system_msg, ls_df_10, ls_model_10, ls_method_10)\n","\n","# df1_gpt4 = run_aa(df_10, v1, m2, prompt1, system_msg, ls_df_10, ls_model_10, ls_method_10)\n","# df2_gpt4 = run_aa(df_10, v2, m2, prompt2, system_msg, ls_df_10, ls_model_10, ls_method_10)\n","# df3_gpt4 = run_aa(df_10, v3, m2, prompt3, system_msg, ls_df_10, ls_model_10, ls_method_10)\n","df4_gpt4 = run_aa(df_sub, v4, m1, prompt4, system_msg, ls_df_10, ls_model_10, ls_method_10)\n","\n","compare_baseline_mod(df_sub, ls_df_10, ls_model_10, ls_method_10)"],"metadata":{"id":"9lhuSOVWAbs6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Wn0skfZWQgFI"},"execution_count":null,"outputs":[]}]}