{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "VVF9XklCuf87",
        "8up1q8JcGzUO",
        "omYAuF01NxXL",
        "eDWEJIRWMKsd",
        "V07TWqonT3H9",
        "u80ucNwtEohn",
        "i8zjk0v9zFTx"
      ],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "SNScQufB5Slx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lda"
      ],
      "metadata": {
        "id": "TgXGLFRLuKhE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0NU6wjaSC4G"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy import sum\n",
        "from numpy.linalg import norm\n",
        "import re\n",
        "import os\n",
        "import string\n",
        "from string import punctuation\n",
        "from collections import Counter, defaultdict\n",
        "from urllib.request import urlretrieve\n",
        "# from tqdm import tqdm\n",
        "from tqdm.notebook import tqdm\n",
        "from joblib import Parallel, delayed\n",
        "import pickle\n",
        "# import benepar\n",
        "# from concurrent.futures import ProcessPoolExecutor\n",
        "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import entropy\n",
        "import argparse\n",
        "import warnings\n",
        "import sys\n",
        "import csv\n",
        "import codecs\n",
        "\n",
        "import lda\n",
        "import lda.datasets\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import preprocessing\n",
        "\n",
        "# import stanza\n",
        "# stanza.download('en')\n",
        "# nlp = stanza.Pipeline('en')\n",
        "\n",
        "# import spacy\n",
        "# nlp = spacy.load('en_core_web_sm')\n",
        "# benepar.download(\"benepar_en3\")\n",
        "# nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "\n",
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tag import map_tag\n",
        "from nltk.util import ngrams\n",
        "from nltk.tree import Tree\n",
        "from nltk.stem import PorterStemmer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "np.random.seed(1337)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(len(stop_words))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess datasets"
      ],
      "metadata": {
        "id": "VVF9XklCuf87"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DiffusionDB"
      ],
      "metadata": {
        "id": "Tgp7UAs1uu1E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Download the parquet table\n",
        "table_url = f'https://huggingface.co/datasets/poloclub/diffusiondb/resolve/main/metadata.parquet'\n",
        "urlretrieve(table_url, 'metadata.parquet')\n",
        "\n",
        "parquet_file = 'metadata.parquet'\n",
        "df = pd.read_parquet(parquet_file, engine='pyarrow')\n",
        "\n",
        "print('Original Initial Size :', len(df))\n",
        "\n",
        "# 2. remove duplicates\n",
        "# columns_to_remove = ['image_name', 'part_id', 'seed', 'step', 'cfg', 'sampler',\n",
        "#        'width', 'height', 'timestamp', 'image_nsfw', 'prompt_nsfw']\n",
        "# df_initial = df_initial.drop(columns=columns_to_remove, errors='ignore')\n",
        "df = df.drop_duplicates(subset=['user_name', 'prompt'])\n",
        "\n",
        "print('Original Size (without duplicates) :', len(df))\n",
        "\n",
        "# 3. remove rows that contain at least one NaN or Null character\n",
        "df.replace(['null', 'NULL', 'NaN', 'nan', '', ' '], np.nan, inplace=True)\n",
        "df = df.dropna(how='any')\n",
        "print('Origianl Size (without duplicates and NaN/Null character)', len(df))\n",
        "\n",
        "# 4. remove rows that contain non-English characters\n",
        "def contains_non_english(text):\n",
        "    if not isinstance(text, str):\n",
        "        print(text)\n",
        "        return True\n",
        "    return bool(re.search(r'[^\\x00-\\x7F]', text))\n",
        "\n",
        "df = df[~df['prompt'].apply(contains_non_english)]\n",
        "\n",
        "print('Original Size (without duplicates, NaN/Null and non-English characters)', len(df))\n",
        "\n",
        "# 5. remove rows that contain only blanks\n",
        "def contains_nothing(text):\n",
        "  text = text.strip()\n",
        "  if len(text) == 0:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "df = df[~df['prompt'].apply(contains_nothing)]\n",
        "print('Original Size (without duplicates, NaN/Null and non-English characters, blank lines)', len(df))\n",
        "\n",
        "# 6. remove authors who delete the accounts\n",
        "df = df[df['user_name'] != 'deleted_acount']\n",
        "print('Original Size (without duplicates, NaN/Null and non-English characters, blank lines, deleted account)', len(df))\n",
        "\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "# Save the cleaned dataset\n",
        "df.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb.csv', index=False)\n",
        "df"
      ],
      "metadata": {
        "id": "_O-B_ckFW3x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_diffusiondb = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb.csv')\n",
        "# for idx, row in df_diffusiondb.iterrows():\n",
        "#   text = row['prompt']\n",
        "#   if type(text) != str:\n",
        "#     print(idx)\n",
        "#     print(text)\n",
        "#     print()"
      ],
      "metadata": {
        "id": "1iYfjhayGW3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df['prompt'][1407722])\n",
        "print(df_diffusiondb['prompt'][1407722])\n",
        "text = 'a     portrait of a female robot made from code, very intricate details, octane render, 8 k, trending on artstation '\n",
        "print(nltk.word_tokenize(text))\n",
        "print(len(df))\n",
        "print(len(df_diffusiondb))"
      ],
      "metadata": {
        "id": "owvVNI8e-6J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate dataset statistics"
      ],
      "metadata": {
        "id": "z8JF4Igvx_he"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):  # tokenize the text\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  return tokens\n",
        "\n",
        "def tokenize_sentences(text):\n",
        "  sentences = nltk.sent_tokenize(text)\n",
        "  return sentences\n",
        "\n",
        "\n",
        "# --- Lexical Features --- #\n",
        "\n",
        "# Lexical feature (word level)\n",
        "def total_word_lengths(tokens):\n",
        "  # if type(tokens) != str:\n",
        "  #   print(tokens)\n",
        "  if type(tokens) == str:\n",
        "    tokens = eval(tokens)\n",
        "  return sum([len(word) for word in tokens])\n",
        "\n",
        "def num_short_words(tokens):\n",
        "  if type(tokens) == str:\n",
        "    tokens = eval(tokens)\n",
        "  return sum(1 for word in tokens if len(word) <= 3)\n",
        "\n",
        "def stopwords_frequency(tokens):\n",
        "  if type(tokens) == str:\n",
        "    tokens = eval(tokens)\n",
        "  stopword_counts = {word: tokens.count(word) for word in stop_words}\n",
        "  return stopword_counts\n",
        "\n",
        "def total_words(tokens):\n",
        "  if type(tokens) == str:\n",
        "    tokens = eval(tokens)\n",
        "  return len(tokens)\n",
        "\n",
        "\n",
        "# Lexical feature (character level)\n",
        "def total_digits(text):\n",
        "  return sum(c.isdigit() for c in text)\n",
        "\n",
        "def total_uppercase(text):\n",
        "  return sum(c.isupper() for c in text)\n",
        "\n",
        "def letter_frequency(text):\n",
        "  # print(string.ascii_letters[:26])\n",
        "  text = text.lower()\n",
        "  return {char: text.count(char) for char in string.ascii_letters[: 26]}\n",
        "\n",
        "def digit_frequency(text):\n",
        "  # print(string.digits)\n",
        "  return {digit: text.count(digit) for digit in string.digits}\n",
        "\n",
        "def total_characters(text):\n",
        "  return len(text)\n",
        "\n",
        "\n",
        "# Lexical feature (sentence level)\n",
        "def total_sentences(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  return len(sentences)\n",
        "\n",
        "\n",
        "# Lexical feature (vocabulary richness)\n",
        "# def hapax_legomena(tokens):\n",
        "#   tokens = eval(tokens)\n",
        "#   word_counts = Counter(tokens)\n",
        "#   return sum(1 for count in word_counts.values() if count == 1)\n",
        "\n",
        "# def dis_legomena(tokens):\n",
        "#   tokens = eval(tokens)\n",
        "#   word_counts = Counter(tokens)\n",
        "#   return sum(1 for count in word_counts.values() if count == 2)\n",
        "\n",
        "\n",
        "# --- Ngram Features --- #\n",
        "\n",
        "def calculate_ngrams(corpus, n, mode):\n",
        "  if mode == 'char':\n",
        "    corpus = corpus.replace(\" \", \"\")\n",
        "  elif mode == 'word':\n",
        "    if type(corpus) == str:\n",
        "      corpus = eval(corpus)\n",
        "    corpus = [token for token in corpus if token not in string.punctuation]\n",
        "  elif mode == 'pos':\n",
        "    if type(corpus) == str:\n",
        "      corpus = eval(corpus)\n",
        "    tag_fd = nltk.FreqDist(map_tag('en-ptb', 'universal', tag) for (word, tag) in corpus)\n",
        "    corpus = list(tag_fd.keys())\n",
        "\n",
        "  return Counter(ngrams(corpus, n))\n",
        "\n",
        "\n",
        "\n",
        "# --- Syntactic Features --- #\n",
        "\n",
        "def punctuation_frequency(text):\n",
        "  punct_freq = Counter(char for char in text if char in string.punctuation)\n",
        "  return dict(punct_freq)\n",
        "\n",
        "def pos_frequency(pos_tags):\n",
        "  if type(pos_tags) == str:\n",
        "    pos_tags = eval(pos_tags)\n",
        "  # print(pos_tags)\n",
        "  pos_tag = ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRT', 'PRON', 'VERB', '.', 'X']\n",
        "  tag_fd = nltk.FreqDist(map_tag('en-ptb', 'universal', tag) for (word, tag) in pos_tags)\n",
        "  pos_tags = list(tag_fd.keys())\n",
        "  pos_freq = Counter(pos_tags)\n",
        "  return dict(pos_freq)\n",
        "\n",
        "# def extract_phrase_structures(subtree):\n",
        "#   phrase_structures = []\n",
        "#   if isinstance(subtree, Tree):\n",
        "#     phrase_structures.append(subtree.label())\n",
        "#     for child in subtree:\n",
        "#       phrase_structures.extend(extract_phrase_structures(child))\n",
        "#   return phrase_structures\n",
        "\n",
        "# def phrase_structures(text):\n",
        "#   doc = nlp(text)\n",
        "#   sentence = doc.sentences[0]\n",
        "#   tree = sentence.constituency\n",
        "#   nltk_tree = Tree.fromstring(str(tree))\n",
        "#   phrase_structures = extract_phrase_structures(nltk_tree)\n",
        "#   return phrase_structures\n",
        "\n",
        "# def dependency_paths(text):\n",
        "#   doc = nlp(text)\n",
        "#   sentence = doc.sentences[0]\n",
        "\n",
        "#   dependency_paths = []\n",
        "#   for dep_edge in sentence.dependencies:\n",
        "#     head = dep_edge[0].text\n",
        "#     dependent = dep_edge[2].text\n",
        "#     dep_type = dep_edge[1]\n",
        "#     dependency_paths.append((head, dep_type, dependent))\n",
        "#   return dependency_paths\n",
        "\n",
        "# def phrase_structures(doc):\n",
        "#   phrase_structures = []\n",
        "#   for chunk in doc.noun_chunks:\n",
        "#     phrase_structures.append(chunk.text)\n",
        "#   return phrase_structures\n",
        "\n",
        "# def dependency_paths(doc):\n",
        "#   paths = []\n",
        "#   for token in doc:\n",
        "#     paths.append((token.dep_, token.head.text, token.text))\n",
        "#   return paths\n",
        "\n",
        "# def to_nltk_tree(node):\n",
        "#     if isinstance(node, spacy.tokens.Token):\n",
        "#         return Tree(node.tag_, [node.text])\n",
        "#     else:\n",
        "#         return Tree(node.label_, [to_nltk_tree(child) for child in node.children])\n",
        "\n",
        "# # Function to extract phrases from the constituency tree\n",
        "# def extract_phrases(tree):\n",
        "#     phrases = []\n",
        "#     if isinstance(tree, Tree):\n",
        "#         if tree.height() > 2:  # Ignore pre-terminals\n",
        "#             phrases.append(tree.label())\n",
        "#         for child in tree:\n",
        "#             phrases.extend(extract_phrases(child))\n",
        "#     return phrases\n",
        "\n",
        "def get_dependency_path(token):\n",
        "    path = []\n",
        "    while token.head != token:\n",
        "        path.append((token.dep_, token.head.text))\n",
        "        token = token.head\n",
        "    path.append(('ROOT', token.text))\n",
        "    path.reverse()\n",
        "    return path\n",
        "\n",
        "def dependency_ngrams(doc, n=3):\n",
        "    tokens = [token.text for token in doc if token.text not in string.punctuation]\n",
        "    tokens_ids = [token.i for token in doc if token.text not in string.punctuation]\n",
        "    word_ngrams = list(ngrams(tokens, n))\n",
        "    dep_ngrams = []\n",
        "\n",
        "    count = 0\n",
        "    for word_ngram in word_ngrams:\n",
        "        indices = [token.i for token in doc if token.text in word_ngram]\n",
        "        if len(indices) > n:\n",
        "          indices_new = []\n",
        "          for idx in indices:\n",
        "            if idx >= tokens_ids[count]:\n",
        "              if count + n > len(tokens_ids) - 1:\n",
        "                indices_new.append(idx)\n",
        "              else:\n",
        "                if idx < tokens_ids[count + n]:\n",
        "                  indices_new.append(idx)\n",
        "        else:\n",
        "          indices_new = indices\n",
        "        if len(indices_new) != n:\n",
        "\n",
        "          print(doc)\n",
        "          print(word_ngram)\n",
        "          print(tokens_ids)\n",
        "          print(indices)\n",
        "          print(indices_new)\n",
        "          print(paths)\n",
        "          print(dep_ngram)\n",
        "          print()\n",
        "        paths = [get_dependency_path(doc[idx]) for idx in indices_new]\n",
        "        dep_ngram = []\n",
        "        for path in paths:\n",
        "          dep_ngram.extend(tuple(p[0] for p in path))\n",
        "        dep_ngrams.append(tuple(dep_ngram))\n",
        "        count += 1\n",
        "\n",
        "\n",
        "    return dep_ngrams\n",
        "\n",
        "# def process_text(text, n=2):\n",
        "#     doc = nlp(text)\n",
        "#     print(doc)\n",
        "#     root = [sent.root for sent in doc.sents][0]\n",
        "#     print(root)\n",
        "#     tree = to_nltk_tree(root)\n",
        "#     print(tree)\n",
        "#     return extract_phrase_ngrams(tree, n)\n",
        "def process_text(text):\n",
        "  return nlp(text)\n",
        "\n",
        "def process_dependency(doc, n):\n",
        "  dep_ngrams = dependency_ngrams(doc, n)\n",
        "  return Counter(dep_ngrams)\n",
        "\n",
        "# def parallel_apply(series, func, n_jobs=4):\n",
        "#     results = Parallel(n_jobs=n_jobs)(delayed(func)(row) for row in tqdm(series))\n",
        "#     return results\n",
        "\n",
        "# --- N-gram based --- #\n",
        "\n",
        "def get_pos_tags(tokens):\n",
        "    if type(tokens) == str:\n",
        "      tokens = eval(tokens)\n",
        "    return nltk.pos_tag(tokens)\n",
        "\n",
        "\n",
        "\n",
        "# --- Hardness --- #\n",
        "\n",
        "# Text pre-processing as required to compare the content of different authors\n",
        "def fil_sent(sent):\n",
        "    \"\"\"\n",
        "    Filter stopwords\n",
        "    \"\"\"\n",
        "    filtered_sentence = ' '.join([w for w in sent.split() if not w in stop_words])\n",
        "    filtered_sentence = ''.join([w for w in filtered_sentence if w not in list(punctuation)])\n",
        "    filtered_sentence = filtered_sentence.strip()\n",
        "    filtered_sentence = filtered_sentence.split()\n",
        "    return filtered_sentence\n",
        "\n",
        "def process(sent):\n",
        "    \"\"\"\n",
        "    Apply stemming\n",
        "    \"\"\"\n",
        "    sent = str(sent)\n",
        "    return fil_sent(' '.join([ps.stem(str(x).lower()) for x in word_tokenize(sent)]))\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "ps = PorterStemmer()\n",
        "\n",
        "# Similarity between two authors' content\n",
        "def jaccard_similarity(list1, list2):\n",
        "\n",
        "    list1 = list1[:512]\n",
        "    list2 = list2[:512]\n",
        "\n",
        "    intersection = len(list(set(list1).intersection(list2)))\n",
        "    union = (len(list1) + len(list2)) - intersection\n",
        "\n",
        "    if union > 0:\n",
        "      return float(intersection) / union\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "# Combines all the text per author in a list of lists\n",
        "def all_text_all_category(df, col_id, col_text):\n",
        "  all_text = []\n",
        "\n",
        "  for val in list(set(df[col_id].values)):\n",
        "    sub_df = df[df[col_id] == val]\n",
        "    all_text.append(list(itertools.chain(sub_df[col_text].apply(lambda x: process(x)).values))[0])\n",
        "\n",
        "  return all_text\n",
        "\n",
        "# Computes the relative hardness\n",
        "def rel_hardness(df, col_id, col_text):\n",
        "\n",
        "  n_labels = len(list(set(df[col_text].values)))\n",
        "  return 1/(n_labels * (n_labels - 1) * 0.5) * sum([jaccard_similarity(x[0], x[1]) for x in itertools.combinations(all_text_all_category(df, col_id, col_text), 2) if x[0] != x[1]])"
      ],
      "metadata": {
        "id": "ABon-YR37cMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_frequency(dictionary):\n",
        "  new_dictionary = {}\n",
        "  count = sum(list(dictionary.values()))\n",
        "  for key, value in dictionary.items():\n",
        "    new_dictionary[key] = value / count if count != 0 else 0\n",
        "  return new_dictionary\n",
        "\n",
        "\n",
        "def extract_features_lexical(df):\n",
        "\n",
        "  df['stopwords_num'] = df['tokenized_prompt'].progress_apply(stopwords_frequency)\n",
        "  df['total_word_lengths'] = df['tokenized_prompt'].progress_apply(total_word_lengths)\n",
        "  df['num_short_words'] = df['tokenized_prompt'].progress_apply(num_short_words)\n",
        "  df['total_words'] = df['tokenized_prompt'].progress_apply(total_words)\n",
        "\n",
        "  df['letter_num'] = df['prompt'].progress_apply(letter_frequency)\n",
        "  df['digit_num'] = df['prompt'].progress_apply(digit_frequency)\n",
        "  df['total_digits'] = df['prompt'].progress_apply(total_digits)\n",
        "  df['total_uppercase'] = df['prompt'].progress_apply(total_uppercase)\n",
        "  df['total_characters'] = df['prompt'].progress_apply(total_characters)\n",
        "\n",
        "  df['total_sentences'] = df['prompt'].progress_apply(total_sentences)\n",
        "\n",
        "  df['average_word_length'] = df['total_word_lengths'] / df['total_words']\n",
        "  df['short_words_ratio'] = df['num_short_words'] / df['total_words']\n",
        "  df['stopwords_frequency'] = df['stopwords_num'].apply(calculate_frequency)\n",
        "  df['percentage_of_digits'] = df['total_digits'] / df['total_characters']\n",
        "  df['percentage_of_uppercase'] = df['total_uppercase'] / df['total_characters']\n",
        "  df['letter_frequency'] = df['letter_num'].apply(calculate_frequency)\n",
        "  df['digit_frequency'] = df['digit_num'].apply(calculate_frequency)\n",
        "  df['average_sentence_length'] = df['total_sentences']\n",
        "  df['average_document_length'] = df['total_words']\n",
        "\n",
        "  return df\n",
        "\n",
        "\n",
        "# def extract_features_syntactic(df):\n",
        "#   df['punctuation_frequency'] = df['prompt'].progress_apply(punctuation_frequency)\n",
        "#   df['pos_frequency'] = df['pos_tags'].progress_apply(pos_frequency)\n",
        "#   # df['spacy_doc'] = df['prompt'].progress_apply(process_text)\n",
        "#   # df['dependency_unigrams'] = df['spacy_doc'].progress_apply(lambda x: process_dependency(x, 1))\n",
        "#   # df['dependency_bigrams'] = df['spacy_doc'].progress_apply(lambda x: process_dependency(x, 2))\n",
        "#   # df['dependency_trigrams'] = df['spacy_doc'].progress_apply(lambda x: process_dependency(x, 3))\n",
        "#   # df['dependency_fourgrams'] = df['spacy_doc'].progress_apply(lambda x: process_dependency(x, 4))\n",
        "#   # tqdm.pandas()\n",
        "#   # df['spacy_doc'] = df['prompt'].progress_apply(process_text)\n",
        "#   # df['punctuation_frequency'] = df['prompt'].progress_apply(punctuation_frequency)\n",
        "#   # df['pos_frequency'] = df['pos_tags'].progress_apply(pos_frequency)\n",
        "#   # df['phrase_structures'] = df['spacy_doc'].progress_apply(phrase_structures)\n",
        "#   # df['dependency_paths'] = df['spacy_doc'].progress_apply(dependency_paths)\n",
        "\n",
        "#   return df\n",
        "\n",
        "def extract_features_syntactic(df):\n",
        "  df['pos_num'] = df['pos_tags'].progress_apply(pos_frequency)\n",
        "  df['punctuation_num'] = df['prompt'].progress_apply(punctuation_frequency)\n",
        "\n",
        "  df['pos_frequency'] = df['pos_num'].progress_apply(calculate_frequency)\n",
        "  df['punctuation_frequency'] = df['punctuation_num'].progress_apply(calculate_frequency)\n",
        "\n",
        "  return df\n",
        "\n",
        "def extract_features_ngrams(df):\n",
        "  df['char_bigrams'] = df['prompt'].progress_apply(calculate_ngrams, args=(2, 'char'))\n",
        "  df['char_trigrams'] = df['prompt'].progress_apply(calculate_ngrams, args=(3, 'char'))\n",
        "\n",
        "  df['word_unigrams'] = df['tokenized_prompt'].progress_apply(calculate_ngrams, args=(1, 'word'))\n",
        "  df['word_bigrams'] = df['tokenized_prompt'].progress_apply(calculate_ngrams, args=(2, 'word'))\n",
        "  df['word_trigrams'] = df['tokenized_prompt'].progress_apply(calculate_ngrams, args=(3, 'word'))\n",
        "\n",
        "  df['pos_bigrams'] = df['pos_tags'].progress_apply(calculate_ngrams, args=(2, 'pos'))\n",
        "  df['pos_trigrams'] = df['pos_tags'].progress_apply(calculate_ngrams, args=(3, 'pos'))\n",
        "\n",
        "  return df\n",
        "\n",
        "def syntactic_richness(df):\n",
        "    words_all = []\n",
        "    words_distinct = []\n",
        "    for idx, row in df.iterrows():\n",
        "      words = row['tokenized_prompt']\n",
        "      if type(words) == str:\n",
        "        words = eval(words)\n",
        "      words_all.extend(words)\n",
        "      # for word in words:\n",
        "      #   if word not in words_distinct:\n",
        "      #     words_distinct.append(word)\n",
        "    return len(set(words_all)) / len(words_all)\n",
        "\n",
        "def hapax_legomena(df):\n",
        "  words_all = []\n",
        "  for idx, row in df.iterrows():\n",
        "    words = row['tokenized_prompt']\n",
        "    if type(words) == str:\n",
        "      words = eval(words)\n",
        "    words_all.extend(words)\n",
        "  print(len(words_all))\n",
        "  word_counts = Counter(words_all)\n",
        "  return sum(1 for count in word_counts.values() if count == 1) / len(words_all)\n",
        "\n",
        "def dis_legomena(df):\n",
        "  words_all = []\n",
        "  for idx, row in df.iterrows():\n",
        "    words = row['tokenized_prompt']\n",
        "    if type(words) == str:\n",
        "      words = eval(words)\n",
        "    words_all.extend(words)\n",
        "  print(len(words_all))\n",
        "  word_counts = Counter(words_all)\n",
        "  return sum(1 for count in word_counts.values() if count == 2) / len(words_all)\n",
        "\n",
        "def herdan_ttr(df):\n",
        "  words_all = []\n",
        "  for idx, row in df.iterrows():\n",
        "    words = row['tokenized_prompt']\n",
        "    if type(words) == str:\n",
        "      words = eval(words)\n",
        "    words_all.extend(words)\n",
        "  return np.log(len(set(words_all))) / np.log(len(words_all))\n",
        "\n",
        "def combine_dictionary_results(df, col_name):\n",
        "  results = {}\n",
        "  count = 0\n",
        "  for idx, row in df.iterrows():\n",
        "    dic = row[col_name]\n",
        "    if type(dic) == str:\n",
        "      dic = eval(dic)\n",
        "    if isinstance(dic, Counter):\n",
        "      dic = dict(dic)\n",
        "    for key, value in dic.items():\n",
        "      if key in results.keys():\n",
        "        results[key] += value\n",
        "      else:\n",
        "        results[key] = value\n",
        "      count += value\n",
        "  for key, value in results.items():\n",
        "    if count == 0:\n",
        "      results[key] = 0\n",
        "    else:\n",
        "      results[key] = value / count\n",
        "  return results\n",
        "\n",
        "def calculate_statistics_lexical(df):\n",
        "\n",
        "  # aggregated_stopwords_frequency = Counter()\n",
        "  # aggregated_letter_frequency = Counter()\n",
        "  # aggregated_digit_frequency = Counter()\n",
        "\n",
        "  # for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
        "  #   aggregated_stopwords_frequency.update(Counter(stopwords_frequency(row['tokenized_prompt'])))\n",
        "  #   aggregated_letter_frequency.update(Counter(letter_frequency(row['prompt'])))\n",
        "  #   aggregated_digit_frequency.update(Counter(digit_frequency(row['prompt'])))\n",
        "\n",
        "  metrics = {\n",
        "    # Lexical feature (word-level)\n",
        "    'average_word_length': df['total_word_lengths'].sum() / df['total_words'].sum(),\n",
        "    'short_words_ratio': df['num_short_words'].sum() / df['total_words'].sum(),\n",
        "    'stopwords_frequency': combine_dictionary_results(df, 'stopwords_num'),\n",
        "    # 'stopwords_frequency': dict(aggregated_stopwords_frequency),\n",
        "    'syntactic_richness': syntactic_richness(df),\n",
        "    'number_of_words': df['total_words'].sum(),\n",
        "\n",
        "    # Lexical feature (character-level)\n",
        "    'percentage_of_digits': df['total_digits'].sum() / df['total_characters'].sum(),\n",
        "    'percentage_of_uppercase': df['total_uppercase'].sum() / df['total_characters'].sum(),\n",
        "    'letter_frequency': combine_dictionary_results(df, 'letter_num'),\n",
        "    'digit_frequency': combine_dictionary_results(df, 'digit_num'),\n",
        "    # 'letter_frequency': dict(aggregated_letter_frequency),\n",
        "    # 'digit_frequency': dict(aggregated_digit_frequency),\n",
        "\n",
        "    # Lexical feature (sentence-level)\n",
        "    'average_sentence_length': df['total_words'].sum() / df['total_sentences'].sum(),\n",
        "    'average_document_length': df['total_words'].sum() / len(df),\n",
        "\n",
        "    # Lexical feature (dataset-level)\n",
        "    'number_of_authors': df['user_name'].nunique(),\n",
        "    'author_document_counts': df['user_name'].value_counts().to_dict(),\n",
        "    'std_dev_document_counts': df['user_name'].value_counts().std(),\n",
        "    'number_of_documents': len(df),\n",
        "    'avg_documents_per_author': len(df) / df['user_name'].nunique(),\n",
        "\n",
        "    # Lexical feature (vocabulary richness)\n",
        "    'hapax_legomena_ratio': hapax_legomena(df),\n",
        "    'dis_legomena_ratio': dis_legomena(df),\n",
        "    'herdan_ttr': herdan_ttr(df)\n",
        "  }\n",
        "\n",
        "  return metrics\n",
        "\n",
        "def get_most_common_ngrams(counter, n=100):\n",
        "    counter = Counter(counter)\n",
        "    return counter.most_common(n)\n",
        "\n",
        "# def calculate_statistics_ngrams(df):\n",
        "\n",
        "#   aggregate_word_unigrams = Counter()\n",
        "#   aggregate_word_bigrams = Counter()\n",
        "#   aggregate_word_trigrams = Counter()\n",
        "\n",
        "#   aggregate_char_bigrams = Counter()\n",
        "#   aggregate_char_trigrams = Counter()\n",
        "\n",
        "#   aggregate_pos_bigrams = Counter()\n",
        "#   aggregate_pos_trigrams = Counter()\n",
        "\n",
        "#   # Process each row\n",
        "#   for index, row in tqdm(df.iterrows(), total=len(df)):\n",
        "#     # if index % 10000 == 0:\n",
        "#     #   print(index)\n",
        "#     # Character n-grams\n",
        "#     char_text = row['prompt'].replace(\" \", \"\")  # Remove spaces for character n-grams\n",
        "#     aggregate_char_bigrams.update(calculate_ngrams(char_text, 2))\n",
        "#     aggregate_char_trigrams.update(calculate_ngrams(char_text, 3))\n",
        "\n",
        "#     # Word n-grams\n",
        "#     words = eval(row['tokenized_prompt'])\n",
        "#     # print(words)\n",
        "#     words = [token for token in words if token not in string.punctuation]\n",
        "#     aggregate_word_unigrams.update(calculate_ngrams(words, 1))\n",
        "#     aggregate_word_bigrams.update(calculate_ngrams(words, 2))\n",
        "#     aggregate_word_trigrams.update(calculate_ngrams(words, 3))\n",
        "\n",
        "#     # POS n-grams\n",
        "#     pos_tags = eval(row['pos_tags'])\n",
        "#     # print(pos_tags)\n",
        "#     pos_tag = ['ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRT', 'PRON', 'VERB', '.', 'X']\n",
        "#     tag_fd = nltk.FreqDist(map_tag('en-ptb', 'universal', tag) for (word, tag) in pos_tags)\n",
        "#     pos_tags = list(tag_fd.keys())\n",
        "#     # print(pos_tags)\n",
        "#     aggregate_pos_bigrams.update(calculate_ngrams(pos_tags, 2))\n",
        "#     aggregate_pos_trigrams.update(calculate_ngrams(pos_tags, 3))\n",
        "\n",
        "\n",
        "#   metrics = {\n",
        "#     'char_bigrams': get_most_common_ngrams(aggregate_char_bigrams),\n",
        "#     'char_trigrams': get_most_common_ngrams(aggregate_char_trigrams),\n",
        "#     'word_unigrams': get_most_common_ngrams(aggregate_word_unigrams),\n",
        "#     'word_bigrams': get_most_common_ngrams(aggregate_word_bigrams),\n",
        "#     'word_trigrams': get_most_common_ngrams(aggregate_word_trigrams),\n",
        "#     'pos_bigrams': get_most_common_ngrams(aggregate_pos_bigrams),\n",
        "#     'pos_trigrams': get_most_common_ngrams(aggregate_pos_trigrams)\n",
        "#   }\n",
        "\n",
        "#   return metrics\n",
        "\n",
        "def common_ngrams(ngram_counter, common_ngrams):\n",
        "    res = {}\n",
        "    for ngram in common_ngrams:\n",
        "      ngram = tuple(ngram[0])\n",
        "      if ngram in ngram_counter:\n",
        "        res[ngram] = ngram_counter[ngram]\n",
        "    return res\n",
        "\n",
        "def common_ngrams_frequency(ngram_counter, common_ngrams):\n",
        "    total = sum(list(ngram_counter.values()))\n",
        "    res = {}\n",
        "    for ngram in common_ngrams:\n",
        "      ngram = tuple(ngram[0])\n",
        "      if ngram in ngram_counter:\n",
        "        res[ngram] = ngram_counter[ngram] / total\n",
        "    return res\n",
        "\n",
        "\n",
        "def calculate_statistics_ngrams(df):\n",
        "\n",
        "  char_bigrams_common = get_most_common_ngrams(combine_dictionary_results(df, 'char_bigrams'))\n",
        "  char_trigrams_common = get_most_common_ngrams(combine_dictionary_results(df, 'char_trigrams'))\n",
        "  word_unigrams_common = get_most_common_ngrams(combine_dictionary_results(df, 'word_unigrams'))\n",
        "  word_bigrams_common = get_most_common_ngrams(combine_dictionary_results(df, 'word_bigrams'))\n",
        "  word_trigrams_common = get_most_common_ngrams(combine_dictionary_results(df, 'word_trigrams'))\n",
        "  pos_bigrams_common = get_most_common_ngrams(combine_dictionary_results(df, 'pos_bigrams'))\n",
        "  pos_trigrams_common = get_most_common_ngrams(combine_dictionary_results(df, 'pos_trigrams'))\n",
        "\n",
        "  # Create the metrics dictionary using the stored variables\n",
        "  metrics = {\n",
        "      'char_bigrams': char_bigrams_common,\n",
        "      'char_trigrams': char_trigrams_common,\n",
        "      'word_unigrams': word_unigrams_common,\n",
        "      'word_bigrams': word_bigrams_common,\n",
        "      'word_trigrams': word_trigrams_common,\n",
        "      'pos_bigrams': pos_bigrams_common,\n",
        "      'pos_trigrams': pos_trigrams_common\n",
        "  }\n",
        "\n",
        "  df['char_bigrams_common'] = df['char_bigrams'].apply(lambda x: common_ngrams(x, char_bigrams_common))\n",
        "  df['char_trigrams_common'] = df['char_trigrams'].apply(lambda x: common_ngrams(x, char_trigrams_common))\n",
        "  df['word_unigrams_common'] = df['word_unigrams'].apply(lambda x: common_ngrams(x, word_unigrams_common))\n",
        "  df['word_bigrams_common'] = df['word_bigrams'].apply(lambda x: common_ngrams(x, word_bigrams_common))\n",
        "  df['word_trigrams_common'] = df['word_trigrams'].apply(lambda x: common_ngrams(x, word_trigrams_common))\n",
        "  df['pos_bigrams_common'] = df['pos_bigrams'].apply(lambda x: common_ngrams(x, pos_bigrams_common))\n",
        "  df['pos_trigrams_common'] = df['pos_trigrams'].apply(lambda x: common_ngrams(x, pos_trigrams_common))\n",
        "\n",
        "  df['char_bigrams_common_frequency'] = df['char_bigrams'].apply(lambda x: common_ngrams_frequency(x, char_bigrams_common))\n",
        "  df['char_trigrams_common_frequency'] = df['char_trigrams'].apply(lambda x: common_ngrams_frequency(x, char_trigrams_common))\n",
        "  df['word_unigrams_common_frequency'] = df['word_unigrams'].apply(lambda x: common_ngrams_frequency(x, word_unigrams_common))\n",
        "  df['word_bigrams_common_frequency'] = df['word_bigrams'].apply(lambda x: common_ngrams_frequency(x, word_bigrams_common))\n",
        "  df['word_trigrams_common_frequency'] = df['word_trigrams'].apply(lambda x: common_ngrams_frequency(x, word_trigrams_common))\n",
        "  df['pos_bigrams_common_frequency'] = df['pos_bigrams'].apply(lambda x: common_ngrams_frequency(x, pos_bigrams_common))\n",
        "  df['pos_trigrams_common_frequency'] = df['pos_trigrams'].apply(lambda x: common_ngrams_frequency(x, pos_trigrams_common))\n",
        "\n",
        "  return metrics\n",
        "\n",
        "\n",
        "def calculate_statistics_syntactic(df):\n",
        "  metrics = {\n",
        "    'punctuation_frequency': combine_dictionary_results(df, 'punctuation_num'),\n",
        "    'pos_frequency': combine_dictionary_results(df, 'pos_num')\n",
        "  }\n",
        "\n",
        "  return metrics\n",
        "\n",
        "\n",
        "# def calculate_statistics_syntactic(df):\n",
        "#   def get_most_common_ngrams(counter, n=100):\n",
        "#     return counter.most_common(n)\n",
        "\n",
        "#   def calculate_ngrams(corpus, n):\n",
        "#     return Counter(ngrams(corpus, n))\n",
        "\n",
        "#   def combine_dictionary_results(df, col_name):\n",
        "#     results = {}\n",
        "#     count = 0\n",
        "#     for idx, row in df.iterrows():\n",
        "#       # print(row[col_name])\n",
        "#       # print(type(row[col_name]))\n",
        "#       if type(row[col_name]) == str:\n",
        "#         dic = eval(row[col_name])\n",
        "#       else:\n",
        "#         dic = row[col_name]\n",
        "#       for key, value in dic.items():\n",
        "#         if key in results.keys():\n",
        "#           results[key] += value\n",
        "#         else:\n",
        "#           results[key] = value\n",
        "#         count += value\n",
        "#     for key, value in results.items():\n",
        "#       if count == 0:\n",
        "#         results[key] = 0\n",
        "#       else:\n",
        "#         results[key] = value / count\n",
        "#     return results\n",
        "\n",
        "#   # aggregate_phrase_unigrams = Counter()\n",
        "#   # aggregate_phrase_bigrams = Counter()\n",
        "#   # aggregate_phrase_trigrams = Counter()\n",
        "#   # aggregate_phrase_fourgrams = Counter()\n",
        "\n",
        "#   # aggregate_dependency_unigrams = Counter()\n",
        "#   # aggregate_dependency_bigrams = Counter()\n",
        "#   # aggregate_dependency_trigrams = Counter()\n",
        "#   # aggregate_dependency_fourgrams = Counter()\n",
        "\n",
        "#   # Process each row\n",
        "#   # for index, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
        "\n",
        "#   #   # # Phrase structure n-grams\n",
        "#   #   # if type(row['phrase_structures']) == str:\n",
        "#   #   #   phrase_structures = eval(row['phrase_structures'])\n",
        "#   #   # else:\n",
        "#   #   #   phrase_structures = row['phrase_structures']\n",
        "#   #   # aggregate_phrase_unigrams.update(calculate_ngrams(phrase_structures, 1))\n",
        "#   #   # aggregate_phrase_bigrams.update(calculate_ngrams(phrase_structures, 2))\n",
        "#   #   # aggregate_phrase_trigrams.update(calculate_ngrams(phrase_structures, 3))\n",
        "#   #   # aggregate_phrase_fourgrams.update(calculate_ngrams(phrase_structures, 4))\n",
        "\n",
        "#   #   # Denpendency structure n-grams\n",
        "#   #   text = row['prompt']\n",
        "#   #   doc = nlp(text)\n",
        "#   #   # print(dependency_ngrams(doc, 1))\n",
        "#   #   # print(Counter(dependency_ngrams(doc, 1)))\n",
        "#   #   # print(dependency_ngrams(doc, 2))\n",
        "#   #   # print(Counter(dependency_ngrams(doc, 2)))\n",
        "#   #   aggregate_dependency_unigrams.update(Counter(dependency_ngrams(doc, 1)))\n",
        "#   #   aggregate_dependency_bigrams.update(Counter(dependency_ngrams(doc, 2)))\n",
        "#   #   aggregate_dependency_trigrams.update(Counter(dependency_ngrams(doc, 3)))\n",
        "#   #   aggregate_dependency_fourgrams.update(Counter(dependency_ngrams(doc, 4)))\n",
        "\n",
        "\n",
        "#   metrics = {\n",
        "#     # 'phrase_unigrams': get_most_common_ngrams(aggregate_phrase_unigrams),\n",
        "#     # 'phrase_bigrams': get_most_common_ngrams(aggregate_phrase_bigrams),\n",
        "#     # 'phrase_trigrams': get_most_common_ngrams(aggregate_phrase_trigrams),\n",
        "#     # 'phrase_fourgrams': get_most_common_ngrams(aggregate_phrase_fourgrams),\n",
        "#     'punctuation_frequency': combine_dictionary_results(df, 'punctuation_frequency'),\n",
        "#     'pos_frequency': combine_dictionary_results(df, 'pos_frequency'),\n",
        "#     # 'dependency_unigrams': get_most_common_ngrams(aggregate_dependency_unigrams),\n",
        "#     # 'dependency_bigrams': get_most_common_ngrams(aggregate_dependency_bigrams),\n",
        "#     # 'dependency_trigrams': get_most_common_ngrams(aggregate_dependency_trigrams),\n",
        "#     # 'dependency_fourgrams': get_most_common_ngrams(aggregate_dependency_fourgrams)\n",
        "#   }\n",
        "\n",
        "#   return metrics"
      ],
      "metadata": {
        "id": "_eirfSdw4dnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topic analysis\n",
        "\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r\"\\d+\", \" \", string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
        "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    return string.strip()\n",
        "\n",
        "\n",
        "def sample_prompts(group, n=100):\n",
        "    return group.sample(n=n, random_state=42) if len(group) > n else group\n",
        "\n",
        "\n",
        "def load_diffusiondb(data_path):\n",
        "    # load data (can be used if data already split into train and test set)\n",
        "    df = pd.read_csv(data_path)\n",
        "    df = df[['user_name', 'prompt']]\n",
        "    unique_user_names_count = df['user_name'].nunique()\n",
        "    print('Unique Users:', unique_user_names_count)\n",
        "    df_filtered = df.groupby('user_name').filter(lambda x: len(x) >= 100)\n",
        "    unique_user_names_count = df['user_name'].nunique()\n",
        "    print(f\"Number of users with at least 100 prompts: {unique_user_names_count}\")\n",
        "\n",
        "    if unique_user_names_count >= 100:\n",
        "      random_users = df_filtered['user_name'].drop_duplicates().sample(n=100, random_state=42).tolist()\n",
        "      sub_df = df_filtered[df_filtered['user_name'].isin(random_users)]\n",
        "      data = sub_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    else:\n",
        "      # random_users = df_filtered['user_name'].drop_duplicates().sample(n=50, random_state=42).tolist()\n",
        "      # sub_df = df_filtered[df_filtered['user_name'].isin(random_users)]\n",
        "      # data = sub_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "      data = df_filtered\n",
        "    print(f'totally {len(data)} data')\n",
        "\n",
        "    x = data['prompt']\n",
        "    y = data['user_name']\n",
        "    dict_author = {}    # id doc: author_name\n",
        "    X = []\n",
        "    for i in range(len(x)):\n",
        "        X.append(clean_str(x[i]))\n",
        "        dict_author[i] = y[i]\n",
        "    auth_class = list(set(y))\n",
        "    le = preprocessing.LabelEncoder()\n",
        "    le.fit(auth_class)\n",
        "    y_numeric = le.transform(y)\n",
        "    return X, dict_author\n",
        "\n",
        "\n",
        "def JSD(P, Q):\n",
        "    _P = P / norm(P, ord=1)\n",
        "    _Q = Q / norm(Q, ord=1)\n",
        "    _M = 0.5 * (_P + _Q)\n",
        "    return 0.5 * (entropy(_P, _M) + entropy(_Q, _M))\n",
        "\n",
        "\n",
        "def topics_analysis(args):\n",
        "    warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "    data = args.data\n",
        "    number_of_topics_list = eval(args.n_topics)\n",
        "    d_path = args.data_path\n",
        "    X = []\n",
        "    dict_author = {}\n",
        "    if data == \"ccat10\" or data == 'ccat50':\n",
        "        X, dict_author = load_ccat(d_path)\n",
        "    elif data == \"judgment\":\n",
        "        X, dict_author = load_judgment(d_path)\n",
        "    elif data == \"imdb\":\n",
        "        X, dict_author = load_imdb62(d_path)\n",
        "    elif data == 'diffusiondb' or data == 'twitter_micro' or data == 'blogs50' or data == 'imdb62':\n",
        "        X, dict_author = load_diffusiondb(d_path)\n",
        "\n",
        "    # print(X)\n",
        "    # create vocabulary\n",
        "    print (\"creating vocabulary..\")\n",
        "    print (\"---------------------------\")\n",
        "\n",
        "    tf_vectorizer = CountVectorizer(max_df=0.95, min_df=5, stop_words='english')\n",
        "    X_tf = tf_vectorizer.fit_transform(X)\n",
        "    vocab = tf_vectorizer.get_feature_names_out()\n",
        "    print(\"shape: {}\\n\".format(X_tf.shape))\n",
        "    print(vocab)\n",
        "\n",
        "    for number_of_topics in number_of_topics_list:\n",
        "\n",
        "      title = str(data) + \" (N_topic = \" + str(number_of_topics) + \")\"\n",
        "\n",
        "      # building topic model using LDA\n",
        "      print (\"building model..\")\n",
        "      print (\"---------------------------\")\n",
        "      model = lda.LDA(n_topics=number_of_topics, n_iter=1000, random_state=1000)\n",
        "      model.fit(X_tf)\n",
        "      topic_word = model.topic_word_\n",
        "      print(\"shape: {}\".format(topic_word.shape))\n",
        "\n",
        "      # show detail of topic\n",
        "      n = 10\n",
        "      for i, topic_dist in enumerate(topic_word):\n",
        "          topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n+1):-1]\n",
        "          print('*Topic {}\\n- {}'.format(i, ' '.join(topic_words)))\n",
        "\n",
        "      print (\"document topic model..\")\n",
        "      print (\"---------------------------\")\n",
        "      doc_topic = model.doc_topic_\n",
        "      topic_most = {}\n",
        "      for n in range(len(doc_topic)):\n",
        "          topic_most_pr = doc_topic[n].argmax()\n",
        "          author = dict_author[n]\n",
        "          if author in topic_most:\n",
        "              tp_most.append(topic_most_pr)\n",
        "          else:\n",
        "              tp_most = []\n",
        "              tp_most.append(topic_most_pr)\n",
        "          topic_most[author] = tp_most\n",
        "\n",
        "      i = 0\n",
        "      for author_p, topic_p in topic_most.items():\n",
        "          print (i, author_p, Counter(topic_p))\n",
        "          i += 1\n",
        "\n",
        "      new_dict = defaultdict(list)\n",
        "      for k, v in dict_author.items():\n",
        "          new_dict[v].append(k)\n",
        "\n",
        "      new_dict_2 = defaultdict(list)\n",
        "      for k, v in new_dict.items():\n",
        "          sum_per_author = np.zeros(number_of_topics)\n",
        "          n_doc = len(v)\n",
        "          for i in range(len(v)):\n",
        "              sum_per_author = sum([sum_per_author, doc_topic[v[i]]], axis=0)\n",
        "          mean_prob = sum_per_author/n_doc\n",
        "          new_dict_2[k].append(mean_prob)\n",
        "\n",
        "      P1 = []\n",
        "      j = 0\n",
        "      for auth, m in new_dict_2.items():\n",
        "          P1.append(m)\n",
        "          j +=1\n",
        "\n",
        "      # calculating JS divergence between authors\n",
        "      print (\"calculating JS divergence..\")\n",
        "      print (\"---------------------------\")\n",
        "      P2 = P1\n",
        "      KL = []\n",
        "      for p in np.array(P1):\n",
        "          kl = []\n",
        "          for q in np.array(P2):\n",
        "              ent = JSD(p.ravel(), q.ravel())\n",
        "              kl.append(ent)\n",
        "          KL.append(kl)\n",
        "      print (\"Average JS Divergence\", np.mean(KL))\n",
        "\n",
        "      # create confusion matrix\n",
        "      print (\"creating heatmap..\")\n",
        "      print (\"---------------------------\")\n",
        "\n",
        "      plt.figure(figsize=(10, 8))\n",
        "      ax = sns.heatmap(np.array(KL))\n",
        "      ax.set_ylabel('author')\n",
        "      ax.set_xlabel('author')\n",
        "      ax.set_title(title)\n",
        "      ax.collections[0].colorbar.set_label(\"JS Divergence\")\n",
        "      plt.savefig(f'/content/drive/MyDrive/msc_project/data/{data}/clean/heatmap_{data}_{number_of_topics}.png')\n",
        "      plt.close()"
      ],
      "metadata": {
        "id": "6SPBKdqPkip5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DiffusionDB"
      ],
      "metadata": {
        "id": "8up1q8JcGzUO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_diffusiondb = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb.csv')\n",
        "df_diffusiondb['tokenized_prompt'] = df_diffusiondb['prompt'].apply(tokenize)\n",
        "df_diffusiondb.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv', index=False)"
      ],
      "metadata": {
        "id": "PJFbBG0gUK1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical\n",
        "# df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "\n",
        "# df_diffusiondb_features = extract_features_lexical(df_diffusiondb_tokenized)\n",
        "# df_diffusiondb_features.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_lexical.csv', index=False)\n",
        "\n",
        "df_diffusiondb_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_lexical.csv')\n",
        "\n",
        "diffusiondb_statistics = calculate_statistics_lexical(df_diffusiondb_features)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in diffusiondb_statistics.items()}\n",
        "df_diffusiondb_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'DiffusionDB'])\n",
        "df_diffusiondb_statistics.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_statistics_lexical.csv', index=False)\n",
        "df_diffusiondb_statistics"
      ],
      "metadata": {
        "id": "wNCfDI63Pc8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntactic\n",
        "tqdm.pandas()\n",
        "df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "df_diffusiondb_features = extract_features_syntactic(df_diffusiondb_tokenized)\n",
        "# print(df_diffusiondb_features)\n",
        "df_diffusiondb_features.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_syntactic.csv', index=False)\n",
        "\n",
        "df_diffusiondb_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_syntactic.csv')\n",
        "\n",
        "diffusiondb_statistics = calculate_statistics_syntactic(df_diffusiondb_features)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in diffusiondb_statistics.items()}\n",
        "df_diffusiondb_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'DiffusionDB'])\n",
        "df_diffusiondb_statistics.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_statistics_syntactic.csv', index=False)\n",
        "df_diffusiondb_statistics"
      ],
      "metadata": {
        "id": "LKweO7Q84U7U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram based\n",
        "# df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "# df_diffusiondb_tokenized['pos_tags'] = df_diffusiondb_tokenized['tokenized_prompt'].apply(get_pos_tags)\n",
        "# df_diffusiondb_tokenized.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv', index=False)\n",
        "\n",
        "df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "\n",
        "# df_diffusiondb_features = extract_features_ngrams(df_diffusiondb_tokenized)\n",
        "# df_diffusiondb_features.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_ngrams.csv', index=False)\n",
        "\n",
        "# df_diffusiondb_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_ngrams.csv')\n",
        "\n",
        "diffusiondb_statistics = calculate_statistics_ngrams(df_diffusiondb_tokenized)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "diffusiondb_statistics = {k: {tuple(ngram): count for ngram, count in v} for k, v in diffusiondb_statistics.items()}\n",
        "df_diffusiondb_statistics = pd.DataFrame(list(diffusiondb_statistics.items()), columns=['Metric', 'DiffusionDB'])\n",
        "df_diffusiondb_statistics.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_statistics_ngrams.csv', index=False)\n",
        "df_diffusiondb_statistics"
      ],
      "metadata": {
        "id": "aAdPOYB3M4LG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hardness\n",
        "def sample_prompts(group, n=100):\n",
        "    return group.sample(n=n, random_state=42) if len(group) > n else group\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb.csv')\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print('Unique Users:', unique_user_names_count)\n",
        "df_filtered = df.groupby('user_name').filter(lambda x: len(x) >= 100)\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print(f\"Number of users with at least 100 prompts: {unique_user_names_count}\")\n",
        "\n",
        "rh_blog = []\n",
        "for lim in [5, 10, 25, 50, 75, 100, 150, 200]:\n",
        "  print(lim)\n",
        "  list_spk = list(pd.DataFrame(df_filtered['user_name'].value_counts()[:lim]).index)\n",
        "  sub_df = df_filtered[df_filtered['user_name'].isin(list_spk)]\n",
        "  sampled_df = sub_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "  rh_blog.append(rel_hardness(sampled_df, 'user_name', 'prompt'))\n",
        "\n",
        "rh_blog"
      ],
      "metadata": {
        "id": "Splk_jgSbhbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topics\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data', type=str, default='ccat10', help='data')\n",
        "parser.add_argument('--data_path', type=str, default='/home/yunita/Data/Dataset/Stamatatos/c10_traintest.csv', help='data path')\n",
        "parser.add_argument('--n_topics', help='number of topics')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args = parser.parse_args(args=[\n",
        "    '--data', 'diffusiondb',\n",
        "    '--data_path', '/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb.csv',\n",
        "    '--n_topics', '[3, 10, 20, 30, 40, 50]'\n",
        "])\n",
        "\n",
        "topics_analysis(args)"
      ],
      "metadata": {
        "id": "FJJTpr9dvlEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3QpPkHDNj81h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test"
      ],
      "metadata": {
        "id": "omYAuF01NxXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_diffusiondb_features)"
      ],
      "metadata": {
        "id": "Wwnsjg9ijn3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diffusiondb_statistics = calculate_statistics_syntactic(df_diffusiondb_features.head(50))"
      ],
      "metadata": {
        "id": "epo8YDDPVjcC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_diffusiondb_features = extract_features_syntactic(df_diffusiondb_tokenized.head(5))\n",
        "diffusiondb_statistics = calculate_statistics_syntactic(df_diffusiondb_features)\n",
        "print(diffusiondb_statistics)"
      ],
      "metadata": {
        "id": "SSScszla919p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "\n",
        "# df_diffusiondb_tokenized = df_diffusiondb_tokenized.head(5)\n",
        "# df_diffusiondb_tokenized['spacy_doc'] = df_diffusiondb_tokenized['prompt'].progress_apply(process_text)\n",
        "\n",
        "# with open('/content/drive/MyDrive/msc_project/data/spacy_docs.pkl', 'wb') as f:\n",
        "#     pickle.dump(df_diffusiondb_tokenized['spacy_doc'].tolist(), f)\n",
        "\n",
        "# with open('/content/drive/MyDrive/msc_project/data/spacy_docs.pkl', 'rb') as f:\n",
        "#     spacy_docs = pickle.load(f)\n",
        "\n",
        "# df_diffusiondb_tokenized['spacy_doc'] = spacy_docs\n",
        "# print(df_diffusiondb_tokenized)\n",
        "# # print(df_diffusiondb_features)\n",
        "\n",
        "# df_diffusiondb_features = extract_features_syntactic(df_diffusiondb_tokenized)\n",
        "# df_diffusiondb_features"
      ],
      "metadata": {
        "id": "-o10umfQ0Xwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The four main performers are riveting.\"\n",
        "# df_diffusiondb_tokenized = pd.DataFrame(data)\n",
        "# df_diffusiondb_tokenized['spacy_doc'] = df_diffusiondb_tokenized['prompt'].progress_apply(process_text)\n",
        "doc = process_text(text)\n",
        "# df_diffusiondb_features = extract_features_syntactic(df_diffusiondb_tokenized)\n",
        "# print(df_diffusiondb_features)\n",
        "# diffusiondb_statistics = calculate_statistics_syntactic(df_diffusiondb_features)\n",
        "# print(diffusiondb_statistics)"
      ],
      "metadata": {
        "id": "EkfgB7kFF83W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import benepar\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "benepar.download(\"benepar_en3\")\n",
        "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "# Sample sentence\n",
        "sentence = \"The four main performers are riveting.\"\n",
        "\n",
        "# Parse the sentence\n",
        "doc = nlp(sentence)\n",
        "sent = list(doc.sents)[0]\n",
        "\n",
        "# Print the phrase structure tree\n",
        "print(sent._.parse_string)\n",
        "\n",
        "# Display the tree using nltk.Tree\n",
        "from nltk import Tree\n",
        "\n",
        "def print_tree(tree):\n",
        "    if not isinstance(tree, Tree):\n",
        "        return\n",
        "    tree.pretty_print()\n",
        "\n",
        "nltk_tree = Tree.fromstring(sent._.parse_string)\n",
        "print_tree(nltk_tree)\n",
        "\n",
        "\n",
        "def extract_ngrams(tree, n):\n",
        "    ngrams = []\n",
        "\n",
        "    def traverse(t):\n",
        "        if isinstance(t, Tree):\n",
        "            for i in range(len(t) - n + 1):\n",
        "                ngram = t[i:i+n]\n",
        "                if all(isinstance(child, Tree) for child in ngram):\n",
        "                    ngrams.append(ngram)\n",
        "            for child in t:\n",
        "                traverse(child)\n",
        "\n",
        "    traverse(tree)\n",
        "    return ngrams\n",
        "\n",
        "def print_ngrams(ngrams):\n",
        "    for ngram in ngrams:\n",
        "        print(' '.join([' '.join(child.leaves()) for child in ngram]))\n",
        "        print(ngram)\n",
        "        print()\n",
        "\n",
        "# Extract n-grams of phrase structures\n",
        "n = 2  # Change this to any n you want\n",
        "ngrams = extract_ngrams(nltk_tree, 2)\n",
        "\n",
        "# Print the extracted n-grams\n",
        "print_ngrams(ngrams)"
      ],
      "metadata": {
        "id": "FTdVxkdAj6fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diffusiondb_statistics = {k: {tuple(ngram): count for ngram, count in v} for k, v in diffusiondb_statistics.items()}\n",
        "df_diffusiondb_statistics = pd.DataFrame(list(diffusiondb_statistics.items()), columns=['feature', 'ngrams'])\n",
        "df_diffusiondb_statistics.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_statistics_syntactic.csv', index=False)\n",
        "df_diffusiondb_statistics"
      ],
      "metadata": {
        "id": "ncsMM-7GuemV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u0W7-PJJ4Bey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/authorship_inference_attack/real_diffusionDB_large_all_features 1.csv')\n",
        "df_diffusiondb1 = pd.read_csv('1.csv')\n",
        "df_diffusiondb1['prompt'] = df_diffusiondb1['prompt'].astype(str)\n",
        "# df_diffusiondb1['tokenized_prompt'] = df_diffusiondb1['prompt'].apply(tokenize)\n",
        "# df_diffusiondb1.to_csv('1.csv', index=False)\n",
        "\n",
        "# df_diffusiondb.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv', index=False)\n",
        "\n",
        "# df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "\n",
        "df_diffusiondb_features1 = extract_features(df_diffusiondb1)\n",
        "# df_diffusiondb_features.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_lexical.csv', index=False)\n",
        "\n",
        "diffusiondb_statistics1 = calculate_statistics(df_diffusiondb_features1)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "df_diffusiondb_statistics1 = pd.DataFrame.from_dict(diffusiondb_statistics1, orient='index', columns=['DiffusionDB'])\n",
        "df_diffusiondb_statistics1"
      ],
      "metadata": {
        "id": "4JJhEoVSEH4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_diffusiondb2 = pd.read_csv('/content/drive/MyDrive/authorship_inference_attack/real_diffusionDB_large_all_features 1.csv')\n",
        "\n",
        "# df_diffusiondb2['prompt'] = df_diffusiondb2['prompt'].astype(str)\n",
        "# df_diffusiondb2['tokenized_prompt'] = df_diffusiondb2['prompt'].apply(lambda x: x.split())\n",
        "# df_diffusiondb2.to_csv('2.csv', index=False)\n",
        "df_diffusiondb2 = pd.read_csv('2.csv')\n",
        "df_diffusiondb2['prompt'] = df_diffusiondb2['prompt'].astype(str)\n",
        "# df_diffusiondb.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv', index=False)\n",
        "\n",
        "# df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "\n",
        "df_diffusiondb_features2 = extract_features(df_diffusiondb2)\n",
        "# df_diffusiondb_features.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_lexical.csv', index=False)\n",
        "\n",
        "diffusiondb_statistics2 = calculate_statistics(df_diffusiondb_features2)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "df_diffusiondb_statistics2 = pd.DataFrame.from_dict(diffusiondb_statistics2, orient='index', columns=['DiffusionDB'])\n",
        "df_diffusiondb_statistics2"
      ],
      "metadata": {
        "id": "TaHqtML07dhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.ascii_letters[:26])"
      ],
      "metadata": {
        "id": "jOjMcnHLCAc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "73d9cAnRCOWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Twitter"
      ],
      "metadata": {
        "id": "eDWEJIRWMKsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_twitter = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/author_texts_cleaned.csv')\n",
        "df_twitter.columns = ['user_name', 'prompt']\n",
        "df_twitter['tokenized_prompt'] = df_twitter['prompt'].apply(tokenize)\n",
        "df_twitter['pos_tags'] = df_twitter['tokenized_prompt'].apply(get_pos_tags)\n",
        "df_twitter.to_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized.csv', index=False)\n",
        "df_twitter"
      ],
      "metadata": {
        "id": "Ckmw22EpMMZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical\n",
        "tqdm.pandas()\n",
        "# df_twitter_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized.csv')\n",
        "\n",
        "# df_twitter_features = extract_features_lexical(df_twitter_tokenized)\n",
        "# df_twitter_features.to_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized_lexical.csv', index=False)\n",
        "\n",
        "df_twitter_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized_lexical.csv')\n",
        "\n",
        "twitter_statistics = calculate_statistics_lexical(df_twitter_features)\n",
        "# print(twitter_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in twitter_statistics.items()}\n",
        "df_twitter_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'Twitter'])\n",
        "df_twitter_statistics.to_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized_statistics_lexical_1.csv', index=False)\n",
        "df_twitter_statistics"
      ],
      "metadata": {
        "id": "7sRT3yCYM5v_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "\n",
        "def normalize_dict_str(dict_str):\n",
        "    if not dict_str or not isinstance(dict_str, str) or not dict_str.startswith('{'):\n",
        "        return dict_str  # Return original if not a dictionary string\n",
        "    dict_obj = ast.literal_eval(dict_str)\n",
        "    total = sum(dict_obj.values())\n",
        "    normalized_dict = {k: v / total for k, v in dict_obj.items()}\n",
        "    return str(normalized_dict)\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized_statistics_lexical.csv')\n",
        "rows_to_normalize = ['stopwords_frequency', 'letter_frequency', 'digit_frequency']\n",
        "df.loc[df['Metric'].isin(rows_to_normalize), 'Twitter'] = df.loc[df['Metric'].isin(rows_to_normalize), 'Twitter'].apply(normalize_dict_str)\n",
        "df.to_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized_statistics_lexical.csv', index=False)\n",
        "df"
      ],
      "metadata": {
        "id": "KULg2G92U8yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntactic\n",
        "tqdm.pandas()\n",
        "df_twitter_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized.csv')\n",
        "# df_twitter_tokenized['pos_tags'] = df_twitter_tokenized['tokenized_prompt'].progress_apply(get_pos_tags)\n",
        "df_twitter_features = extract_features_syntactic(df_twitter_tokenized)\n",
        "# print(df_diffusiondb_features)\n",
        "df_twitter_features.to_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized_syntactic.csv', index=False)\n",
        "\n",
        "df_twitter_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized_syntactic.csv')\n",
        "\n",
        "twitter_statistics = calculate_statistics_syntactic(df_twitter_features)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in twitter_statistics.items()}\n",
        "df_twitter_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'Twitter'])\n",
        "df_twitter_statistics.to_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized_statistics_syntactic.csv', index=False)\n",
        "df_twitter_statistics"
      ],
      "metadata": {
        "id": "AzFp4GsENsBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram based\n",
        "df_twitter_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized.csv')\n",
        "# df_twitter_tokenized['pos_tags'] = df_twitter_tokenized['tokenized_prompt'].apply(get_pos_tags)\n",
        "# df_twitter_tokenized.to_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized.csv', index=False)\n",
        "\n",
        "# df_twitter_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized.csv')\n",
        "\n",
        "twitter_statistics = calculate_statistics_ngrams(df_twitter_tokenized)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "twitter_statistics = {k: {tuple(ngram): count for ngram, count in v} for k, v in twitter_statistics.items()}\n",
        "df_twitter_statistics = pd.DataFrame(list(twitter_statistics.items()), columns=['Metric', 'Twitter'])\n",
        "df_twitter_statistics.to_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/preprocessed_twitter_tokenized_statistics_ngrams.csv', index=False)\n",
        "df_twitter_statistics"
      ],
      "metadata": {
        "id": "z8Mp_JrVOKYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hardness\n",
        "def sample_prompts(group, n=100):\n",
        "    return group.sample(n=n, random_state=42) if len(group) > n else group\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/author_texts_cleaned.csv')\n",
        "df.columns = ['user_name', 'prompt']\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print('Unique Users:', unique_user_names_count)\n",
        "df_filtered = df.groupby('user_name').filter(lambda x: len(x) >= 100)\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print(f\"Number of users with at least 100 prompts: {unique_user_names_count}\")\n",
        "\n",
        "rh_blog = []\n",
        "for lim in [5, 10, 25, 50, 75, 100, 150, 200]:\n",
        "  print(lim)\n",
        "  list_spk = list(pd.DataFrame(df_filtered['user_name'].value_counts()[:lim]).index)\n",
        "  sub_df = df_filtered[df_filtered['user_name'].isin(list_spk)]\n",
        "  sampled_df = sub_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "  rh_blog.append(rel_hardness(sampled_df, 'user_name', 'prompt'))\n",
        "\n",
        "rh_blog"
      ],
      "metadata": {
        "id": "aSnkbtl2Ou-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topics\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data', type=str, default='ccat10', help='data')\n",
        "parser.add_argument('--data_path', type=str, default='/home/yunita/Data/Dataset/Stamatatos/c10_traintest.csv', help='data path')\n",
        "parser.add_argument('--n_topics', help='number of topics')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args = parser.parse_args(args=[\n",
        "    '--data', 'twitter_micro',\n",
        "    '--data_path', '/content/drive/MyDrive/msc_project/data/twitter_micro/author_texts_cleaned.csv',\n",
        "    '--n_topics', '[3, 10, 20, 30, 40, 50]'\n",
        "])\n",
        "\n",
        "topics_analysis(args)"
      ],
      "metadata": {
        "id": "ktiZ63yFOzuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xBms_uQVSVWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## IMDB62"
      ],
      "metadata": {
        "id": "V07TWqonT3H9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "df_imdb62 = pd.read_csv('/content/drive/MyDrive/msc_project/data/imdb62/imdb62.csv')\n",
        "df_imdb62.columns = ['user_name', 'prompt']\n",
        "df_imdb62['tokenized_prompt'] = df_imdb62['prompt'].progress_apply(tokenize)\n",
        "df_imdb62['pos_tags'] = df_imdb62['tokenized_prompt'].progress_apply(get_pos_tags)\n",
        "df_imdb62.to_csv('/content/drive/MyDrive/msc_project/data/imdb62/preprocessed_imdb62_tokenized.csv', index=False)\n",
        "df_imdb62"
      ],
      "metadata": {
        "id": "kOMbAGNqT4np"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical\n",
        "tqdm.pandas()\n",
        "df_imdb62_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/imdb62/preprocessed_imdb62_tokenized.csv')\n",
        "\n",
        "df_imdb62_features = extract_features_lexical(df_imdb62_tokenized)\n",
        "df_imdb62_features.to_csv('/content/drive/MyDrive/msc_project/data/imdb62/preprocessed_imdb62_tokenized_lexical.csv', index=False)\n",
        "\n",
        "df_imdb62_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/imdb62/preprocessed_imdb62_tokenized_lexical.csv')\n",
        "\n",
        "imdb62_statistics = calculate_statistics_lexical(df_imdb62_features)\n",
        "# print(twitter_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in imdb62_statistics.items()}\n",
        "df_imdb62_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'IMDB62'])\n",
        "df_imdb62_statistics.to_csv('/content/drive/MyDrive/msc_project/data/imdb62/preprocessed_imdb62_tokenized_statistics_lexical.csv', index=False)\n",
        "df_imdb62_statistics"
      ],
      "metadata": {
        "id": "lap5iQySUqy9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntactic\n",
        "tqdm.pandas()\n",
        "df_imdb62_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/imdb62/preprocessed_imdb62_tokenized.csv')\n",
        "\n",
        "df_imdb62_features = extract_features_syntactic(df_imdb62_tokenized)\n",
        "# print(df_diffusiondb_features)\n",
        "df_imdb62_features.to_csv('/content/drive/MyDrive/msc_project/data/imdb62/preprocessed_imdb62_tokenized_syntactic.csv', index=False)\n",
        "\n",
        "df_imdb62_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/imdb62/preprocessed_imdb62_tokenized_syntactic.csv')\n",
        "\n",
        "imdb62_statistics = calculate_statistics_syntactic(df_imdb62_features)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in imdb62_statistics.items()}\n",
        "df_imdb62_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'IMDB62'])\n",
        "df_imdb62_statistics.to_csv('/content/drive/MyDrive/msc_project/data/imdb62/preprocessed_imdb62_tokenized_statistics_syntactic.csv', index=False)\n",
        "df_imdb62_statistics"
      ],
      "metadata": {
        "id": "dDBfnyU0WxZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram based\n",
        "df_imdb62_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/imdb62/preprocessed_imdb62_tokenized.csv')\n",
        "\n",
        "imdb62_statistics = calculate_statistics_ngrams(df_imdb62_tokenized)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "imdb62_statistics = {k: {tuple(ngram): count for ngram, count in v} for k, v in imdb62_statistics.items()}\n",
        "df_imdb62_statistics = pd.DataFrame(list(imdb62_statistics.items()), columns=['Metric', 'IMDB62'])\n",
        "df_imdb62_statistics.to_csv('/content/drive/MyDrive/msc_project/data/imdb62/preprocessed_imdb62_tokenized_statistics_ngrams.csv', index=False)\n",
        "df_imdb62_statistics"
      ],
      "metadata": {
        "id": "ID4B9h6_XoH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hardness\n",
        "def sample_prompts(group, n=100):\n",
        "    return group.sample(n=n, random_state=42) if len(group) > n else group\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/imdb62/imdb62.csv')\n",
        "df.columns = ['user_name', 'prompt']\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print('Unique Users:', unique_user_names_count)\n",
        "df_filtered = df.groupby('user_name').filter(lambda x: len(x) >= 100)\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print(f\"Number of users with at least 100 prompts: {unique_user_names_count}\")\n",
        "\n",
        "rh_blog = []\n",
        "for lim in [5, 10, 25, 50, 75, 100, 150, 200]:\n",
        "  print(lim)\n",
        "  list_spk = list(pd.DataFrame(df_filtered['user_name'].value_counts()[:lim]).index)\n",
        "  sub_df = df_filtered[df_filtered['user_name'].isin(list_spk)]\n",
        "  sampled_df = sub_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "  rh_blog.append(rel_hardness(sampled_df, 'user_name', 'prompt'))\n",
        "\n",
        "rh_blog"
      ],
      "metadata": {
        "id": "3waqBtE3YO6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topics\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data', type=str, default='ccat10', help='data')\n",
        "parser.add_argument('--data_path', type=str, default='/home/yunita/Data/Dataset/Stamatatos/c10_traintest.csv', help='data path')\n",
        "parser.add_argument('--n_topics', help='number of topics')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args = parser.parse_args(args=[\n",
        "    '--data', 'imdb62',\n",
        "    '--data_path', '/content/drive/MyDrive/msc_project/data/imdb62/imdb62.csv',\n",
        "    '--n_topics', '[3, 10, 20, 30, 40, 50]'\n",
        "])\n",
        "\n",
        "topics_analysis(args)"
      ],
      "metadata": {
        "id": "swjewfNlYaRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MtVF-urccN8k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blogs"
      ],
      "metadata": {
        "id": "u80ucNwtEohn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "df_blogs = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs/blogs.csv')\n",
        "df_blogs.columns = ['user_name', 'prompt']\n",
        "df_blogs['tokenized_prompt'] = df_blogs['prompt'].progress_apply(tokenize)\n",
        "df_blogs['pos_tags'] = df_blogs['tokenized_prompt'].progress_apply(get_pos_tags)\n",
        "df_blogs.to_csv('/content/drive/MyDrive/msc_project/data/blogs/preprocessed_blogs_tokenized.csv', index=False)\n",
        "df_blogs"
      ],
      "metadata": {
        "id": "82CQToUfEp0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical\n",
        "tqdm.pandas()\n",
        "df_blogs_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs/preprocessed_blogs_tokenized.csv')\n",
        "\n",
        "df_blogs_features = extract_features_lexical(df_blogs_tokenized)\n",
        "df_blogs_features.to_csv('/content/drive/MyDrive/msc_project/data/blogs/preprocessed_blogs_tokenized_lexical.csv', index=False)\n",
        "\n",
        "df_blogs_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs/preprocessed_blogs_tokenized_lexical.csv')\n",
        "\n",
        "blogs_statistics = calculate_statistics_lexical(df_blogs_features)\n",
        "# print(twitter_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in blogs_statistics.items()}\n",
        "df_blogs_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'blogs'])\n",
        "df_blogs_statistics.to_csv('/content/drive/MyDrive/msc_project/data/blogs/preprocessed_blogs_tokenized_statistics_lexical.csv', index=False)\n",
        "df_blogs_statistics"
      ],
      "metadata": {
        "id": "euF80ww3E8JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntactic\n",
        "tqdm.pandas()\n",
        "df_blogs_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs/preprocessed_blogs_tokenized.csv')\n",
        "\n",
        "df_blogs_features = extract_features_syntactic(df_blogs_tokenized)\n",
        "# print(df_diffusiondb_features)\n",
        "df_blogs_features.to_csv('/content/drive/MyDrive/msc_project/data/blogs/preprocessed_blogs_tokenized_syntactic.csv', index=False)\n",
        "\n",
        "df_blogs_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs/preprocessed_blogs_tokenized_syntactic.csv')\n",
        "\n",
        "blogs_statistics = calculate_statistics_syntactic(df_blogs_features)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in blogs_statistics.items()}\n",
        "df_blogs_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'blogs'])\n",
        "df_blogs_statistics.to_csv('/content/drive/MyDrive/msc_project/data/blogs/preprocessed_blogs_tokenized_statistics_syntactic.csv', index=False)\n",
        "df_blogs_statistics"
      ],
      "metadata": {
        "id": "aPaDUrAuFzAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram based\n",
        "df_blogs_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs/preprocessed_blogs_tokenized.csv')\n",
        "\n",
        "blogs_statistics = calculate_statistics_ngrams(df_blogs_tokenized)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "blogs_statistics = {k: {tuple(ngram): count for ngram, count in v} for k, v in blogs_statistics.items()}\n",
        "df_blogs_statistics = pd.DataFrame(list(blogs_statistics.items()), columns=['Metric', 'blogs'])\n",
        "df_blogs_statistics.to_csv('/content/drive/MyDrive/msc_project/data/blogs/preprocessed_blogs_tokenized_statistics_ngrams.csv', index=False)\n",
        "df_blogs_statistics"
      ],
      "metadata": {
        "id": "7MDOXOd0GIzW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hardness\n",
        "def sample_prompts(group, n=100):\n",
        "    return group.sample(n=n, random_state=42) if len(group) > n else group\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs/blogs.csv')\n",
        "df.columns = ['user_name', 'prompt']\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print('Unique Users:', unique_user_names_count)\n",
        "df_filtered = df.groupby('user_name').filter(lambda x: len(x) >= 100)\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print(f\"Number of users with at least 100 prompts: {unique_user_names_count}\")\n",
        "\n",
        "rh_blog = []\n",
        "for lim in [5, 10, 25, 50]:\n",
        "  print(lim)\n",
        "  list_spk = list(pd.DataFrame(df_filtered['user_name'].value_counts()[:lim]).index)\n",
        "  sub_df = df_filtered[df_filtered['user_name'].isin(list_spk)]\n",
        "  sampled_df = sub_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "  rh_blog.append(rel_hardness(sampled_df, 'user_name', 'prompt'))\n",
        "\n",
        "rh_blog"
      ],
      "metadata": {
        "id": "OaHiS7P9GaUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topics\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data', type=str, default='ccat10', help='data')\n",
        "parser.add_argument('--data_path', type=str, default='/home/yunita/Data/Dataset/Stamatatos/c10_traintest.csv', help='data path')\n",
        "parser.add_argument('--n_topics', help='number of topics')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args = parser.parse_args(args=[\n",
        "    '--data', 'blogs',\n",
        "    '--data_path', '/content/drive/MyDrive/msc_project/data/blogs/blogs.csv',\n",
        "    '--n_topics', '[3, 10, 20, 30, 40, 50]'\n",
        "])\n",
        "\n",
        "topics_analysis(args)"
      ],
      "metadata": {
        "id": "EfZsBkWdGwu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DmAJaiadG_5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6R43axMtzAak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blogs50"
      ],
      "metadata": {
        "id": "i8zjk0v9zFTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "df_blogs = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs50/blogs50.csv')\n",
        "df_blogs.columns = ['user_name', 'prompt']\n",
        "df_blogs['tokenized_prompt'] = df_blogs['prompt'].progress_apply(tokenize)\n",
        "df_blogs['pos_tags'] = df_blogs['tokenized_prompt'].progress_apply(get_pos_tags)\n",
        "df_blogs.to_csv('/content/drive/MyDrive/msc_project/data/blogs50/preprocessed_blogs50_tokenized.csv', index=False)\n",
        "df_blogs"
      ],
      "metadata": {
        "id": "I6b_tN63zFT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical\n",
        "tqdm.pandas()\n",
        "df_blogs_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs50/preprocessed_blogs50_tokenized.csv')\n",
        "\n",
        "df_blogs_features = extract_features_lexical(df_blogs_tokenized)\n",
        "df_blogs_features.to_csv('/content/drive/MyDrive/msc_project/data/blogs50/preprocessed_blogs50_tokenized_lexical.csv', index=False)\n",
        "\n",
        "df_blogs_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs50/preprocessed_blogs50_tokenized_lexical.csv')\n",
        "\n",
        "blogs_statistics = calculate_statistics_lexical(df_blogs_features)\n",
        "# print(twitter_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in blogs_statistics.items()}\n",
        "df_blogs_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'blogs50'])\n",
        "df_blogs_statistics.to_csv('/content/drive/MyDrive/msc_project/data/blogs50/preprocessed_blogs50_tokenized_statistics_lexical.csv', index=False)\n",
        "df_blogs_statistics"
      ],
      "metadata": {
        "id": "ngRlgzZazFUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntactic\n",
        "tqdm.pandas()\n",
        "df_blogs_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs50/preprocessed_blogs50_tokenized.csv')\n",
        "\n",
        "df_blogs_features = extract_features_syntactic(df_blogs_tokenized)\n",
        "# print(df_diffusiondb_features)\n",
        "df_blogs_features.to_csv('/content/drive/MyDrive/msc_project/data/blogs50/preprocessed_blogs50_tokenized_syntactic.csv', index=False)\n",
        "\n",
        "df_blogs_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs50/preprocessed_blogs50_tokenized_syntactic.csv')\n",
        "\n",
        "blogs_statistics = calculate_statistics_syntactic(df_blogs_features)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in blogs_statistics.items()}\n",
        "df_blogs_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'blogs50'])\n",
        "df_blogs_statistics.to_csv('/content/drive/MyDrive/msc_project/data/blogs50/preprocessed_blogs50_tokenized_statistics_syntactic.csv', index=False)\n",
        "df_blogs_statistics"
      ],
      "metadata": {
        "id": "vPsNF2fNzFUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram based\n",
        "df_blogs_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs50/preprocessed_blogs50_tokenized.csv')\n",
        "\n",
        "blogs_statistics = calculate_statistics_ngrams(df_blogs_tokenized)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "blogs_statistics = {k: {tuple(ngram): count for ngram, count in v} for k, v in blogs_statistics.items()}\n",
        "df_blogs_statistics = pd.DataFrame(list(blogs_statistics.items()), columns=['Metric', 'blogs50'])\n",
        "df_blogs_statistics.to_csv('/content/drive/MyDrive/msc_project/data/blogs50/preprocessed_blogs50_tokenized_statistics_ngrams.csv', index=False)\n",
        "df_blogs_statistics"
      ],
      "metadata": {
        "id": "i3jPRqg-zFUA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hardness\n",
        "def sample_prompts(group, n=100):\n",
        "    return group.sample(n=n, random_state=42) if len(group) > n else group\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs50/blogs50.csv')\n",
        "df.columns = ['user_name', 'prompt']\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print('Unique Users:', unique_user_names_count)\n",
        "df_filtered = df.groupby('user_name').filter(lambda x: len(x) >= 100)\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print(f\"Number of users with at least 100 prompts: {unique_user_names_count}\")\n",
        "\n",
        "rh_blog = []\n",
        "for lim in [5, 10, 25, 50]:\n",
        "  print(lim)\n",
        "  list_spk = list(pd.DataFrame(df_filtered['user_name'].value_counts()[:lim]).index)\n",
        "  sub_df = df_filtered[df_filtered['user_name'].isin(list_spk)]\n",
        "  sampled_df = sub_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "  rh_blog.append(rel_hardness(sampled_df, 'user_name', 'prompt'))\n",
        "\n",
        "rh_blog"
      ],
      "metadata": {
        "id": "9JD-JymNzFUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topics\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data', type=str, default='ccat10', help='data')\n",
        "parser.add_argument('--data_path', type=str, default='/home/yunita/Data/Dataset/Stamatatos/c10_traintest.csv', help='data path')\n",
        "parser.add_argument('--n_topics', help='number of topics')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args = parser.parse_args(args=[\n",
        "    '--data', 'blogs50',\n",
        "    '--data_path', '/content/drive/MyDrive/msc_project/data/blogs50/blogs50.csv',\n",
        "    '--n_topics', '[3, 10, 20, 30, 40, 50]'\n",
        "])\n",
        "\n",
        "topics_analysis(args)"
      ],
      "metadata": {
        "id": "iPdsFdO_zFUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0F_lxZtjzFUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compare datasets"
      ],
      "metadata": {
        "id": "NHS1F5pv8J8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/train_random100_1.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_1.csv')\n",
        "\n",
        "df = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
        "df"
      ],
      "metadata": {
        "id": "hDm-2nLB8LC_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "df['tokenized_prompt'] = df['prompt'].apply(tokenize)\n",
        "df['pos_tags'] = df['tokenized_prompt'].apply(get_pos_tags)\n",
        "# print(df)\n",
        "df = extract_features_lexical(df)\n",
        "df = extract_features_syntactic(df)\n",
        "df = extract_features_ngrams(df)\n",
        "# print(df)\n",
        "lexical1 = calculate_statistics_lexical(df)\n",
        "ngrams1 = calculate_statistics_ngrams(df)\n",
        "syntactic1 = calculate_statistics_syntactic(df)\n",
        "print(lexical1)\n",
        "print(ngrams1)\n",
        "print(syntactic1)\n",
        "\n",
        "lexical2 = {k: str(v) for k, v in lexical1.items()}\n",
        "lexical3 = pd.DataFrame(list(lexical2.items()), columns=['Metric', 'DiffusionDB'])\n",
        "ngrams2 = {k: str(v) for k, v in ngrams1.items()}\n",
        "ngrams3 = pd.DataFrame(list(ngrams2.items()), columns=['Metric', 'DiffusionDB'])\n",
        "syntactic2 = {k: str(v) for k, v in syntactic1.items()}\n",
        "syntactic3 = pd.DataFrame(list(syntactic2.items()), columns=['Metric', 'DiffusionDB'])\n",
        "\n",
        "statistics = pd.concat([lexical3, ngrams3], ignore_index=True)\n",
        "statistics = pd.concat([statistics, syntactic3], ignore_index=True)\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/features_random100_1.csv', index=False)\n",
        "statistics.to_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/statistics_random100_1.csv', index=False)\n",
        "statistics"
      ],
      "metadata": {
        "id": "d9XMZqRNGcl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "19TzlpIowuhV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "ZDQrTimy1WUE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "columns = ['average_word_length', 'short_words_ratio', 'percentage_of_digits',\n",
        "       'percentage_of_uppercase', 'average_sentence_length', 'average_document_length']\n",
        "\n",
        "for column in columns:\n",
        "    plt.figure(figsize=(8, 6))  # Optional: to set the figure size for each plot\n",
        "    sns.histplot(df[column], kde=True, bins=30)  # kde=False to not include the kernel density estimate\n",
        "    plt.title(f'Histogram of {column}')  # Adding a title for clarity\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "slBExMp11Ol0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# df = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/author_texts_cleaned.csv')\n",
        "# df.columns = ['user_name', 'prompts']\n",
        "# # df_selected = df[['user_name', 'prompt']]\n",
        "# df_filtered = df.groupby('user_name').filter(lambda x: len(x) >= 100)\n",
        "# print('number of authors', len(df_filtered['user_name'].drop_duplicates()))\n",
        "\n",
        "# for idx in range(3):\n",
        "#   sampled_authors = df_filtered['user_name'].drop_duplicates().sample(n=100)\n",
        "#   df_sampled = df_filtered[df_filtered['user_name'].isin(sampled_authors)]\n",
        "#   df_final = df_sampled.groupby('user_name').apply(lambda x: x.sample(n=100)).reset_index(drop=True)\n",
        "\n",
        "#   train_data = pd.DataFrame()\n",
        "#   test_data = pd.DataFrame()\n",
        "\n",
        "#   for author in df_final['user_name'].unique():\n",
        "#       author_data = df_final[df_final['user_name'] == author]\n",
        "#       train, test = train_test_split(author_data, test_size=0.2)\n",
        "#       train_data = pd.concat([train_data, train])\n",
        "#       test_data = pd.concat([test_data, test])\n",
        "\n",
        "#   train_data.to_csv(f'/content/drive/MyDrive/msc_project/data/twitter_micro/processed/train_random100_{idx+1}.csv', index=False)\n",
        "#   test_data.to_csv(f'/content/drive/MyDrive/msc_project/data/twitter_micro/processed/test_random100_{idx+1}.csv', index=False)\n",
        "#   # print(train_data)\n",
        "#   # print(test_data)"
      ],
      "metadata": {
        "id": "xL6-51IxH-lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/processed/train_random100_1.csv')\n",
        "df_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/processed/test_random100_1.csv')\n",
        "\n",
        "df = pd.concat([df_train, df_test], axis=0).reset_index(drop=True)\n",
        "df.columns = ['user_name', 'prompt']\n",
        "df"
      ],
      "metadata": {
        "id": "8jRYI0z4FvxX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "df['tokenized_prompt'] = df['prompt'].apply(tokenize)\n",
        "df['pos_tags'] = df['tokenized_prompt'].apply(get_pos_tags)\n",
        "# print(df)\n",
        "df = extract_features_lexical(df)\n",
        "df = extract_features_syntactic(df)\n",
        "df = extract_features_ngrams(df)\n",
        "lexical1 = calculate_statistics_lexical(df)\n",
        "ngrams1 = calculate_statistics_ngrams(df)\n",
        "syntactic1 = calculate_statistics_syntactic(df)\n",
        "print(lexical1)\n",
        "print(ngrams1)\n",
        "print(syntactic1)\n",
        "\n",
        "lexical2 = {k: str(v) for k, v in lexical1.items()}\n",
        "lexical3 = pd.DataFrame(list(lexical2.items()), columns=['Metric', 'Twitter_micro'])\n",
        "ngrams2 = {k: str(v) for k, v in ngrams1.items()}\n",
        "ngrams3 = pd.DataFrame(list(ngrams2.items()), columns=['Metric', 'Twitter_micro'])\n",
        "syntactic2 = {k: str(v) for k, v in syntactic1.items()}\n",
        "syntactic3 = pd.DataFrame(list(syntactic2.items()), columns=['Metric', 'Twitter_micro'])\n",
        "\n",
        "statistics = pd.concat([lexical3, ngrams3], ignore_index=True)\n",
        "statistics = pd.concat([statistics, syntactic3], ignore_index=True)\n",
        "statistics\n",
        "\n",
        "df.to_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/processed/features_random100_1.csv', index=False)\n",
        "statistics.to_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/processed/statistics_random100_1.csv', index=False)\n",
        "statistics"
      ],
      "metadata": {
        "id": "P9BwO-d1ONsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "RF6qu7jyOSfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "columns = ['average_word_length', 'short_words_ratio', 'percentage_of_digits',\n",
        "       'percentage_of_uppercase', 'average_sentence_length', 'average_document_length']\n",
        "\n",
        "for column in columns:\n",
        "    plt.figure(figsize=(8, 6))  # Optional: to set the figure size for each plot\n",
        "    sns.histplot(df[column], kde=True, bins=30)  # kde=False to not include the kernel density estimate\n",
        "    plt.title(f'Histogram of {column}')  # Adding a title for clarity\n",
        "    plt.xlabel(column)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "kYMoyjHFJ6j2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test whether the data is normally distributed\n",
        "import scipy.stats as stats\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "def test_normal_distribution(df):\n",
        "  columns = ['average_word_length', 'short_words_ratio', 'percentage_of_digits',\n",
        "        'percentage_of_uppercase', 'average_sentence_length', 'average_document_length']\n",
        "\n",
        "  for column in columns:\n",
        "    print(column)\n",
        "    data = df[column].tolist()\n",
        "    # Shapiro-Wilk Test\n",
        "    stat, p = stats.shapiro(data)\n",
        "    print(f'Shapiro-Wilk Test: Statistics={stat}, p={p}')\n",
        "\n",
        "    # Kolmogorov-Smirnov Test\n",
        "    stat, p = stats.kstest(data, 'norm')\n",
        "    print(f'Kolmogorov-Smirnov Test: Statistics={stat}, p={p}')\n",
        "\n",
        "    # Anderson-Darling Test\n",
        "    result = stats.anderson(data, dist='norm')\n",
        "    print(f'Anderson-Darling Test: Statistic={result.statistic}')\n",
        "    for i in range(len(result.critical_values)):\n",
        "        sl = result.significance_level[i]\n",
        "        cv = result.critical_values[i]\n",
        "        if result.statistic < cv:\n",
        "            print(f'{sl}%: {cv}, data looks normal (fail to reject H0)')\n",
        "        else:\n",
        "            print(f'{sl}%: {cv}, data does not look normal (reject H0)')\n",
        "\n",
        "    # D'Agostino's K^2 Test\n",
        "    stat, p = stats.normaltest(data)\n",
        "    print(f'D\\'Agostino\\'s K^2 Test: Statistics={stat}, p={p}')\n",
        "\n",
        "    skewness = skew(data)\n",
        "    kurt = kurtosis(data)\n",
        "\n",
        "    print(f'Skewness: {skewness}')\n",
        "    print(f'Kurtosis: {kurt}')  # If kurtosis() gives excess kurtosis (subtract 3 from kurtosis)\n",
        "\n",
        "    print()\n",
        "\n",
        "print('---diffusiondb---')\n",
        "test_normal_distribution(df_diffusiondb)\n",
        "print('---df_twitter---')\n",
        "test_normal_distribution(df_twitter)"
      ],
      "metadata": {
        "id": "-wev7a255eUl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare datasets\n",
        "import scipy.stats as stats\n",
        "\n",
        "def compare_two_datasets(df1, df2):\n",
        "  columns = ['average_word_length', 'short_words_ratio', 'percentage_of_digits',\n",
        "        'percentage_of_uppercase', 'average_sentence_length', 'average_document_length']\n",
        "\n",
        "  for column in columns:\n",
        "    print(column)\n",
        "    data1 = df1[column].tolist()\n",
        "    data2 = df2[column].tolist()\n",
        "\n",
        "    # Mann-Whitney U Test\n",
        "    stat, p = stats.mannwhitneyu(data1, data2)\n",
        "    print(f'Mann-Whitney U Test: Statistics={stat}, p={p}')\n",
        "    print(f\"z-score: {z}\")\n",
        "    print(f\"Effect size (Cohen's r): {r}\")\n",
        "\n",
        "    # Convert U statistic to z-score\n",
        "    n1 = len(data1)\n",
        "    n2 = len(data2)\n",
        "    mean_u = n1 * n2 / 2\n",
        "    std_u = np.sqrt(n1 * n2 * (n1 + n2 + 1) / 12)\n",
        "    z = (stat - mean_u) / std_u\n",
        "\n",
        "    # Calculate the effect size (Cohen's r)\n",
        "    r = z / np.sqrt(n1 + n2)\n",
        "\n",
        "    # Interpretation\n",
        "    if p < 0.05:\n",
        "        print(\"The two datasets have significantly different distributions (reject H0).\")\n",
        "    else:\n",
        "        print(\"The two datasets do not have significantly different distributions (fail to reject H0).\")\n",
        "\n",
        "    # if r <= 0.1:\n",
        "    #   print('small effect size')\n",
        "    # elif r <= 0.3:\n",
        "    #   print('medium effect size')\n",
        "    # elif r <\n",
        "\n",
        "\n",
        "    print()\n",
        "\n",
        "\n",
        "compare_two_datasets(df_diffusiondb, df_twitter)\n"
      ],
      "metadata": {
        "id": "swY0IcXb6ku1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-bio"
      ],
      "metadata": {
        "id": "w1Z3CfRgGske"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_diffusiondb = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/features_random100_1.csv')\n",
        "df_twitter = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/processed/features_random100_1.csv')"
      ],
      "metadata": {
        "id": "Yuiys6B43WsV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.spatial.distance import pdist, squareform  # Correct import\n",
        "from skbio.stats.ordination import pcoa\n",
        "from skbio.stats.distance import permanova, DistanceMatrix  # Import DistanceMatrix\n",
        "\n",
        "# PERMANOVA\n",
        "def compare_two_datasets_multiple(df1, df2):\n",
        "    columns = ['digit_frequency', 'pos_frequency', 'stopwords_frequency',\n",
        "               'punctuation_frequency', 'letter_frequency',\n",
        "               'char_bigrams_common_frequency', 'char_trigrams_common_frequency',\n",
        "               'word_unigrams_common_frequency', 'word_bigrams_common_frequency',\n",
        "               'word_trigrams_common_frequency', 'pos_bigrams_common_frequency',\n",
        "               'pos_trigrams_common_frequency']\n",
        "\n",
        "    for column in columns:\n",
        "        # Convert the dictionary column to DataFrame for both df1 and df2\n",
        "        # print(type(df1[column]))\n",
        "        df1[column] = df1[column].apply(lambda x: eval(x))\n",
        "        df2[column] = df2[column].apply(lambda x: eval(x))\n",
        "        df11 = pd.json_normalize(df1[column])\n",
        "        df22 = pd.json_normalize(df2[column])\n",
        "        # print(df11.head(5))\n",
        "        # Concatenate the two DataFrames along rows\n",
        "        combined_df = pd.concat([df11, df22], axis=0)\n",
        "        # print(combined_df)\n",
        "        if combined_df.isnull().values.any():\n",
        "            print(f\"NaN values found in {column}. Filling NaNs with 0.\")\n",
        "            combined_df = combined_df.fillna(0)  # or combined_df.dropna()\n",
        "\n",
        "        distance_matrix = pdist(combined_df, metric='euclidean')\n",
        "        distance_matrix = squareform(distance_matrix)\n",
        "\n",
        "        # Convert to DistanceMatrix object\n",
        "        distance_matrix = DistanceMatrix(distance_matrix)\n",
        "\n",
        "        # Create a grouping variable that distinguishes between df1 and df2\n",
        "        labels = np.array([0]*len(df11) + [1]*len(df22))\n",
        "\n",
        "        # Perform PERMANOVA\n",
        "        result = permanova(distance_matrix, labels, permutations=999)\n",
        "\n",
        "        # Extract sums of squares\n",
        "        SSB = result['test statistic'] * result['sample size'] * (len(np.unique(labels)) - 1) / (len(labels) - 1)\n",
        "        SST = SSB + result['test statistic'] * result['sample size']\n",
        "\n",
        "        # Calculate eta squared\n",
        "        eta_squared = SSB / SST\n",
        "\n",
        "        print(f\"PERMANOVA result for {column}:\")\n",
        "        print(result)\n",
        "        print(f\"Eta^2 (Effect Size) for {column}: {eta_squared}\")\n",
        "        print(\"\\n\")\n",
        "\n",
        "# Example usage:\n",
        "compare_two_datasets_multiple(df_diffusiondb, df_twitter)\n"
      ],
      "metadata": {
        "id": "OGBpp51y_JtK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bEc-EIWWSSc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# diffusiondb clean"
      ],
      "metadata": {
        "id": "tcg24IS4NGV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_diffusiondb = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/clean/all.csv')\n",
        "df_diffusiondb['tokenized_prompt'] = df_diffusiondb['prompt'].apply(tokenize)\n",
        "df_diffusiondb.to_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/clean/preprocessed_diffusiondb_tokenized.csv', index=False)"
      ],
      "metadata": {
        "id": "yRcScOuENIyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical\n",
        "tqdm.pandas()\n",
        "df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/clean/preprocessed_diffusiondb_tokenized.csv')\n",
        "\n",
        "df_diffusiondb_features = extract_features_lexical(df_diffusiondb_tokenized)\n",
        "df_diffusiondb_features.to_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/clean/preprocessed_diffusiondb_tokenized_lexical.csv', index=False)\n",
        "\n",
        "df_diffusiondb_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/clean/preprocessed_diffusiondb_tokenized_lexical.csv')\n",
        "\n",
        "diffusiondb_statistics = calculate_statistics_lexical(df_diffusiondb_features)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in diffusiondb_statistics.items()}\n",
        "df_diffusiondb_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'DiffusionDB'])\n",
        "df_diffusiondb_statistics.to_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/clean/preprocessed_diffusiondb_tokenized_statistics_lexical.csv', index=False)\n",
        "df_diffusiondb_statistics"
      ],
      "metadata": {
        "id": "wTd7S5OjNJmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topics\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data', type=str, default='ccat10', help='data')\n",
        "parser.add_argument('--data_path', type=str, default='/home/yunita/Data/Dataset/Stamatatos/c10_traintest.csv', help='data path')\n",
        "parser.add_argument('--n_topics', help='number of topics')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args = parser.parse_args(args=[\n",
        "    '--data', 'diffusiondb',\n",
        "    '--data_path', '/content/drive/MyDrive/msc_project/data/diffusiondb/clean/all.csv',\n",
        "    '--n_topics', '[3, 10, 20, 30, 40, 50]'\n",
        "])\n",
        "\n",
        "topics_analysis(args)"
      ],
      "metadata": {
        "id": "TAuXk_WcNM5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DiffusionDB 10000"
      ],
      "metadata": {
        "id": "OFHjzZTKHCl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_diffusiondb = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/final_llama3.csv')\n",
        "df_diffusiondb['tokenized_prompt'] = df_diffusiondb['prompt'].apply(tokenize)\n",
        "df_diffusiondb.to_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/preprocessed_diffusiondb_tokenized.csv', index=False)"
      ],
      "metadata": {
        "id": "p8Lgxs5oHCl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical\n",
        "tqdm.pandas()\n",
        "df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/preprocessed_diffusiondb_tokenized.csv')\n",
        "\n",
        "df_diffusiondb_features = extract_features_lexical(df_diffusiondb_tokenized)\n",
        "df_diffusiondb_features.to_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/preprocessed_diffusiondb_tokenized_lexical.csv', index=False)\n",
        "\n",
        "df_diffusiondb_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/preprocessed_diffusiondb_tokenized_lexical.csv')\n",
        "\n",
        "diffusiondb_statistics = calculate_statistics_lexical(df_diffusiondb_features)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in diffusiondb_statistics.items()}\n",
        "df_diffusiondb_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'DiffusionDB'])\n",
        "df_diffusiondb_statistics.to_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/preprocessed_diffusiondb_tokenized_statistics_lexical.csv', index=False)\n",
        "df_diffusiondb_statistics"
      ],
      "metadata": {
        "id": "hb7W9Ta2HCl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Syntactic\n",
        "tqdm.pandas()\n",
        "df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "df_diffusiondb_features = extract_features_syntactic(df_diffusiondb_tokenized)\n",
        "# print(df_diffusiondb_features)\n",
        "df_diffusiondb_features.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_syntactic.csv', index=False)\n",
        "\n",
        "df_diffusiondb_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_syntactic.csv')\n",
        "\n",
        "diffusiondb_statistics = calculate_statistics_syntactic(df_diffusiondb_features)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "flattened_data = {k: str(v) for k, v in diffusiondb_statistics.items()}\n",
        "df_diffusiondb_statistics = pd.DataFrame(list(flattened_data.items()), columns=['Metric', 'DiffusionDB'])\n",
        "df_diffusiondb_statistics.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_statistics_syntactic.csv', index=False)\n",
        "df_diffusiondb_statistics"
      ],
      "metadata": {
        "id": "xHHYMRHwHCl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N-gram based\n",
        "# df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "# df_diffusiondb_tokenized['pos_tags'] = df_diffusiondb_tokenized['tokenized_prompt'].apply(get_pos_tags)\n",
        "# df_diffusiondb_tokenized.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv', index=False)\n",
        "\n",
        "df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "\n",
        "# df_diffusiondb_features = extract_features_ngrams(df_diffusiondb_tokenized)\n",
        "# df_diffusiondb_features.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_ngrams.csv', index=False)\n",
        "\n",
        "# df_diffusiondb_features = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_ngrams.csv')\n",
        "\n",
        "diffusiondb_statistics = calculate_statistics_ngrams(df_diffusiondb_tokenized)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "diffusiondb_statistics = {k: {tuple(ngram): count for ngram, count in v} for k, v in diffusiondb_statistics.items()}\n",
        "df_diffusiondb_statistics = pd.DataFrame(list(diffusiondb_statistics.items()), columns=['Metric', 'DiffusionDB'])\n",
        "df_diffusiondb_statistics.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_statistics_ngrams.csv', index=False)\n",
        "df_diffusiondb_statistics"
      ],
      "metadata": {
        "id": "ta95bcYUHCl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hardness\n",
        "def sample_prompts(group, n=100):\n",
        "    return group.sample(n=n, random_state=42) if len(group) > n else group\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb.csv')\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print('Unique Users:', unique_user_names_count)\n",
        "df_filtered = df.groupby('user_name').filter(lambda x: len(x) >= 100)\n",
        "unique_user_names_count = df['user_name'].nunique()\n",
        "print(f\"Number of users with at least 100 prompts: {unique_user_names_count}\")\n",
        "\n",
        "rh_blog = []\n",
        "for lim in [5, 10, 25, 50, 75, 100, 150, 200]:\n",
        "  print(lim)\n",
        "  list_spk = list(pd.DataFrame(df_filtered['user_name'].value_counts()[:lim]).index)\n",
        "  sub_df = df_filtered[df_filtered['user_name'].isin(list_spk)]\n",
        "  sampled_df = sub_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "  rh_blog.append(rel_hardness(sampled_df, 'user_name', 'prompt'))\n",
        "\n",
        "rh_blog"
      ],
      "metadata": {
        "id": "8K7eViK9HCl4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Topics\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--data', type=str, default='ccat10', help='data')\n",
        "parser.add_argument('--data_path', type=str, default='/home/yunita/Data/Dataset/Stamatatos/c10_traintest.csv', help='data path')\n",
        "parser.add_argument('--n_topics', help='number of topics')\n",
        "\n",
        "# args = parser.parse_args()\n",
        "args = parser.parse_args(args=[\n",
        "    '--data', 'diffusiondb',\n",
        "    '--data_path', '/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/final_llama3.csv',\n",
        "    '--n_topics', '[3, 10, 20, 30, 40, 50]'\n",
        "])\n",
        "\n",
        "topics_analysis(args)"
      ],
      "metadata": {
        "id": "6WT-tdiyHCl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BQdhbPayHCl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_diffusiondb_features)"
      ],
      "metadata": {
        "id": "bNU0TEQVHCl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diffusiondb_statistics = calculate_statistics_syntactic(df_diffusiondb_features.head(50))"
      ],
      "metadata": {
        "id": "TqlZq9skHCl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_diffusiondb_features = extract_features_syntactic(df_diffusiondb_tokenized.head(5))\n",
        "diffusiondb_statistics = calculate_statistics_syntactic(df_diffusiondb_features)\n",
        "print(diffusiondb_statistics)"
      ],
      "metadata": {
        "id": "_deOBwL7HCl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tqdm.pandas()\n",
        "df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "\n",
        "# df_diffusiondb_tokenized = df_diffusiondb_tokenized.head(5)\n",
        "# df_diffusiondb_tokenized['spacy_doc'] = df_diffusiondb_tokenized['prompt'].progress_apply(process_text)\n",
        "\n",
        "# with open('/content/drive/MyDrive/msc_project/data/spacy_docs.pkl', 'wb') as f:\n",
        "#     pickle.dump(df_diffusiondb_tokenized['spacy_doc'].tolist(), f)\n",
        "\n",
        "# with open('/content/drive/MyDrive/msc_project/data/spacy_docs.pkl', 'rb') as f:\n",
        "#     spacy_docs = pickle.load(f)\n",
        "\n",
        "# df_diffusiondb_tokenized['spacy_doc'] = spacy_docs\n",
        "# print(df_diffusiondb_tokenized)\n",
        "# # print(df_diffusiondb_features)\n",
        "\n",
        "# df_diffusiondb_features = extract_features_syntactic(df_diffusiondb_tokenized)\n",
        "# df_diffusiondb_features"
      ],
      "metadata": {
        "id": "8w_TDcsCHCl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The four main performers are riveting.\"\n",
        "# df_diffusiondb_tokenized = pd.DataFrame(data)\n",
        "# df_diffusiondb_tokenized['spacy_doc'] = df_diffusiondb_tokenized['prompt'].progress_apply(process_text)\n",
        "doc = process_text(text)\n",
        "# df_diffusiondb_features = extract_features_syntactic(df_diffusiondb_tokenized)\n",
        "# print(df_diffusiondb_features)\n",
        "# diffusiondb_statistics = calculate_statistics_syntactic(df_diffusiondb_features)\n",
        "# print(diffusiondb_statistics)"
      ],
      "metadata": {
        "id": "wWyUcwr6HCl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import benepar\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "benepar.download(\"benepar_en3\")\n",
        "nlp.add_pipe(\"benepar\", config={\"model\": \"benepar_en3\"})\n",
        "# Sample sentence\n",
        "sentence = \"The four main performers are riveting.\"\n",
        "\n",
        "# Parse the sentence\n",
        "doc = nlp(sentence)\n",
        "sent = list(doc.sents)[0]\n",
        "\n",
        "# Print the phrase structure tree\n",
        "print(sent._.parse_string)\n",
        "\n",
        "# Display the tree using nltk.Tree\n",
        "from nltk import Tree\n",
        "\n",
        "def print_tree(tree):\n",
        "    if not isinstance(tree, Tree):\n",
        "        return\n",
        "    tree.pretty_print()\n",
        "\n",
        "nltk_tree = Tree.fromstring(sent._.parse_string)\n",
        "print_tree(nltk_tree)\n",
        "\n",
        "\n",
        "def extract_ngrams(tree, n):\n",
        "    ngrams = []\n",
        "\n",
        "    def traverse(t):\n",
        "        if isinstance(t, Tree):\n",
        "            for i in range(len(t) - n + 1):\n",
        "                ngram = t[i:i+n]\n",
        "                if all(isinstance(child, Tree) for child in ngram):\n",
        "                    ngrams.append(ngram)\n",
        "            for child in t:\n",
        "                traverse(child)\n",
        "\n",
        "    traverse(tree)\n",
        "    return ngrams\n",
        "\n",
        "def print_ngrams(ngrams):\n",
        "    for ngram in ngrams:\n",
        "        print(' '.join([' '.join(child.leaves()) for child in ngram]))\n",
        "        print(ngram)\n",
        "        print()\n",
        "\n",
        "# Extract n-grams of phrase structures\n",
        "n = 2  # Change this to any n you want\n",
        "ngrams = extract_ngrams(nltk_tree, 2)\n",
        "\n",
        "# Print the extracted n-grams\n",
        "print_ngrams(ngrams)"
      ],
      "metadata": {
        "id": "gqTygqR9HCl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diffusiondb_statistics = {k: {tuple(ngram): count for ngram, count in v} for k, v in diffusiondb_statistics.items()}\n",
        "df_diffusiondb_statistics = pd.DataFrame(list(diffusiondb_statistics.items()), columns=['feature', 'ngrams'])\n",
        "df_diffusiondb_statistics.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_statistics_syntactic.csv', index=False)\n",
        "df_diffusiondb_statistics"
      ],
      "metadata": {
        "id": "OIBDyuEyHCl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5ibPvrteHCl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df = pd.read_csv('/content/drive/MyDrive/authorship_inference_attack/real_diffusionDB_large_all_features 1.csv')\n",
        "df_diffusiondb1 = pd.read_csv('1.csv')\n",
        "df_diffusiondb1['prompt'] = df_diffusiondb1['prompt'].astype(str)\n",
        "# df_diffusiondb1['tokenized_prompt'] = df_diffusiondb1['prompt'].apply(tokenize)\n",
        "# df_diffusiondb1.to_csv('1.csv', index=False)\n",
        "\n",
        "# df_diffusiondb.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv', index=False)\n",
        "\n",
        "# df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "\n",
        "df_diffusiondb_features1 = extract_features(df_diffusiondb1)\n",
        "# df_diffusiondb_features.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_lexical.csv', index=False)\n",
        "\n",
        "diffusiondb_statistics1 = calculate_statistics(df_diffusiondb_features1)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "df_diffusiondb_statistics1 = pd.DataFrame.from_dict(diffusiondb_statistics1, orient='index', columns=['DiffusionDB'])\n",
        "df_diffusiondb_statistics1"
      ],
      "metadata": {
        "id": "ueDvsjq3HCl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_diffusiondb2 = pd.read_csv('/content/drive/MyDrive/authorship_inference_attack/real_diffusionDB_large_all_features 1.csv')\n",
        "\n",
        "# df_diffusiondb2['prompt'] = df_diffusiondb2['prompt'].astype(str)\n",
        "# df_diffusiondb2['tokenized_prompt'] = df_diffusiondb2['prompt'].apply(lambda x: x.split())\n",
        "# df_diffusiondb2.to_csv('2.csv', index=False)\n",
        "df_diffusiondb2 = pd.read_csv('2.csv')\n",
        "df_diffusiondb2['prompt'] = df_diffusiondb2['prompt'].astype(str)\n",
        "# df_diffusiondb.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv', index=False)\n",
        "\n",
        "# df_diffusiondb_tokenized = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized.csv')\n",
        "\n",
        "df_diffusiondb_features2 = extract_features(df_diffusiondb2)\n",
        "# df_diffusiondb_features.to_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb_tokenized_lexical.csv', index=False)\n",
        "\n",
        "diffusiondb_statistics2 = calculate_statistics(df_diffusiondb_features2)\n",
        "# print(diffusiondb_statistics)\n",
        "\n",
        "df_diffusiondb_statistics2 = pd.DataFrame.from_dict(diffusiondb_statistics2, orient='index', columns=['DiffusionDB'])\n",
        "df_diffusiondb_statistics2"
      ],
      "metadata": {
        "id": "y9GC5ItHHCl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.ascii_letters[:26])"
      ],
      "metadata": {
        "id": "xhzO2RCRHCl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pJBb1Gd0HCl6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}