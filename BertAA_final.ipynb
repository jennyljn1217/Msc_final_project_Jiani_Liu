{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN8jdKvAz02q"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhY14uZoydBF"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jg3jYBPhyjxk"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNIMVKa5t2Jx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import os\n",
        "import sys\n",
        "import tqdm\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import copy\n",
        "import argparse\n",
        "import logging\n",
        "import time\n",
        "from datetime import datetime\n",
        "from urllib.request import urlretrieve\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import rcParams\n",
        "import joblib\n",
        "from functools import partial\n",
        "import wandb\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from transformers import BertConfig, BertModel, BertTokenizer, BertForSequenceClassification\n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L_iPyEnA4S0X"
      },
      "outputs": [],
      "source": [
        "# set seed for reproducibility\n",
        "def set_seed(seed_value=42):\n",
        "    \"\"\"Set seed for reproducibility.\"\"\"\n",
        "    np.random.seed(seed_value)  # Numpy module.\n",
        "    random.seed(seed_value)  # Python random module.\n",
        "    torch.manual_seed(seed_value)  # PyTorch to ensure deterministic behavior\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed(seed_value)  # Sets the seed for generating random numbers on all GPUs.\n",
        "        torch.cuda.manual_seed_all(seed_value)  # For multi-GPU.\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed_value)  # Set Python hash seed for reproducibility in hash-based operations.\n",
        "\n",
        "seed = 0\n",
        "set_seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mcs8BqAZ_8JJ"
      },
      "source": [
        "# dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8AriKsg8XR0"
      },
      "outputs": [],
      "source": [
        "def calculate_length_data(texts, tokenizer):\n",
        "  token_counts = []\n",
        "  i = 0\n",
        "  for text in texts:\n",
        "    # if i % 1000 == 0:\n",
        "    #   print(i)\n",
        "    # i += 1\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_counts.append(len(tokens))\n",
        "  print('max_length', max(token_counts))\n",
        "  plt.hist(token_counts, bins=30, alpha=0.5, color='blue', edgecolor='black')\n",
        "  plt.title('Histogram of token length')\n",
        "  plt.xlabel('Length')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_train.csv')\n",
        "print(df)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "calculate_length_data(df['text'], tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uk86PvEp37dQ"
      },
      "outputs": [],
      "source": [
        "def calculate_length_data(texts, tokenizer):\n",
        "  token_counts = []\n",
        "  i = 0\n",
        "  for text in texts:\n",
        "    # if i % 1000 == 0:\n",
        "    #   print(i)\n",
        "    # i += 1\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    token_counts.append(len(tokens))\n",
        "  print('max_length', max(token_counts))\n",
        "  plt.hist(token_counts, bins=30, alpha=0.5, color='blue', edgecolor='black')\n",
        "  plt.title('Histogram of token length')\n",
        "  plt.xlabel('Length')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/blogs50/processed/blogs50_train.csv')\n",
        "print(df)\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "calculate_length_data(df['text'], tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXlxjhR3xAz7"
      },
      "outputs": [],
      "source": [
        "# def preprocess_data(df, tokenizer, max_length, encoder_path, train=False):\n",
        "\n",
        "#     texts = df['prompt'].tolist()\n",
        "#     labels = df['user_name'].tolist()\n",
        "\n",
        "#     input_ids = []\n",
        "#     attention_masks = []\n",
        "#     for text in texts:\n",
        "#         encoded = tokenizer.encode_plus(\n",
        "#             text,\n",
        "#             add_special_tokens=True,\n",
        "#             max_length=max_length,\n",
        "#             padding='max_length',\n",
        "#             truncation=True,\n",
        "#             return_attention_mask=True,\n",
        "#             return_tensors='pt',\n",
        "#         )\n",
        "#         input_ids.append(encoded['input_ids'])\n",
        "#         attention_masks.append(encoded['attention_mask'])\n",
        "#     input_ids = torch.cat(input_ids, dim=0)\n",
        "#     attention_masks = torch.cat(attention_masks, dim=0)\n",
        "\n",
        "#     if train:\n",
        "#       label_encoder = LabelEncoder()\n",
        "#       labels = label_encoder.fit_transform(labels)\n",
        "#       joblib.dump(label_encoder, encoder_path)\n",
        "#     else:\n",
        "#       labels = label_encoder.transform(labels)\n",
        "\n",
        "#     labels = torch.tensor(labels)\n",
        "\n",
        "#     return TensorDataset(input_ids, attention_masks, labels)\n",
        "\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            self.texts[item],\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            return_token_type_ids=False,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt',\n",
        "        )\n",
        "        return {\n",
        "            'text': self.texts[item],\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(self.labels[item], dtype=torch.long)\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qjx0TVN3N1M"
      },
      "source": [
        "## DiffusionDB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoXVVvKsbnDW"
      },
      "outputs": [],
      "source": [
        "#############################################\n",
        "#### RUN THIS TO GET THE DATASET WE NEED\n",
        "\n",
        "# df_initial = pd.read_csv('cleaned_diffusionDB_large_data.csv', encoding='utf-8')\n",
        "df = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/cleaned_diffusionDB_dataFinal_T_25.csv', encoding='utf-8')\n",
        "# df = pd.read_csv('/content/drive/MyDrive/authorship_inference_attack/cleaned_diffusionDB_large_data_50_chars_final.csv', encoding='utf-8')\n",
        "\n",
        "\n",
        "df = df.astype(str)\n",
        "\n",
        "\n",
        "df_initial = df[df['user_name'] != 'deleted-account']\n",
        "\n",
        "print('Columns :', df_initial.columns)\n",
        "print(len(df_initial))\n",
        "\n",
        "# Step 1: Eliminate Leading or Trailing Whitespaces\n",
        "df_initial['prompt'] = df_initial['prompt'].str.strip()\n",
        "print('Rows (Step 1) :', len(df_initial))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BKS84GpuIJ8"
      },
      "outputs": [],
      "source": [
        "# Count the number of unique values in the 'user_name' column\n",
        "user_counts = df_initial['user_name'].value_counts()\n",
        "unique_user_names_count = df_initial['user_name'].nunique()\n",
        "print('Unique Users (without deleted_user):', unique_user_names_count)\n",
        "\n",
        "# Filter for users with at least 100 rows\n",
        "users_more_than_250_prompts = user_counts[user_counts >= 120]\n",
        "users_with_100_prompts = user_counts[user_counts >= 120].index\n",
        "\n",
        "# Get the number of unique users with at least 250 rows\n",
        "num_users_more_than_250_prompts = len(users_more_than_250_prompts)\n",
        "\n",
        "# Print the result\n",
        "print(f\"Number of users with at least 100 prompts: {num_users_more_than_250_prompts}\")\n",
        "\n",
        "df_at_least_250_prompts_per_user = df_initial[df_initial['user_name'].isin(users_with_100_prompts)]\n",
        "\n",
        "# # Create a DataFrame to store 250 prompts for each user\n",
        "# df_at_least_250_prompts_per_user = pd.DataFrame()\n",
        "\n",
        "# # Loop through each user and sample 250 prompts\n",
        "# for user in users_more_than_250_prompts.index:\n",
        "# #    user_prompts = df_initial[df_initial['user_name'] == user].sample(250, random_state=42) wrong, because it selects only 250 from the beginning\n",
        "#     user_prompts = df_initial[df_initial['user_name'] == user]\n",
        "#     df_at_least_250_prompts_per_user = pd.concat([df_at_least_250_prompts_per_user, user_prompts])\n",
        "\n",
        "# df_at_least_250_prompts_per_user"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdSI0M3JzbNT"
      },
      "outputs": [],
      "source": [
        "# df_at_least_250_prompts_per_user.to_csv('/content/drive/MyDrive/authorship_inference_attack/df_real_data 1.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "735APIwouK32"
      },
      "outputs": [],
      "source": [
        "def get_random_100_users(number_of_users):\n",
        "    # Randomly select X distinct users\n",
        "    X_users_number = np.random.choice(users_more_than_250_prompts.index, size = number_of_users, replace=False)\n",
        "\n",
        "    # Create a DataFrame with samples from the randomly selected users # WRONG!!!\n",
        "    #df_100_random_users_number = df_at_least_250_prompts_per_user[df_at_least_250_prompts_per_user['user_name'].isin(X_users_number)]\n",
        "\n",
        "    # Create an empty DataFrame to store the selected prompts\n",
        "    df_100_random_users_number = pd.DataFrame()\n",
        "\n",
        "    # Iterate over selected users\n",
        "    for user in X_users_number:\n",
        "        # Get all prompts for the current user\n",
        "        user_prompts = df_at_least_250_prompts_per_user[df_at_least_250_prompts_per_user['user_name'] == user]\n",
        "\n",
        "        # If the user has more than 250 prompts, select 250 randomly\n",
        "        if len(user_prompts) >= 120: # 250:\n",
        "            user_prompts = user_prompts.sample(n=100, random_state=42, replace = False)  # Select 250 prompts randomly\n",
        "\n",
        "        # Append selected prompts to the DataFrame\n",
        "        df_100_random_users_number = df_100_random_users_number._append(user_prompts)\n",
        "        # df_100_random_users_number = pd.concat([df_100_random_users_number, user_prompts], ignore_index=True)\n",
        "\n",
        "    # Reset index of the DataFrame\n",
        "    df_100_random_users_number.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    # Count the number of unique values in the 'user_name' column\n",
        "    user_counts = df_100_random_users_number['user_name'].value_counts()\n",
        "\n",
        "    print('Final Number of users (that we will use) with at least 100 prompts :', len(user_counts))\n",
        "    print('Length of DF :', len(df_100_random_users_number))\n",
        "    print('Result (proof) :', number_of_users, ' * 250 = ', number_of_users * 250, '\\n')\n",
        "\n",
        "    return df_100_random_users_number"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faNgjcFxBoeu"
      },
      "outputs": [],
      "source": [
        "number_of_users = 200\n",
        "df_100_random_users = get_random_100_users(number_of_users)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "\n",
        "# Step 2: Initialize and fit the LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "df_100_random_users['user_name'] = label_encoder.fit_transform(df_100_random_users['user_name'])\n",
        "\n",
        "# Step 3: Save the LabelEncoder for later use\n",
        "with open('/content/drive/MyDrive/msc_project/data/diffusiondb/vary/label_encoder_100_200.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "\n",
        "# Step 4: Display the encoded DataFrame\n",
        "print(df_100_random_users)\n",
        "\n",
        "train_list = []\n",
        "val_list = []\n",
        "test_list = []\n",
        "\n",
        "# Loop over each author and split their data\n",
        "for author in df_100_random_users['user_name'].unique():\n",
        "    # Filter data for the current author\n",
        "    author_data = df_100_random_users[df_100_random_users['user_name'] == author]\n",
        "\n",
        "    # Split the author's data into train (60%) and temp (40%)\n",
        "    train_data, temp_data = train_test_split(author_data, test_size=0.4, random_state=42)\n",
        "\n",
        "    # Further split temp data into validation (20%) and test (20%)\n",
        "    val_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Append the splits to the respective lists\n",
        "    train_list.append(train_data)\n",
        "    val_list.append(val_data)\n",
        "    test_list.append(test_data)\n",
        "\n",
        "# Concatenate all splits into final train, validation, and test DataFrames\n",
        "train_df = pd.concat(train_list).reset_index(drop=True)\n",
        "val_df = pd.concat(val_list).reset_index(drop=True)\n",
        "test_df = pd.concat(test_list).reset_index(drop=True)\n",
        "\n",
        "# Now you can save the three datasets to CSV or any other format you prefer\n",
        "train_df.to_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/vary/train_random100_200_label_1.csv', index=False)\n",
        "val_df.to_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/vary/val_random100_200_label_1.csv', index=False)\n",
        "test_df.to_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random100_200_label_1.csv', index=False)\n",
        "\n",
        "print(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")\n"
      ],
      "metadata": {
        "id": "njlxNldXjTE6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jMoarQL3SIc"
      },
      "source": [
        "## IMDB62"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0aKsf67K3USZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u24wni9o6CT_"
      },
      "source": [
        "# model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pFuJk-Fk8W5"
      },
      "outputs": [],
      "source": [
        "class BertClassifier(nn.Module):\n",
        "  def __init__(self, pretrained_model, num_classes):\n",
        "    super(BertClassifier, self).__init__()\n",
        "    self.bert = BertModel.from_pretrained(pretrained_model)\n",
        "    self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
        "\n",
        "    # for param in self.bert.parameters():\n",
        "    #   param.requires_grad = False\n",
        "\n",
        "  def forward(self, input_ids, attention_mask):\n",
        "    outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "    embedding = outputs[1]\n",
        "    logits = self.classifier(embedding)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlTKd6IFDJUO"
      },
      "outputs": [],
      "source": [
        "def get_model(model_path, num_labels, use_cuda=False, cuda_device=0, only_train_classifier=False):\n",
        "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=num_labels)\n",
        "    if only_train_classifier:\n",
        "        for param in model.bert.parameters():\n",
        "            param.requires_grad = False\n",
        "    print(model)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYiwwreh93u9"
      },
      "source": [
        "# training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A946AgSXiLEL"
      },
      "outputs": [],
      "source": [
        "def aa_metrics(labels, predictions, raw_outputs=[], prefix='', no_auc=False, special=False):\n",
        "\n",
        "    accuracy = metrics.accuracy_score(labels, predictions)\n",
        "    macro_accuracy = metrics.balanced_accuracy_score(labels, predictions)\n",
        "    results = {\n",
        "      f'{prefix}accuracy': accuracy,\n",
        "      f'{prefix}macro_accuracy': macro_accuracy,\n",
        "    }\n",
        "\n",
        "    if len(raw_outputs) != 0:\n",
        "      top5_accuracy = 0\n",
        "      for i, prediction in enumerate(raw_outputs):\n",
        "          true_author = labels[i]\n",
        "          top5_indices = prediction.argsort()[-5:][::-1]  # Top-5 predicted indices\n",
        "          if true_author in top5_indices:\n",
        "              top5_accuracy += 1\n",
        "      top5_accuracy /= len(labels)\n",
        "      results.update({\n",
        "        f'{prefix}top5_accuracy': top5_accuracy\n",
        "      })\n",
        "\n",
        "    if special:\n",
        "        return results\n",
        "\n",
        "    micro_recall = metrics.recall_score(labels, predictions, average='micro')\n",
        "    macro_recall = metrics.recall_score(labels, predictions, average='macro')\n",
        "    micro_precision = metrics.precision_score(labels, predictions, average='micro')\n",
        "    macro_precision = metrics.precision_score(labels, predictions, average='macro')\n",
        "\n",
        "    # Calculate micro and macro F1 scores\n",
        "    micro_f1 = metrics.f1_score(labels, predictions, average='micro')\n",
        "    macro_f1 = metrics.f1_score(labels, predictions, average='macro')\n",
        "\n",
        "    results.update({\n",
        "        f'{prefix}micro_recall': micro_recall,\n",
        "        f'{prefix}macro_recall': macro_recall,\n",
        "        f'{prefix}micro_precision': micro_precision,\n",
        "        f'{prefix}macro_precision': macro_precision,\n",
        "        f'{prefix}micro_f1': micro_f1,\n",
        "        f'{prefix}macro_f1': macro_f1,\n",
        "    })\n",
        "\n",
        "\n",
        "    if not no_auc:\n",
        "        ovr_weighted_auc = metrics.roc_auc_score(labels, raw_outputs, average='weighted', multi_class='ovr')\n",
        "        ovr_macro_auc = metrics.roc_auc_score(labels, raw_outputs, average='macro', multi_class='ovr')\n",
        "        ovo_weighted_auc = metrics.roc_auc_score(labels, raw_outputs, average='weighted', multi_class='ovo')\n",
        "        ovo_macro_auc = metrics.roc_auc_score(labels, raw_outputs, average='macro', multi_class='ovo')\n",
        "        top2 = metrics.top_k_accuracy_score(labels, raw_outputs, k=2)\n",
        "        top3 = metrics.top_k_accuracy_score(labels, raw_outputs, k=3)\n",
        "        top4 = metrics.top_k_accuracy_score(labels, raw_outputs, k=4)\n",
        "        top5 = metrics.top_k_accuracy_score(labels, raw_outputs, k=5)\n",
        "        top6 = metrics.top_k_accuracy_score(labels, raw_outputs, k=6)\n",
        "        top7 = metrics.top_k_accuracy_score(labels, raw_outputs, k=7)\n",
        "        top8 = metrics.top_k_accuracy_score(labels, raw_outputs, k=8)\n",
        "        top9 = metrics.top_k_accuracy_score(labels, raw_outputs, k=9)\n",
        "        top10 = metrics.top_k_accuracy_score(labels, raw_outputs, k=10)\n",
        "        micro_f1 = metrics.f1_score(labels, predictions, average=\"micro\")\n",
        "        macro_f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n",
        "\n",
        "        results.update({\n",
        "            f'{prefix}ovr_weighted_auc': ovr_weighted_auc,\n",
        "            f'{prefix}ovr_macro_auc': ovr_macro_auc,\n",
        "            f'{prefix}ovo_weighted_auc': ovo_weighted_auc,\n",
        "            f'{prefix}ovo_macro_auc': ovo_macro_auc,\n",
        "            f'{prefix}micro_f1': micro_f1,\n",
        "            f'{prefix}macro_f1': macro_f1,\n",
        "            f'{prefix}top2': top2,\n",
        "            f'{prefix}top3': top3,\n",
        "            f'{prefix}top4': top4,\n",
        "            f'{prefix}top5': top5,\n",
        "            f'{prefix}top6': top6,\n",
        "            f'{prefix}top7': top7,\n",
        "            f'{prefix}top8': top8,\n",
        "            f'{prefix}top9': top9,\n",
        "            f'{prefix}top10': top10\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n",
        "def chunk_text(text, tokenizer, max_len=512):\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=True)\n",
        "    return [tokens[i:i + max_len] for i in range(0, len(tokens), max_len)]\n",
        "\n",
        "def majority_vote(predictions):\n",
        "    vote_count = Counter(predictions)\n",
        "    return vote_count.most_common(1)[0][0]\n",
        "\n",
        "def evaluate_with_chunking(text, model, tokenizer, max_len=512):\n",
        "    chunks = chunk_text(text, tokenizer, max_len)\n",
        "    predictions = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        input_ids = torch.tensor([chunk]).to(model.device)\n",
        "        attention_mask = torch.tensor([[1] * len(chunk)]).to(model.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            prediction = torch.argmax(logits, dim=1).item()\n",
        "            predictions.append(prediction)\n",
        "\n",
        "    final_prediction = majority_vote(predictions)\n",
        "    return final_prediction\n",
        "\n",
        "\n",
        "def evaluate_chunk(model, dataloader, tokenizer, device):\n",
        "  model.eval()\n",
        "  predictions, true_labels = [], []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "      texts = batch['text']  # Assuming dataloader yields batches with 'text' and 'labels'\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      batch_predictions = []\n",
        "      for text in texts:\n",
        "        prediction = evaluate_with_chunking(text, model, tokenizer)\n",
        "        batch_predictions.append(prediction)\n",
        "\n",
        "      predictions.extend(batch_predictions)\n",
        "      true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  accuracy = metrics.accuracy_score\n",
        "  macro_accuracy = metrics.balanced_accuracy_score\n",
        "  micro_recall = partial(metrics.recall_score, average='micro')\n",
        "  macro_recall = partial(metrics.recall_score, average='macro')\n",
        "  micro_precision = partial(metrics.precision_score, average='micro')\n",
        "  macro_precision = partial(metrics.precision_score, average='macro')\n",
        "  micro_f1 = partial(metrics.f1_score, average='micro')\n",
        "  macro_f1 = partial(metrics.f1_score, average='macro')\n",
        "\n",
        "  eval_results = {\n",
        "    'accuracy': accuracy(true_labels, predictions),\n",
        "    'macro_accuracy': macro_accuracy(true_labels, predictions),\n",
        "    'micro_recall': micro_recall(true_labels, predictions),\n",
        "    'macro_recall': macro_recall(true_labels, predictions),\n",
        "    'micro_precision': micro_precision(true_labels, predictions),\n",
        "    'macro_precision': macro_precision(true_labels, predictions),\n",
        "    'micro_f1': micro_f1(true_labels, predictions),\n",
        "    'macro_f1': macro_f1(true_labels, predictions)\n",
        "  }\n",
        "\n",
        "  return eval_results\n",
        "\n",
        "\n",
        "def test_chunk(model, dataloader, tokenizer, device):\n",
        "  model.eval()\n",
        "  predictions, true_labels = [], []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in dataloader:\n",
        "      texts = batch['text']  # Assuming dataloader yields batches with 'text' and 'labels'\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      batch_predictions = []\n",
        "      for text in texts:\n",
        "        prediction = evaluate_with_chunking(text, model, tokenizer)\n",
        "        batch_predictions.append(prediction)\n",
        "\n",
        "      predictions.extend(batch_predictions)\n",
        "      true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "  test_results = aa_metrics(true_labels, predictions, prefix='test_chunk/', no_auc=True)\n",
        "\n",
        "  return test_results\n",
        "\n",
        "\n",
        "def evaluate(model, val_loader, device):\n",
        "  model.eval()\n",
        "\n",
        "  predictions = []\n",
        "  true_labels = []\n",
        "  total_loss = 0\n",
        "  with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "      loss = outputs.loss\n",
        "\n",
        "      logits = outputs.logits.detach().cpu().numpy()\n",
        "      label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "      predictions.extend(np.argmax(logits, axis=1).flatten())\n",
        "      true_labels.extend(label_ids.flatten())\n",
        "\n",
        "      total_loss += loss.item()\n",
        "\n",
        "  accuracy = metrics.accuracy_score\n",
        "  macro_accuracy = metrics.balanced_accuracy_score\n",
        "  micro_recall = partial(metrics.recall_score, average='micro')\n",
        "  macro_recall = partial(metrics.recall_score, average='macro')\n",
        "  micro_precision = partial(metrics.precision_score, average='micro')\n",
        "  macro_precision = partial(metrics.precision_score, average='macro')\n",
        "  micro_f1 = partial(metrics.f1_score, average='micro')\n",
        "  macro_f1 = partial(metrics.f1_score, average='macro')\n",
        "\n",
        "  eval_results = {\n",
        "    'eval_loss': total_loss / len(val_loader),\n",
        "    'accuracy': accuracy(true_labels, predictions),\n",
        "    'macro_accuracy': macro_accuracy(true_labels, predictions),\n",
        "    'micro_recall': micro_recall(true_labels, predictions),\n",
        "    'macro_recall': macro_recall(true_labels, predictions),\n",
        "    'micro_precision': micro_precision(true_labels, predictions),\n",
        "    'macro_precision': macro_precision(true_labels, predictions),\n",
        "    'micro_f1': micro_f1(true_labels, predictions),\n",
        "    'macro_f1': macro_f1(true_labels, predictions)\n",
        "  }\n",
        "\n",
        "  return eval_results\n",
        "\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "  model.eval()\n",
        "  predictions, true_labels = [], []\n",
        "  predictions_1 = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "      input_ids = batch['input_ids'].to(device)\n",
        "      attention_mask = batch['attention_mask'].to(device)\n",
        "      labels = batch['labels'].to(device)\n",
        "\n",
        "      outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "      logits = outputs.logits.detach().cpu().numpy()\n",
        "      label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "      predictions.extend(np.argmax(logits, axis=1).flatten())\n",
        "      true_labels.extend(label_ids.flatten())\n",
        "      predictions_1.extend(logits)\n",
        "\n",
        "  print(predictions)\n",
        "  print(true_labels)\n",
        "  probabilities = F.softmax(torch.tensor(predictions_1), dim=1).numpy()\n",
        "  test_results = aa_metrics(true_labels, predictions, probabilities, prefix='test/', no_auc=False)\n",
        "\n",
        "  return test_results\n",
        "\n",
        "\n",
        "def train(model, train_loader, optimizer, scheduler, device):\n",
        "\n",
        "  model.train()\n",
        "  total_loss = 0\n",
        "\n",
        "  predictions, true_labels = [], []\n",
        "  for batch in tqdm.tqdm(train_loader, total=len(train_loader)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    input_ids = batch['input_ids'].to(device)\n",
        "    attention_mask = batch['attention_mask'].to(device)\n",
        "    labels = batch['labels'].to(device)\n",
        "\n",
        "    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "    loss = outputs.loss\n",
        "    logits = outputs.logits.detach().cpu().numpy()\n",
        "    label_ids = labels.to('cpu').numpy()\n",
        "\n",
        "    predictions.extend(np.argmax(logits, axis=1).flatten())\n",
        "    true_labels.extend(label_ids.flatten())\n",
        "\n",
        "    total_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "  accuracy = accuracy_score(true_labels, predictions)\n",
        "\n",
        "  return total_loss / len(train_loader), accuracy\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sz6qn129-pC"
      },
      "outputs": [],
      "source": [
        "def run_bertaa(train_df, val_df, test_df, params):\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained(params['model_path'])\n",
        "\n",
        "  if len(val_df) == 0 :\n",
        "    # train_dataset = preprocess_data(train_df, tokenizer, params['max_seq_len'], encoder_path=params['encoder_path'], train=True)\n",
        "    # test_dataset = preprocess_data(test_df, tokenizer, params['max_seq_len'], encoder_path=params['encoder_path'])\n",
        "    label_encoder = joblib.load(params['encoder_path'])\n",
        "    train_labels = label_encoder.transform(train_df['user_name'])\n",
        "    test_labels = label_encoder.transform(test_df['user_name'])\n",
        "\n",
        "    train_dataset = TextDataset(train_df['prompt'], train_labels, tokenizer, params['max_seq_len'])\n",
        "    test_dataset = TextDataset(test_df['prompt'], test_labels, tokenizer, params['max_seq_len'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "    model = get_model(params['model_path'], params['num_labels'], use_cuda=torch.cuda.is_available(), cuda_device=0, only_train_classifier=params['only_train_classifier'])\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "    total_steps = len(train_loader) * params['epochs']\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=params['warmup_steps'], num_training_steps=total_steps)\n",
        "\n",
        "    wandb.init(project=params['wandb_project'], config=params, reinit=True, name=params['wandb_name'], tags=eval(params['wandb_tags']))\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(params['epochs']), total=params['epochs']):\n",
        "      train_loss, train_acc = train(model, train_loader, optimizer, scheduler, device)\n",
        "      print(f\"Epoch {epoch}, Loss: {train_loss}, Accuracy: {train_acc}\")\n",
        "\n",
        "      wandb.log({\n",
        "          'epoch': epoch,\n",
        "          'train_loss': train_loss,\n",
        "          'train_acc' : train_acc,\n",
        "      })\n",
        "\n",
        "    save_path = params['output_dir'] + 'final_model'\n",
        "    model.save_pretrained(save_path)\n",
        "\n",
        "  else:\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    train_labels = label_encoder.fit_transform(train_df['user_name'])\n",
        "    joblib.dump(label_encoder, params['encoder_path'])\n",
        "    # label_encoder = joblib.load(params['encoder_path'])\n",
        "    # train_labels = label_encoder.transform(train_df['user_name'])\n",
        "    val_labels = label_encoder.transform(val_df['user_name'])\n",
        "    test_labels = label_encoder.transform(test_df['user_name'])\n",
        "\n",
        "    # train_dataset = preprocess_data(train_df, tokenizer, params['max_seq_len'], encoder_path=params['encoder_path'], train=True)\n",
        "    # val_dataset = preprocess_data(val_df, tokenizer, params['max_seq_len'], encoder_path=params['encoder_path'])\n",
        "    # test_dataset = preprocess_data(test_df, tokenizer, params['max_seq_len'], encoder_path=params['encoder_path'])\n",
        "\n",
        "    train_dataset = TextDataset(train_df['prompt'], train_labels, tokenizer, params['max_seq_len'])\n",
        "    val_dataset = TextDataset(val_df['prompt'], val_labels, tokenizer, params['max_seq_len'])\n",
        "    test_dataset = TextDataset(test_df['prompt'], test_labels, tokenizer, params['max_seq_len'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "    model = get_model(params['model_path'], params['num_labels'], use_cuda=torch.cuda.is_available(), cuda_device=0, only_train_classifier=params['only_train_classifier'])\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "    total_steps = len(train_loader) * params['epochs']\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=params['warmup_steps'], num_training_steps=total_steps)\n",
        "\n",
        "    wandb.init(project=params['wandb_project'], config=params, reinit=True, name=params['wandb_name'], tags=eval(params['wandb_tags']))\n",
        "    save_path = params['output_dir'] + params['wandb_name'].split('_')[0] + '/best_model'\n",
        "\n",
        "    best_score = None\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    for epoch in range(params['epochs']):\n",
        "      print(epoch)\n",
        "      train_loss, train_acc = train(model, train_loader, optimizer, scheduler, device)\n",
        "      if params['evaluate_chunk']:\n",
        "        eval_metrics = evaluate_chunk(model, val_loader, tokenizer, device)\n",
        "        print(f\"Epoch {epoch}, Loss: {train_loss}, Accuracy: {train_acc}, Evaluation Accuracy: {eval_metrics['accuracy']}, Evaluation Macro Accuracy: {eval_metrics['macro_accuracy']}\")\n",
        "      else:\n",
        "        eval_metrics = evaluate(model, val_loader, device)\n",
        "        print(f\"Epoch {epoch}, Loss: {train_loss}, Accuracy: {train_acc}, Evaluation Loss: {eval_metrics['eval_loss']}, Evaluation Accuracy: {eval_metrics['accuracy']}, Evaluation Macro Accuracy: {eval_metrics['macro_accuracy']}\")\n",
        "\n",
        "      wandb.log({\n",
        "          'epoch': epoch,\n",
        "          'train_loss': train_loss,\n",
        "          'train_acc' : train_acc,\n",
        "          **eval_metrics\n",
        "      })\n",
        "\n",
        "      current_score = eval_metrics['macro_accuracy']\n",
        "\n",
        "      # Check for improvement\n",
        "      if best_score is None or current_score > best_score + params['early_stopping_delta']:\n",
        "          best_score = current_score\n",
        "          epochs_no_improve = 0\n",
        "          # Optionally, save the model here as the best model so far\n",
        "          model.save_pretrained(save_path)\n",
        "      else:\n",
        "          epochs_no_improve += 1\n",
        "\n",
        "      if epochs_no_improve >= params['early_stopping_patience']:\n",
        "          print(f\"Early stopping at epoch {epoch + 1}\")\n",
        "          break\n",
        "\n",
        "    model = BertForSequenceClassification.from_pretrained(save_path)\n",
        "    model.to(device)\n",
        "\n",
        "  if params['evaluate_chunk']:\n",
        "    test_metrics = test_chunk(model, test_loader, tokenizer, device)\n",
        "  else:\n",
        "    test_metrics = test(model, test_loader, device)\n",
        "  wandb.log({'test_' + k: v for k, v in test_metrics.items()})\n",
        "  print(test_metrics)\n",
        "  wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_bertaa(train_df, val_df, test_df, params):\n",
        "\n",
        "  tokenizer = BertTokenizer.from_pretrained(params['model_path'])\n",
        "\n",
        "  if len(val_df) == 0 :\n",
        "    # train_dataset = preprocess_data(train_df, tokenizer, params['max_seq_len'], encoder_path=params['encoder_path'], train=True)\n",
        "    # test_dataset = preprocess_data(test_df, tokenizer, params['max_seq_len'], encoder_path=params['encoder_path'])\n",
        "    label_encoder = joblib.load(params['encoder_path'])\n",
        "    train_labels = label_encoder.transform(train_df['user_name'])\n",
        "    test_labels = label_encoder.transform(test_df['user_name'])\n",
        "\n",
        "    train_dataset = TextDataset(train_df['prompt'], train_labels, tokenizer, params['max_seq_len'])\n",
        "    test_dataset = TextDataset(test_df['prompt'], test_labels, tokenizer, params['max_seq_len'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "    model = get_model(params['model_path'], params['num_labels'], use_cuda=torch.cuda.is_available(), cuda_device=0, only_train_classifier=params['only_train_classifier'])\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])\n",
        "    total_steps = len(train_loader) * params['epochs']\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=params['warmup_steps'], num_training_steps=total_steps)\n",
        "\n",
        "    wandb.init(project=params['wandb_project'], config=params, reinit=True, name=params['wandb_name'], tags=eval(params['wandb_tags']))\n",
        "\n",
        "    for epoch in tqdm.tqdm(range(params['epochs']), total=params['epochs']):\n",
        "      train_loss, train_acc = train(model, train_loader, optimizer, scheduler, device)\n",
        "      print(f\"Epoch {epoch}, Loss: {train_loss}, Accuracy: {train_acc}\")\n",
        "\n",
        "      wandb.log({\n",
        "          'epoch': epoch,\n",
        "          'train_loss': train_loss,\n",
        "          'train_acc' : train_acc,\n",
        "      })\n",
        "\n",
        "    save_path = params['output_dir'] + 'final_model'\n",
        "    model.save_pretrained(save_path)\n",
        "\n",
        "  else:\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    train_labels = label_encoder.fit_transform(train_df['user_name'])\n",
        "    # joblib.dump(label_encoder, params['encoder_path'])\n",
        "    # label_encoder = joblib.load(params['encoder_path'])\n",
        "    # train_labels = label_encoder.transform(train_df['user_name'])\n",
        "    val_labels = label_encoder.transform(val_df['user_name'])\n",
        "    test_labels = label_encoder.transform(test_df['user_name'])\n",
        "\n",
        "    # train_dataset = preprocess_data(train_df, tokenizer, params['max_seq_len'], encoder_path=params['encoder_path'], train=True)\n",
        "    # val_dataset = preprocess_data(val_df, tokenizer, params['max_seq_len'], encoder_path=params['encoder_path'])\n",
        "    # test_dataset = preprocess_data(test_df, tokenizer, params['max_seq_len'], encoder_path=params['encoder_path'])\n",
        "\n",
        "    train_dataset = TextDataset(train_df['prompt'], train_labels, tokenizer, params['max_seq_len'])\n",
        "    val_dataset = TextDataset(val_df['prompt'], val_labels, tokenizer, params['max_seq_len'])\n",
        "    test_dataset = TextDataset(test_df['prompt'], test_labels, tokenizer, params['max_seq_len'])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=params['batch_size'], shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=params['batch_size'], shuffle=False)\n",
        "\n",
        "    wandb.init(project=params['wandb_project'], config=params, reinit=True, name=params['wandb_name'], tags=eval(params['wandb_tags']))\n",
        "    # model = get_model(params['model_path'], params['num_labels'], use_cuda=torch.cuda.is_available(), cuda_device=0, only_train_classifier=params['only_train_classifier'])\n",
        "    save_path = params['output_dir'] + params['wandb_name'].split('_')[0] + '/best_model'\n",
        "    model = BertForSequenceClassification.from_pretrained(save_path)\n",
        "    model.to(device)\n",
        "\n",
        "  if params['evaluate_chunk']:\n",
        "    test_metrics = test_chunk(model, test_loader, tokenizer, device)\n",
        "  else:\n",
        "    test_metrics = test(model, test_loader, device)\n",
        "  wandb.log({'test_' + k: v for k, v in test_metrics.items()})\n",
        "  print(test_metrics)\n",
        "  wandb.finish()"
      ],
      "metadata": {
        "id": "ylzd06igOfY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayo1r4bB17Sp"
      },
      "source": [
        "# run"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFjVzSU518yi"
      },
      "source": [
        "## IMDB62"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubkMjJ3qA9IQ"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'imdb62_1',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_train.csv',\n",
        "      'val_dataset': '/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_val.csv',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_test.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/label_encoder.pkl',\n",
        "      'evaluate_chunk': False,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ahb7lBbTLcV6"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'imdb62_2',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_train.csv',\n",
        "      'val_dataset': '/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_val.csv',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_test.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBmok-bK49BW"
      },
      "source": [
        "## blogs50"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oda90CgG5BF-"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'blogs50_1',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/blogs50/processed/blogs50_train.csv',\n",
        "      'val_dataset': '/content/drive/MyDrive/msc_project/data/blogs50/processed/blogs50_AA_val.csv',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/blogs50/processed/blogs50_AA_test.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/blogs50/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    test_bertaa(train_df, val_df, test_df, training_args)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjrpijk_nyc8"
      },
      "source": [
        "## diffusiondb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "7P7QBxgZnxwd"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'diffusiondb_1',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/processed/train_random100_1.csv',\n",
        "      'val_dataset': '',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_1.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/diffusiondb/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_data = pd.read_csv(training_args['train_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "    train_data.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.DataFrame()\n",
        "    val_df = pd.DataFrame()\n",
        "\n",
        "    for author in train_data['user_name'].unique():\n",
        "        author_df = train_data[train_data['user_name'] == author]\n",
        "        train1, val1 = train_test_split(author_df, test_size=0.25)\n",
        "        train_df = pd.concat([train_df, train1])\n",
        "        val_df = pd.concat([val_df, val1])\n",
        "\n",
        "    train_df.reset_index(drop=True, inplace=True)\n",
        "    val_df.reset_index(drop=True, inplace=True)\n",
        "    # train_df = train_data\n",
        "    # val_df = val_data\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'diffusiondb_1',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/processed/train_random100_1.csv',\n",
        "      'val_dataset': '',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_1.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/diffusiondb/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_data = pd.read_csv(training_args['train_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "    train_data.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.DataFrame()\n",
        "    val_df = pd.DataFrame()\n",
        "\n",
        "    for author in train_data['user_name'].unique():\n",
        "        author_df = train_data[train_data['user_name'] == author]\n",
        "        train1, val1 = train_test_split(author_df, test_size=0.25)\n",
        "        train_df = pd.concat([train_df, train1])\n",
        "        val_df = pd.concat([val_df, val1])\n",
        "\n",
        "    train_df.reset_index(drop=True, inplace=True)\n",
        "    val_df.reset_index(drop=True, inplace=True)\n",
        "    # train_df = train_data\n",
        "    # val_df = val_data\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    test_bertaa(train_df, val_df, test_df, training_args)\n"
      ],
      "metadata": {
        "id": "8--lrvRcSGV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'diffusiondb_1',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/train_random100_1.csv',\n",
        "      'val_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/val_random100_1.csv',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_1.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/diffusiondb/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "    print(train_df.columns)\n",
        "    train_df = train_df[['user_name', 'prompt']]\n",
        "    test_df = test_df[['user_name', 'prompt']]\n",
        "    val_df = val_df[['user_name', 'prompt']]\n",
        "\n",
        "    # train_df = pd.DataFrame()\n",
        "    # val_df = pd.DataFrame()\n",
        "\n",
        "    # for author in train_data['user_name'].unique():\n",
        "    #     author_df = train_data[train_data['user_name'] == author]\n",
        "    #     train1, val1 = train_test_split(author_df, test_size=0.25)\n",
        "    #     train_df = pd.concat([train_df, train1])\n",
        "    #     val_df = pd.concat([val_df, val1])\n",
        "\n",
        "    # train_df.reset_index(drop=True, inplace=True)\n",
        "    # val_df.reset_index(drop=True, inplace=True)\n",
        "    # train_df = train_data\n",
        "    # val_df = val_data\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ],
      "metadata": {
        "id": "AWG1SSfizKKY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# diffusiondb_clean"
      ],
      "metadata": {
        "id": "zdPH7LLtxd-z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'diffusiondb_clean',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/clean/train_random100_label_1.csv',\n",
        "      'val_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/clean/val_random100_label_1.csv',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/clean/test_random100_label_1.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/diffusiondb/clean/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "    print(train_df.columns)\n",
        "    train_df = train_df[['user_name', 'prompt']]\n",
        "    test_df = test_df[['user_name', 'prompt']]\n",
        "    val_df = val_df[['user_name', 'prompt']]\n",
        "    # train_df = train_data\n",
        "    # val_df = val_data\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ],
      "metadata": {
        "id": "y03n2rk_xf-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# diffusiondb_vary"
      ],
      "metadata": {
        "id": "Cl1DYRMCnDFQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'diffusiondb_vary_60',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/train_random60_label_1.csv',\n",
        "      'val_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/val_random60_label_1.csv',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random60_label_1.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/diffusiondb/clean/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "    print(train_df.columns)\n",
        "    train_df = train_df[['user_name', 'prompt']]\n",
        "    test_df = test_df[['user_name', 'prompt']]\n",
        "    val_df = val_df[['user_name', 'prompt']]\n",
        "    # train_df = train_data\n",
        "    # val_df = val_data\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ],
      "metadata": {
        "id": "xFp6Sy9lnGqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'diffusiondb_vary_80',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/train_random80_label_1.csv',\n",
        "      'val_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/val_random80_label_1.csv',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random80_label_1.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/diffusiondb/clean/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "    print(train_df.columns)\n",
        "    train_df = train_df[['user_name', 'prompt']]\n",
        "    test_df = test_df[['user_name', 'prompt']]\n",
        "    val_df = val_df[['user_name', 'prompt']]\n",
        "    # train_df = train_data\n",
        "    # val_df = val_data\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ],
      "metadata": {
        "id": "9Re7pi5wtfSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'diffusiondb_vary_120',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/train_random120_label_1.csv',\n",
        "      'val_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/val_random120_label_1.csv',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random120_label_1.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/diffusiondb/clean/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "    print(train_df.columns)\n",
        "    train_df = train_df[['user_name', 'prompt']]\n",
        "    test_df = test_df[['user_name', 'prompt']]\n",
        "    val_df = val_df[['user_name', 'prompt']]\n",
        "    # train_df = train_data\n",
        "    # val_df = val_data\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ],
      "metadata": {
        "id": "MG2kRhRo0ZI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'diffusiondb_vary_100_150',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/train_random100_150_label_1.csv',\n",
        "      'val_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/val_random100_150_label_1.csv',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random100_150_label_1.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/diffusiondb/clean/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "    print(train_df.columns)\n",
        "    train_df = train_df[['user_name', 'prompt']]\n",
        "    test_df = test_df[['user_name', 'prompt']]\n",
        "    val_df = val_df[['user_name', 'prompt']]\n",
        "    # train_df = train_data\n",
        "    # val_df = val_data\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ],
      "metadata": {
        "id": "OQwwg161_EHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'diffusiondb_vary_100_200',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/train_random100_200_label_1.csv',\n",
        "      'val_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/val_random100_200_label_1.csv',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random100_200_label_1.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/diffusiondb/clean/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "    print(train_df.columns)\n",
        "    train_df = train_df[['user_name', 'prompt']]\n",
        "    test_df = test_df[['user_name', 'prompt']]\n",
        "    val_df = val_df[['user_name', 'prompt']]\n",
        "    # train_df = train_data\n",
        "    # val_df = val_data\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ],
      "metadata": {
        "id": "TdJNLfchKoMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## twitter_micro"
      ],
      "metadata": {
        "id": "5gg87nW4HUvq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'twitter_micro_1',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/twitter_micro/processed/train_random100_1.csv',\n",
        "      'val_dataset': '',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/twitter_micro/processed/test_random100_1.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/twitter_micro/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_data = pd.read_csv(training_args['train_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "    train_data.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.DataFrame()\n",
        "    val_df = pd.DataFrame()\n",
        "\n",
        "    for author in train_data['user_name'].unique():\n",
        "        author_df = train_data[train_data['user_name'] == author]\n",
        "        train1, val1 = train_test_split(author_df, test_size=0.25)\n",
        "        train_df = pd.concat([train_df, train1])\n",
        "        val_df = pd.concat([val_df, val1])\n",
        "\n",
        "    train_df.reset_index(drop=True, inplace=True)\n",
        "    val_df.reset_index(drop=True, inplace=True)\n",
        "    # train_df = train_data\n",
        "    # val_df = val_data\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ],
      "metadata": {
        "id": "iftDiVtzHXmM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  training_args = {\n",
        "      'wandb_project': 'bertaa',\n",
        "      'wandb_name': 'twitter_micro_1',\n",
        "      'wandb_tags': '[\"baseline\"]',\n",
        "      'train_dataset': '/content/drive/MyDrive/msc_project/data/twitter_micro/processed/train_random100_1.csv',\n",
        "      'val_dataset': '/content/drive/MyDrive/msc_project/data/twitter_micro/processed/val_random100_1.csv',\n",
        "      'test_dataset': '/content/drive/MyDrive/msc_project/data/twitter_micro/processed/test_random100_1.csv',\n",
        "      'model_path': 'bert-base-cased',\n",
        "      'epochs': 20,\n",
        "      'batch_size': 16,\n",
        "      'lr': 3e-5,\n",
        "      'warmup_ratio': 0.15,\n",
        "      'warmup_steps': 0,\n",
        "      'weight_decay': 1e-5,\n",
        "      'max_seq_len': 512,\n",
        "      'doc_stride': 0.8,\n",
        "      'output_dir': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/',\n",
        "      'early_stopping_delta': 0.01,\n",
        "      'early_stopping_patience': 5,\n",
        "      'final_run': False,\n",
        "      'only_train_classifier': False,\n",
        "      'encoder_path': '/content/drive/MyDrive/msc_project/model/baseline/BertAA/twitter_micro/label_encoder.pkl',\n",
        "      'evaluate_chunk': True,\n",
        "  }\n",
        "\n",
        "  if training_args['final_run']:\n",
        "    train_df = pd.read_csv(training_args['train_dataset'])\n",
        "    val_df = pd.read_csv(training_args['val_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "\n",
        "    train_df.columns = ['user_name', 'prompt']\n",
        "    val_df.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.concat([train_df, val_df], ignore_index=True)\n",
        "    val_df = pd.Dataframe()\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, test_df, training_args)\n",
        "\n",
        "  else:\n",
        "    train_data = pd.read_csv(training_args['train_dataset'])\n",
        "    test_df = pd.read_csv(training_args['test_dataset'])\n",
        "    train_data.columns = ['user_name', 'prompt']\n",
        "    test_df.columns = ['user_name', 'prompt']\n",
        "\n",
        "    train_df = pd.DataFrame()\n",
        "    val_df = pd.DataFrame()\n",
        "\n",
        "    for author in train_data['user_name'].unique():\n",
        "        author_df = train_data[train_data['user_name'] == author]\n",
        "        train1, val1 = train_test_split(author_df, test_size=0.25)\n",
        "        train_df = pd.concat([train_df, train1])\n",
        "        val_df = pd.concat([val_df, val1])\n",
        "\n",
        "    train_df.reset_index(drop=True, inplace=True)\n",
        "    val_df.reset_index(drop=True, inplace=True)\n",
        "    # train_df = train_data\n",
        "    # val_df = val_data\n",
        "\n",
        "    # def sample_prompts(group, n=1):\n",
        "    #   return group.sample(n=min(n, len(group)), random_state=42)\n",
        "\n",
        "    # train_df = train_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # val_df = val_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "    # test_df = test_df.groupby('user_name').apply(sample_prompts).reset_index(drop=True)\n",
        "\n",
        "    training_args['num_labels'] = len(train_df['user_name'].drop_duplicates())\n",
        "\n",
        "    run_bertaa(train_df, val_df, test_df, training_args)\n"
      ],
      "metadata": {
        "id": "9CRw80mRSsxC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZVwdTkC5ByQ"
      },
      "source": [
        "## test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TQrhAL94mpP"
      },
      "outputs": [],
      "source": [
        "def training_BERT_varying(df_100_random_users, sample, epochs, number_of_users):\n",
        "\n",
        "\n",
        "  pretrained_model = 'bert-base-cased'\n",
        "  batch_size = 128\n",
        "  # learning_rate = 0.001\n",
        "  learning_rate = 2e-5\n",
        "\n",
        "  # load tokenizer\n",
        "  tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
        "\n",
        "  print('Size of DF :', len(df_100_random_users))\n",
        "  total_rows = len(df_100_random_users)\n",
        "  desired_train_size = sample * 100\n",
        "\n",
        "  print('Training Samples :', sample)\n",
        "\n",
        "  # Calculate the percentage of the DataFrame to be used for training\n",
        "  train_size_percent = desired_train_size / total_rows\n",
        "\n",
        "  # Perform train-test split based on the calculated train size percentage\n",
        "  train_df, test_df = train_test_split(df_100_random_users, test_size=1-train_size_percent, random_state=42)\n",
        "  train_df, val_df = train_test_split(train_df, test_size=0.2, random_state=42)\n",
        "  train_df.to_csv('train_df.csv', index=False)\n",
        "  test_df.to_csv('test_df.csv', index=False)\n",
        "  val_df.to_csv('val_df.csv', index=False)\n",
        "\n",
        "  print('Train DF Size :', len(train_df))\n",
        "\n",
        "  print('Remaining DF Size :', len(df_100_random_users) - len(train_df))\n",
        "  # Randomly select 30% of the prompts from test_dfs for testing\n",
        "  #    test_df = pd.concat([df.sample(frac=0.3, random_state=42) for df in test_dfs])\n",
        "\n",
        "  print('Test DF Size :', len(test_df))\n",
        "  print('Check (proof) :', '30 \\%', 'of', len(df_100_random_users) - len(train_df), '=',  (len(df_100_random_users) - len(train_df)) * 0.3)\n",
        "  # print(train_df)\n",
        "\n",
        "  train_dataset, val_dataset, test_dataset = preprocess_data(train_df, val_df, test_df, tokenizer)\n",
        "  train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "  val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "  test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "  # model = BertForSequenceClassification.from_pretrained(\n",
        "  #     pretrained_model,\n",
        "  #     num_labels=100,\n",
        "  #     output_attentions=False,\n",
        "  #     output_hidden_states=False,\n",
        "  # )\n",
        "\n",
        "  model = BertClassifier(pretrained_model, number_of_users)\n",
        "  model = model.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  # total_steps = len(train_loader) * epochs\n",
        "  # scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "  #                                             num_warmup_steps=0, # Default value\n",
        "  #                                             num_training_steps=total_steps)\n",
        "\n",
        "  patience = 3\n",
        "  train(model, optimizer, criterion, epochs, patience, train_loader, val_loader)\n",
        "  torch.save(model, 'model.pth')\n",
        "  accuracy, top5_accuracy = test(model, test_loader)\n",
        "\n",
        "  return top5_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kayod7WM0Gbx"
      },
      "outputs": [],
      "source": [
        "# fine tune bert + linear - new experiments - T=50, number_of_users=100, 150, 200\n",
        "results_BERT = []\n",
        "#samples = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 120, 140, 160, 170, 175, 180, 200]\n",
        "samples = [70]\n",
        "times = [1, 2, 3, 4, 5]\n",
        "# times = [1]\n",
        "epochs = 30\n",
        "# epochs = 0\n",
        "\n",
        "number_of_users_all = [196]\n",
        "\n",
        "for number_of_users in number_of_users_all:\n",
        "  for time in times:\n",
        "    df_100_random_users = pd.DataFrame()\n",
        "    df_100_random_users = get_random_100_users(number_of_users)\n",
        "\n",
        "    i = 0\n",
        "    for sample in samples:\n",
        "      i += 1\n",
        "      print('Subrun :', i, '/', len(samples))\n",
        "\n",
        "      #### BERT ####\n",
        "      # try:\n",
        "      print(' - BERT - ')\n",
        "      top5_accuracy = training_BERT(df_100_random_users, sample, epochs, number_of_users)\n",
        "      results_BERT.append([sample, top5_accuracy, number_of_users])\n",
        "      print(results_BERT)\n",
        "      # except:\n",
        "      #   print('Error with BERT')\n",
        "      ##############\n",
        "\n",
        "      print('Run', time, 'finished!')\n",
        "      print('\\nInterim Results :\\n')\n",
        "      print('BERT_Results_left =', results_BERT, '\\n')\n",
        "\n",
        "  print('BERT_Results_left =', results_BERT, '\\n')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}