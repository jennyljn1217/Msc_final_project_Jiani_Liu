{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","collapsed_sections":["TcTBOj3qY4OE","NQaeB45gVUpo","bam6ToZ8oVul","H2bANzAqrxLq","Mcowp3wZBzE2"],"mount_file_id":"12v6IR2ISIiHnM1XMLCaUg2pqLez4kRFd","authorship_tag":"ABX9TyMILtqd+VByJwsRKH9bsYyK"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install wandb"],"metadata":{"id":"PV_CNlqBxycP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import wandb\n","wandb.login()"],"metadata":{"id":"szhtgcp1x19J"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvsM9EdSeE8V"},"outputs":[],"source":["\"\"\"\n","This file if a modified version of the one found here:\n","    https://github.com/pan-webis-de/muttenthaler19/blob/master/AuthorshipAttribution.ipynb\n","\"\"\"\n","import os\n","import pickle\n","import re\n","import json\n","import argparse\n","import time\n","import logging\n","import numpy as np\n","import wandb\n","import csv\n","import pandas as pd\n","from typing import List, Callable, Tuple, Union\n","import argparse\n","import copy\n","\n","from sklearn import preprocessing\n","from sklearn.calibration import CalibratedClassifierCV\n","from sklearn.decomposition import TruncatedSVD\n","# from sklearn.svm import SVC  # used in the original implementation but very slow on large datasets\n","from sklearn.svm import LinearSVC\n","from sklearn.multiclass import OneVsRestClassifier\n","from sklearn.linear_model import SGDClassifier, LogisticRegression\n","from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n","from sklearn.metrics import roc_auc_score, f1_score, brier_score_loss\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","# Get the root logger\n","logger = logging.getLogger()\n","logger.setLevel(logging.DEBUG)  # Set the logging level to DEBUG\n","\n","# Create a console handler\n","console_handler = logging.StreamHandler()\n","console_handler.setLevel(logging.DEBUG)"]},{"cell_type":"markdown","source":["# dataset"],"metadata":{"id":"U9AjsNhX2Jr8"}},{"cell_type":"code","source":["def aa_as_pandas(data: List[List[Union[int, str]]]) -> pd.DataFrame:\n","    return pd.DataFrame(data, columns=['labels', 'text'])\n","\n","\n","def av_as_pandas(data: List[List[Union[int, str]]]) -> pd.DataFrame:\n","    return pd.DataFrame(data, columns=['same/diff', 'text0', 'text1'])\n","\n","\n","# def get_aa_dataset(dataset_path: str) -> List[List[Union[int, str]]]:\n","#     data = []\n","#     with open(dataset_path, 'r', errors='ignore') as f:\n","#         reader = csv.reader(f)\n","#         for i, line in enumerate(reader):\n","#             if i > 0:  # skip header\n","#                 data.append([int(line[0]), str(line[1])])\n","#     return data\n","\n","\n","def get_aa_dataset(dataset_path: str) -> List[List[Union[int, str]]]:\n","    data = []\n","    with open(dataset_path, 'r', errors='ignore') as f:\n","        reader = csv.reader(f)\n","        for i, line in enumerate(reader):\n","            # print(line)\n","            if i > 0:  # skip header\n","              if len(line) != 0:\n","                data.append([int(line[9]), str(line[1])])\n","                # data.append([int(line[0]), str(line[1])])\n","                # data.append([int(line[1]), str(line[2])])\n","    return data\n","\n","\n","def get_av_dataset(dataset_path: str) -> List[List[Union[int, str, str]]]:\n","    data = []\n","    with open(dataset_path, 'r', errors='ignore') as f:\n","        reader = csv.reader(f)\n","        for i, line in enumerate(reader):\n","            if i > 0:  # skip header\n","                data.append([int(line[0]), str(line[1]), str(line[2])])\n","    return data\n","\n","\n","def get_aa_as_pandas(dataset_path: str) -> pd.DataFrame:\n","    return pd.read_csv(dataset_path, header=0, names=['labels', 'text'])\n","\n","\n","def get_av_as_pandas(dataset_path: str) -> pd.DataFrame:\n","    return pd.read_csv(dataset_path, names=['same/diff', 'text0', 'text1'])"],"metadata":{"id":"GIWk71x2ywwK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# evaluation"],"metadata":{"id":"SyXnMqeu2L45"}},{"cell_type":"code","source":["\n","#!/usr/bin/env python\n","# -*- coding: utf-8 -*-\n","\n","\"\"\"\n","# Evaluation script for the Cross-Domain Authorship Verification task @PAN2020.\n","\n","## Measures\n","The following evaluation measures are provided:\n","    - F1-score [Pedregosa et al. 2011]\n","    - Area-Under-the-Curve [Pedregosa et al. 2011]\n","    - c@1 [Pe単as and Rodrigo 2011; Stamatatos 2014]\n","    - f_05_u_score [Bevendorff et al. 2019]\n","    - the complement of the Brier score loss [Pedregosa et al. 2011]\n","\n","Systems will be evaluated, taking all of the measures into account.\n","\n","## Formats\n","The script requires two files, one for the ground truth (gold standard)\n","and one for the system predictions. These files should be formatted using\n","the `jsonl`-convention, whereby each line should contain a valid\n","json-string: e.g.\n","\n","``` json\n","    {\"id\": \"1\", \"value\": 0.123}\n","    {\"id\": \"2\", \"value\": 0.5}\n","    {\"id\": \"3\", \"value\": 0.888}\n","```\n","\n","Only files will be considered that:\n","- have the `.jsonl` extension\n","- are properly encoded as UTF-8.\n","\n","Please note:\n","    * For the c@1, all scores are will binarized using\n","      the conventional thresholds:\n","        * score < 0.5 -> 0\n","        * score > 0.5 -> 1\n","    * A score of *exactly* 0.5, will be considered a non-decision.\n","    * All problems which are present in the ground truth, but which\n","      are *not* provided an answer to by the system, will automatically\n","      be set to 0.5.\n","    * Non-answers are removed for the F1 score calculation below, but they\n","      are taken into account by the AUC and Brier score.\n","\n","## Dependencies:\n","- Python 3.6+ (we recommend the Anaconda Python distribution)\n","- scikit-learn\n","\n","## Usage\n","\n","From the command line:\n","\n",">>> python pan20-verif-evaluator.py -i COLLECTION -a ANSWERS -o OUTPUT\n","\n","where\n","    COLLECTION is the path to the file with the ground truth\n","    ANSWERS is the path to the answers file for a submitted method\n","    OUTPUT is the path to the folder where the results of the evaluation will be saved\n","\n","Example:\n","\n",">>> python pan20_verif_evaluator.py -i \"datasets/test_truth/truth.jsonl\" \\\n","        -a \"out/answers.jsonl\" \\\n","        -o \"pan20-evaluation\"\n","\n","## References\n","- E. Stamatatos, et al. Overview of the Author Identification\n","  Task at PAN 2014. CLEF Working Notes (2014): 877-897.\n","- Pedregosa, F. et al. Scikit-learn: Machine Learning in Python,\n","  Journal of Machine Learning Research 12 (2011), 2825--2830.\n","- A. Pe単as and A. Rodrigo. A Simple Measure to Assess Nonresponse.\n","  In Proc. of the 49th Annual Meeting of the Association for\n","  Computational Linguistics, Vol. 1, pages 1415-1424, 2011.\n","- Bevendorff et al. Generalizing Unmasking for Short Texts,\n","  Proceedings of NAACL (2019), 654-659.\n","\n","\"\"\"\n","\n","\n","def binarize(y, threshold=0.5, triple_valued=False):\n","    y = np.array(y)\n","    # y = np.ma.fix_invalid(y, fill_value=threshold)\n","    if triple_valued:\n","        y[y > threshold] = 1\n","    else:\n","        y[y >= threshold] = 1\n","    y[y < threshold] = 0\n","    return y\n","\n","\n","def auc(true_y, pred_y):\n","    \"\"\"\n","    Calculates the AUC score (Area Under the Curve), a well-known\n","    scalar evaluation score for binary classifiers. This score\n","    also considers \"unanswered\" problem, where score = 0.5.\n","\n","    Parameters\n","    ----------\n","    prediction_scores : array [n_problems]\n","\n","        The predictions outputted by a verification system.\n","        Assumes `0 >= prediction <=1`.\n","\n","    ground_truth_scores : array [n_problems]\n","\n","        The gold annotations provided for each problem.\n","        Will typically be `0` or `1`.\n","\n","    Returns\n","    ----------\n","    auc = the Area Under the Curve.\n","\n","    References\n","    ----------\n","        E. Stamatatos, et al. Overview of the Author Identification\n","        Task at PAN 2014. CLEF (Working Notes) 2014: 877-897.\n","\n","    \"\"\"\n","    try:\n","        return roc_auc_score(true_y, pred_y)\n","    except ValueError:\n","        return 0.0\n","\n","\n","def c_at_1(true_y, pred_y, threshold=0.5):\n","    \"\"\"\n","    Calculates the c@1 score, an evaluation method specific to the\n","    PAN competition. This method rewards predictions which leave\n","    some problems unanswered (score = 0.5). See:\n","\n","        A. Pe単as and A. Rodrigo. A Simple Measure to Assess Nonresponse.\n","        In Proc. of the 49th Annual Meeting of the Association for\n","        Computational Linguistics, Vol. 1, pages 1415-1424, 2011.\n","\n","    Parameters\n","    ----------\n","    prediction_scores : array [n_problems]\n","\n","        The predictions outputted by a verification system.\n","        Assumes `0 >= prediction <=1`.\n","\n","    ground_truth_scores : array [n_problems]\n","\n","        The gold annotations provided for each problem.\n","        Will always be `0` or `1`.\n","\n","    Returns\n","    ----------\n","    c@1 = the c@1 measure (which accounts for unanswered\n","        problems.)\n","\n","\n","    References\n","    ----------\n","        - E. Stamatatos, et al. Overview of the Author Identification\n","        Task at PAN 2014. CLEF (Working Notes) 2014: 877-897.\n","        - A. Pe単as and A. Rodrigo. A Simple Measure to Assess Nonresponse.\n","        In Proc. of the 49th Annual Meeting of the Association for\n","        Computational Linguistics, Vol. 1, pages 1415-1424, 2011.\n","\n","    \"\"\"\n","\n","    n = float(len(pred_y))\n","    nc, nu = 0.0, 0.0\n","\n","    for gt_score, pred_score in zip(true_y, pred_y):\n","        if pred_score == 0.5:\n","            nu += 1\n","        elif (pred_score > 0.5) == (gt_score > 0.5):\n","            nc += 1.0\n","\n","    return (1 / n) * (nc + (nu * nc / n))\n","\n","\n","def f1(true_y, pred_y, threshold=0.5):\n","    \"\"\"\n","    Assesses verification performance, assuming that every\n","    `score > 0.5` represents a same-author pair decision.\n","    Note that all non-decisions (scores == 0.5) are ignored\n","    by this metric.\n","\n","    Parameters\n","    ----------\n","    prediction_scores : array [n_problems]\n","\n","        The predictions outputted by a verification system.\n","        Assumes `0 >= prediction <=1`.\n","\n","    ground_truth_scores : array [n_problems]\n","\n","        The gold annotations provided for each problem.\n","        Will typically be `0` or `1`.\n","\n","    Returns\n","    ----------\n","    acc = The number of correct attributions.\n","\n","    References\n","    ----------\n","        E. Stamatatos, et al. Overview of the Author Identification\n","        Task at PAN 2014. CLEF (Working Notes) 2014: 877-897.\n","    \"\"\"\n","    true_y_filtered, pred_y_filtered = [], []\n","\n","    for true, pred in zip(true_y, pred_y):\n","        if pred != threshold:\n","            true_y_filtered.append(true)\n","            pred_y_filtered.append(pred)\n","\n","    pred_y_filtered = binarize(pred_y_filtered, threshold=threshold)\n","\n","    return f1_score(true_y_filtered, pred_y_filtered)\n","\n","\n","def f_05_u_score(true_y, pred_y, pos_label=1, threshold=0.5):\n","    \"\"\"\n","    Return F0.5u score of prediction.\n","\n","    :param true_y: true labels\n","    :param pred_y: predicted labels\n","    :param threshold: indication for non-decisions (default = 0.5)\n","    :param pos_label: positive class label (default = 1)\n","    :return: F0.5u score\n","    \"\"\"\n","\n","    pred_y = binarize(pred_y, triple_valued=True)\n","\n","    n_tp = 0\n","    n_fn = 0\n","    n_fp = 0\n","    n_u = 0\n","\n","    for i, pred in enumerate(pred_y):\n","        if pred == threshold:\n","            n_u += 1\n","        elif pred == pos_label and pred == true_y[i]:\n","            n_tp += 1\n","        elif pred == pos_label and pred != true_y[i]:\n","            n_fp += 1\n","        elif true_y[i] == pos_label and pred != true_y[i]:\n","            n_fn += 1\n","\n","    return (1.25 * n_tp) / (1.25 * n_tp + 0.25 * (n_fn + n_u) + n_fp)\n","\n","def brier_score(true_y, pred_y):\n","    \"\"\"\n","    Calculates the complement of the Brier score loss (which is bounded\n","    to the [0-1]), so that higher scores indicate better performance.\n","    This score also considers \"unanswered\" problem, where score = 0.5.\n","    We use the Brier implementation in scikit-learn [Pedregosa et al.\n","    2011].\n","\n","    Parameters\n","    ----------\n","    prediction_scores : array [n_problems]\n","\n","        The predictions outputted by a verification system.\n","        Assumes `0 >= prediction <=1`.\n","\n","    ground_truth_scores : array [n_problems]\n","\n","        The gold annotations provided for each problem.\n","        Will typically be `0` or `1`.\n","\n","    Returns\n","    ----------\n","    brier = float\n","        the complement of the Brier score\n","\n","    References\n","    ----------\n","    - Pedregosa, F. et al. Scikit-learn: Machine Learning in Python,\n","      Journal of Machine Learning Research 12 (2011), 2825--2830.\n","\n","    \"\"\"\n","    try:\n","        return 1 - brier_score_loss(true_y, pred_y)\n","    except ValueError:\n","        return 0.0\n","\n","\n","def load_file(fn):\n","    problems = {}\n","    for line in open(fn):\n","        d =  json.loads(line.strip())\n","        if 'value' in d:\n","            problems[d['id']] = d['value']\n","        else:\n","            problems[d['id']] = int(d['same'])\n","    return problems\n","\n","\n","def evaluate_all(true_y, pred_y):\n","    \"\"\"\n","    Convenience function: calculates all PAN20 evaluation measures\n","    and returns them as a dict, including the 'overall' score, which\n","    is the mean of the individual metrics (0 >= metric >= 1). All\n","    scores get rounded to three digits.\n","    \"\"\"\n","\n","    results = {'auc': auc(true_y, pred_y),\n","               'c@1': c_at_1(true_y, pred_y),\n","               'f_05_u': f_05_u_score(true_y, pred_y),\n","               'F1': f1(true_y, pred_y),\n","               'brier': brier_score(true_y, pred_y)\n","              }\n","\n","    results['overall'] = np.mean(list(results.values()))\n","\n","    for k, v in results.items():\n","        results[k] = round(v, 3)\n","\n","    return results\n","\n","\n","def aa_metrics(labels, predictions, raw_outputs, prefix='', no_auc=False, special=False):\n","\n","    accuracy = metrics.accuracy_score(labels, predictions)\n","    macro_accuracy = metrics.balanced_accuracy_score(labels, predictions)\n","    results = {\n","        f'{prefix}accuracy': accuracy,\n","        f'{prefix}macro_accuracy': macro_accuracy,\n","    }\n","    if special:\n","        return results\n","\n","    micro_recall = metrics.recall_score(labels, predictions, average='micro')\n","    macro_recall = metrics.recall_score(labels, predictions, average='macro')\n","    micro_precision = metrics.precision_score(labels, predictions, average='micro')\n","    macro_precision = metrics.precision_score(labels, predictions, average='macro')\n","\n","    # Calculate micro and macro F1 scores\n","    micro_f1 = metrics.f1_score(labels, predictions, average='micro')\n","    macro_f1 = metrics.f1_score(labels, predictions, average='macro')\n","\n","    results.update({\n","        f'{prefix}micro_recall': micro_recall,\n","        f'{prefix}macro_recall': macro_recall,\n","        f'{prefix}micro_precision': micro_precision,\n","        f'{prefix}macro_precision': macro_precision,\n","        f'{prefix}micro_f1': micro_f1,\n","        f'{prefix}macro_f1': macro_f1,\n","    })\n","\n","    if not no_auc:\n","        ovr_weighted_auc = metrics.roc_auc_score(labels, raw_outputs, average='weighted', multi_class='ovr')\n","        ovr_macro_auc = metrics.roc_auc_score(labels, raw_outputs, average='macro', multi_class='ovr')\n","        ovo_weighted_auc = metrics.roc_auc_score(labels, raw_outputs, average='weighted', multi_class='ovo')\n","        ovo_macro_auc = metrics.roc_auc_score(labels, raw_outputs, average='macro', multi_class='ovo')\n","        top2 = metrics.top_k_accuracy_score(labels, raw_outputs, k=2)\n","        top3 = metrics.top_k_accuracy_score(labels, raw_outputs, k=3)\n","        top4 = metrics.top_k_accuracy_score(labels, raw_outputs, k=4)\n","        top5 = metrics.top_k_accuracy_score(labels, raw_outputs, k=5)\n","        top6 = metrics.top_k_accuracy_score(labels, raw_outputs, k=6)\n","        top7 = metrics.top_k_accuracy_score(labels, raw_outputs, k=7)\n","        top8 = metrics.top_k_accuracy_score(labels, raw_outputs, k=8)\n","        top9 = metrics.top_k_accuracy_score(labels, raw_outputs, k=9)\n","        top10 = metrics.top_k_accuracy_score(labels, raw_outputs, k=10)\n","        micro_f1 = metrics.f1_score(labels, predictions, average=\"micro\")\n","        macro_f1 = metrics.f1_score(labels, predictions, average=\"macro\")\n","\n","        results.update({\n","            f'{prefix}ovr_weighted_auc': ovr_weighted_auc,\n","            f'{prefix}ovr_macro_auc': ovr_macro_auc,\n","            f'{prefix}ovo_weighted_auc': ovo_weighted_auc,\n","            f'{prefix}ovo_macro_auc': ovo_macro_auc,\n","            f'{prefix}micro_f1': micro_f1,\n","            f'{prefix}macro_f1': macro_f1,\n","            f'{prefix}top2': top2,\n","            f'{prefix}top3': top3,\n","            f'{prefix}top4': top4,\n","            f'{prefix}top5': top5,\n","            f'{prefix}top6': top6,\n","            f'{prefix}top7': top7,\n","            f'{prefix}top8': top8,\n","            f'{prefix}top9': top9,\n","            f'{prefix}top10': top10\n","        })\n","\n","    return results\n","\n","\n","def av_metrics(labels, predictions=None, probas=None, threshold=0.5, prefix=''):\n","\n","    assert (predictions is not None) or (probas is not None), \"no predictions or probas were passed in. . .\"\n","    if predictions is None:\n","        predictions = binarize(probas, threshold=threshold)\n","\n","    accuracy = metrics.accuracy_score(labels, predictions)\n","    macro_accuracy = metrics.balanced_accuracy_score(labels, predictions)\n","    micro_recall = metrics.recall_score(labels, predictions, average='micro', zero_division=0)\n","    macro_recall = metrics.recall_score(labels, predictions, average='macro', zero_division=0)\n","    micro_precision = metrics.precision_score(labels, predictions, average='micro', zero_division=0)\n","    macro_precision = metrics.precision_score(labels, predictions, average='macro', zero_division=0)\n","\n","    results = {\n","        f'{prefix}accuracy': accuracy,\n","        f'{prefix}macro_accuracy': macro_accuracy,\n","        f'{prefix}micro_recall': micro_recall,\n","        f'{prefix}macro_recall': macro_recall,\n","        f'{prefix}micro_precision': micro_precision,\n","        f'{prefix}macro_precision': macro_precision,\n","        f'{prefix}threshold': threshold,\n","    }\n","\n","    auc = metrics.roc_auc_score(labels, probas)\n","    f1 = metrics.f1_score(labels, predictions, zero_division=0)\n","\n","    results.update({\n","        f'{prefix}auc': auc,\n","        f'{prefix}f1': f1,\n","    })\n","\n","    return results\n","\n","\n","def main():\n","    parser = argparse.ArgumentParser(description='Evaluation script AA@PAN2020')\n","    parser.add_argument('-i', type=str,\n","                        help='Path to the jsonl-file with ground truth scores')\n","    parser.add_argument('-a', type=str,\n","                        help='Path to the jsonl-file with the answers (system prediction)')\n","    parser.add_argument('-o', type=str,\n","                        help='Path to output files')\n","    args = parser.parse_args()\n","\n","    # validate:\n","    if not args.i:\n","        raise ValueError('The ground truth path is required')\n","    if not args.a:\n","        raise ValueError('The answers path is required')\n","    if not args.o:\n","        raise ValueError('The output folder path is required')\n","\n","    # load:\n","    gt = load_file(f\"{args.i}/truth.jsonl\")\n","    pred = load_file(f\"{args.a}/answers.jsonl\")\n","\n","    print('->', len(gt), 'problems in ground truth')\n","    print('->', len(pred), 'solutions explicitly proposed')\n","\n","    # default missing problems to 0.5\n","    for probl_id in sorted(gt):\n","        if probl_id not in pred:\n","            pred[probl_id] = 0.5\n","\n","    # sanity check:\n","    assert len(gt) == len(pred)\n","    assert set(gt.keys()).union(set(pred)) == set(gt.keys())\n","\n","    # align the scores:\n","    scores = [(gt[k], pred[k]) for k in sorted(gt)]\n","    gt, pred = zip(*scores)\n","    gt = np.array(gt, dtype=np.float64)\n","    pred = np.array(pred, dtype=np.float64)\n","\n","    assert len(gt) == len(pred)\n","\n","    # evaluate:\n","    results = evaluate_all(gt, pred)\n","    print(results)\n","\n","    with open(args.o + os.sep + 'out.json', 'w') as f:\n","        json.dump(results, f, indent=4, sort_keys=True)\n","\n","    with open(args.o + os.sep + 'evaluation.prototext', 'w') as f:\n","        for metric, score in results.items():\n","            f.write('measure {\\n')\n","            f.write(' key: \"' + metric + '\"\\n')\n","            f.write(' value: \"' + str(score) + '\"\\n')\n","            f.write('}\\n')\n","\n","\n","def accuracy_calculator(normalized_similarities, truth, num_points=5):\n","    all_thresholds = [x for x in np.linspace(np.min(normalized_similarities), np.max(normalized_similarities), num_points)]\n","    best_acc = [0, 0]\n","    for threshold in all_thresholds:\n","        binarized_predictions = binarize(normalized_similarities, threshold)\n","        correct_predictions = np.zeros_like(binarized_predictions)\n","        correct_predictions[truth == binarized_predictions] = 1\n","        accuracy = sum(correct_predictions) / float(len(correct_predictions))\n","        best_acc = best_acc if best_acc[0] > accuracy else [accuracy, threshold]\n","    return best_acc\n","\n","\n","def accuracy_calculator_fixed_threshold(normalized_similarities, truth, threshold=None):\n","    if not threshold:\n","        threshold = np.mean(normalized_similarities)\n","    binarized_predictions = binarize(normalized_similarities, threshold)\n","    correct_predictions = np.zeros_like(binarized_predictions)\n","    correct_predictions[truth == binarized_predictions] = 1\n","    accuracy = sum(correct_predictions) / float(len(correct_predictions))\n","    return [accuracy, threshold]\n","\n","\n","def threshold_search(normalized_similarities, truth, num_thresholds=100):\n","\n","    # i think we actually should normalize the simialrities first\n","    # renormed_sims = normalized_similarities - np.min(normalized_similarities)\n","    # renormed_sims = renormed_sims / np.max(renormed_sims)\n","\n","    best_results = {'accuracy': 0,\n","                    'auc': 0,\n","                    'ca1': 0,\n","                    'f_05_u': 0,\n","                    'F1': 0,\n","                    'overall': 0,\n","                    'threshold': 0}\n","    all_thresholds = [x for x in np.linspace(np.min(normalized_similarities), np.max(normalized_similarities), num_thresholds)]\n","\n","    for threshold in all_thresholds:\n","        results = {\n","            'accuracy': accuracy_calculator_fixed_threshold(normalized_similarities, truth, threshold)[0],\n","            'auc': roc_auc_score(truth, normalized_similarities),\n","            'ca1': c_at_1(truth, normalized_similarities, threshold),\n","            'f_05_u': f_05_u_score(truth, normalized_similarities, threshold=threshold),\n","            'F1': f1(truth, normalized_similarities, threshold=threshold)\n","        }\n","        results['overall'] = np.mean(list(results.values()))\n","        results['threshold'] = threshold\n","\n","        best_results = best_results if best_results['accuracy'] > results['accuracy'] else copy.deepcopy(results)\n","\n","    return best_results\n"],"metadata":{"id":"o0_pLRE82N-Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# training"],"metadata":{"id":"rhr35yFHGNJ1"}},{"cell_type":"code","source":["def base_preprocessor(string: str) -> str:\n","    \"\"\"\n","    Function that computes regular expressions.\n","    \"\"\"\n","    string = re.sub(\"[0-9]\", \"0\", string)  # each digit will be represented as a 0\n","    string = re.sub(r'( \\n| \\t)+', '', string)\n","    # text = re.sub(\"[0-9]+(([.,^])[0-9]+)?\", \"#\", text)\n","    string = re.sub(\"https:\\\\\\+([a-zA-Z0-9.]+)?\", \"@\", string)\n","    return string\n","\n","\n","def char_diff_preprocessor(string: str) -> str:\n","    \"\"\"\n","    Function that computes regular expressions.\n","    \"\"\"\n","    string = base_preprocessor(string)\n","    string = re.sub(\"[a-zA-Z]+\", \"*\", string)\n","    # string = ''.join(['*' if char.isalpha() else char for char in string])\n","    return string\n","\n","\n","def word_preprocessor(string: str) -> str:\n","    \"\"\"\n","    Function that computes regular expressions.\n","    \"\"\"\n","    string = base_preprocessor(string)\n","    # if model is a word n-gram model, remove all punctuation\n","    string = ''.join([char for char in string if char.isalnum() or char.isspace()])\n","    return string\n","\n","\n","def get_vectorizers(analyzer: str = 'char',\n","                    gram_range: List = (1, 2),\n","                    preprocessor: Callable = base_preprocessor,\n","                    max_features: int = 1000,\n","                    min_df: float = 0.1,\n","                    smooth_idf: bool = True,\n","                    sublinear_tf: bool = True) -> Tuple[CountVectorizer, TfidfTransformer]:\n","    \"\"\"\n","    Get a vectorizer for this project\n","    \"\"\"\n","    logging.debug(f'Building a {gram_range} TfidfVectorizer for {analyzer} with the {preprocessor} preprocessor.')\n","    logging.debug(f'Other params:\\n\\t\\tmax_features: {max_features}\\n\\t\\tmin_df: {min_df}\\n\\t\\tsmooth_idf: '\n","                  f'{smooth_idf}\\n\\t\\tsublinear_tf: {sublinear_tf}')\n","    count_vectorizer = CountVectorizer(decode_error='ignore', strip_accents='unicode', lowercase=False, stop_words=None,\n","                                       ngram_range=gram_range, analyzer=analyzer, min_df=min_df,\n","                                       max_features=max_features)\n","    tfidf_vectorizer = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=smooth_idf, sublinear_tf=sublinear_tf)\n","    return count_vectorizer, tfidf_vectorizer\n","\n","\n","def ngram(analyzer: str, train_texts: List, train_labels: List, test_texts: List, test_labels: List, gram_range: List,\n","          preprocessor: Callable, max_features: int, min_df: float, sublinear_tf: bool, use_lsa: bool, lsa_factors: int,\n","          dual: bool, log_prefix: str, save_path: str = None, project: str = '', logistic_regression: bool = False,\n","          num_workers: int = 1):\n","    logging.info(f'{analyzer}: building the tf-idf vectorizer for the {analyzer} n-gram model')\n","    count_vectorizer, tfidf_transformer = get_vectorizers(analyzer=analyzer if 'dist' not in analyzer else 'char',\n","                                                          gram_range=gram_range,\n","                                                          preprocessor=preprocessor,\n","                                                          max_features=max_features,\n","                                                          min_df=min_df,\n","                                                          smooth_idf=True,\n","                                                          sublinear_tf=sublinear_tf)\n","\n","    # cache the vectorizer, just load it if the params match up\n","    count_vectorizer_cache_path = save_path + f'/cv_{project}_{analyzer}_{gram_range[0]}-{gram_range[1]}_' \\\n","                                  f'{max_features}_{min_df}.pkl'\n","    tfidf_vectorizer_cache_path = save_path + f'/idf_{project}_{analyzer}_{gram_range[0]}-{gram_range[1]}_' \\\n","                                  f'{max_features}_{min_df}_{sublinear_tf}.pkl'\n","\n","    # fit the count vectorizer\n","    if os.path.isfile(count_vectorizer_cache_path):\n","        logging.info(f'loading the pre-fit count vectorizer from {count_vectorizer_cache_path}')\n","        start = time.time()\n","        with open(count_vectorizer_cache_path, 'rb') as f:\n","            count_vectorizer = pickle.load(f)\n","        logging.debug(f'took {(time.time() - start) / 60} minutes')\n","        logging.info(f'transforming the texts with the pre-fit vectorizer.')\n","        train_term_matrix = count_vectorizer.transform(train_texts)\n","\n","    else:\n","        logging.info(f'{analyzer}: fitting the count vectorizer')\n","        start = time.time()\n","        train_term_matrix = count_vectorizer.fit_transform(train_texts).toarray()\n","        logging.info(f'saving count vectorizer to cache: {count_vectorizer_cache_path}')\n","        os.makedirs(os.path.dirname(count_vectorizer_cache_path), exist_ok=True)\n","        with open(count_vectorizer_cache_path, 'wb') as f:\n","            pickle.dump(count_vectorizer, f)\n","        logging.debug(f'took {(time.time() - start) / 60} minutes')\n","\n","    # fit the tfidf transformer\n","    if os.path.isfile(tfidf_vectorizer_cache_path):\n","        logging.info(f'loading the pre-fit tfidf vectorizer from {tfidf_vectorizer_cache_path}')\n","        start = time.time()\n","        with open(tfidf_vectorizer_cache_path, 'rb') as f:\n","            tfidf_transformer = pickle.load(f)\n","        logging.debug(f'took {(time.time() - start) / 60} minutes')\n","        logging.info(f'transforming the training texts with the  tfidf transformer')\n","        train_data = tfidf_transformer.transform(train_term_matrix)\n","    else:\n","        logging.info(f'{analyzer}: fitting the tfidf vectorizer')\n","        start = time.time()\n","        train_data = tfidf_transformer.fit_transform(train_term_matrix).toarray()\n","        logging.info(f'saving tfidf vectorizer to cache: {tfidf_vectorizer_cache_path}')\n","        os.makedirs(os.path.dirname(tfidf_vectorizer_cache_path), exist_ok=True)\n","        with open(tfidf_vectorizer_cache_path, 'wb') as f:\n","            pickle.dump(tfidf_transformer, f)\n","        logging.debug(f'took {(time.time() - start) / 60} minutes')\n","\n","\n","    logging.info(f'{analyzer}: vectorizing the test texts')\n","    test_data = tfidf_transformer.transform(count_vectorizer.transform(test_texts).toarray()).toarray()\n","\n","    logging.info(f'{analyzer}: scaling the vectorized data')\n","    max_abs_scaler = preprocessing.MaxAbsScaler()\n","    scaled_train_data = max_abs_scaler.fit_transform(train_data)\n","    scaled_test_data = max_abs_scaler.transform(test_data)\n","\n","    if use_lsa:\n","        lsa_cache_path = save_path + f'/lsa_{project}_{analyzer}_{gram_range[0]}-{gram_range[1]}_{max_features}_' \\\n","                                f'{min_df}_{sublinear_tf}_{lsa_factors}.pkl'\n","        if os.path.isfile(lsa_cache_path):\n","            logging.info(f'loading the svd transform from cache')\n","            start = time.time()\n","            with open(lsa_cache_path, 'rb') as f:\n","                svd = pickle.load(f)\n","            scaled_train_data = svd.transform(scaled_train_data)\n","            scaled_test_data = svd.transform(scaled_test_data)\n","            logging.debug(f'took {(time.time() - start) / 60} minutes')\n","        else:\n","            logging.info(f'{analyzer}: reducing demensionality with TruncatedSVD')\n","            start = time.time()\n","            svd = TruncatedSVD(n_components=lsa_factors, algorithm='randomized', random_state=0)\n","            # Char\n","            scaled_train_data = svd.fit_transform(scaled_train_data)\n","            scaled_test_data = svd.transform(scaled_test_data)\n","            logging.debug(f'took {(time.time() - start) / 60} minutes')\n","            # cache the svd\n","            with open(lsa_cache_path, 'wb') as f:\n","                pickle.dump(svd, f)\n","\n","    logging.info(f'{analyzer}: fitting the classifier')\n","    start = time.time()\n","    # This was the classifier used in the original implementation, but we need a more efficient one\n","    # char_std = CalibratedClassifierCV(OneVsRestClassifier(SVC(C=1, kernel='linear',\n","    #                                                           gamma='auto', verbose=True)))\n","    if logistic_regression:\n","        # classifier = LogisticRegression(multi_class='multinomial', dual=dual)\n","        classifier = SGDClassifier(loss='log_loss', n_jobs=num_workers, early_stopping=False, verbose=1)\n","    else:\n","        classifier = LogisticRegression(multi_class='multinomial', dual=dual)\n","\n","    classifier.fit(scaled_train_data, train_labels)\n","    logging.debug(f'took {(time.time() - start) / 60} minutes')\n","\n","    logging.info(f'{analyzer}: inference on the test set')\n","    start - time.time()\n","    predictions = classifier.predict(scaled_test_data)\n","    predicted_probs = classifier.predict_proba(scaled_test_data)\n","    logging.debug(f'took {(time.time() - start) / 60} minutes')\n","\n","    # compute and log char ngram\n","    logging.info(f'{analyzer}: logging to wandb')\n","    wandb.sklearn.plot_classifier(classifier,\n","                                  scaled_train_data, scaled_test_data,\n","                                  train_labels, test_labels,\n","                                  predictions, predicted_probs,\n","                                  [x for x in range(len(set(train_labels)))],\n","                                  is_binary=False,\n","                                  model_name=analyzer)\n","    results = aa_metrics(test_labels, predictions, predicted_probs, prefix=log_prefix, no_auc=True)\n","    wandb.log(results)\n","\n","    # save the model\n","    clf_name = 'logreg_sgd' if logistic_regression else 'logreg'\n","    svm_path = os.path.join(os.path.dirname(tfidf_vectorizer_cache_path), f'{analyzer}_{clf_name}.pkl')\n","    logging.debug(f'saving the {analyzer}_{clf_name} to {svm_path}')\n","    with open(svm_path, 'wb') as f:\n","        pickle.dump(classifier, f)\n","\n","    wandb.save(svm_path)\n","\n","    return predicted_probs\n","\n","\n","def run_ngram(config={}, ngram_type: str = 'char', train_pth: str = None, val_pth: str = None, test_pth: str = None,\n","              project='', num_workers=10, wandb_name=''):\n","\n","    # need to make sure config is a namespace\n","    if isinstance(config, dict):\n","        config = argparse.Namespace(**config)\n","\n","    sweep = True if project != '' else False\n","    project = project if project != '' else config.project\n","    # wandb_name = config.wandb_name\n","    wandb_tags = config.wandb_tags\n","    tmp = vars(config)\n","    tmp['model'] = ngram_type\n","    # print(sweep)\n","\n","    with wandb.init(project=project, config=tmp, reinit=True, name=wandb_name, tags=eval(wandb_tags)):\n","        if sweep:\n","            config = wandb.config\n","            config.project = project\n","            config.num_workers = num_workers\n","\n","        # config.project = project\n","        # config.save_path = os.path.join('ngram', project, wandb.run.name)\n","\n","        # config.model = ngram_type\n","        # wandb.config.update(config)\n","\n","        # add the run name to make sure we don't overwrite other models\n","        # if config.save_path is not None:\n","        save_path = os.path.join('/content/drive/MyDrive/msc_project/model/baseline', project, wandb.run.name.split('_')[0])\n","        print(save_path)\n","        logging.info('starting')\n","\n","        # get the training and testing dataset as List[List[Union[int, str]]]\n","        logging.info('loading the datasets')\n","        train_dset = get_aa_dataset(train_pth)\n","        test_dset = get_aa_dataset(test_pth)\n","        # print(train_dset)\n","        # print(test_dset)\n","\n","        if val_pth is not None and val_pth != '':\n","            log_prefix = 'test/'\n","            train_dset.extend(get_aa_dataset(val_pth))\n","        else:\n","            log_prefix = 'val/'\n","\n","        train_texts = [text for _, text in train_dset]\n","        train_labels = [label for label, _ in train_dset]\n","        test_texts = [text for _, text in test_dset]\n","        test_labels = [label for label, _ in test_dset]\n","        # print(train_texts)\n","        # print(train_labels)\n","        # print(test_texts)\n","        # print(test_labels)\n","\n","        # get the proper preprocessor and make sure ngram_type is set for the vectorizer\n","        if sweep:\n","            gram_range = config.gram_range\n","        if ngram_type == 'char':\n","            preprocessor = base_preprocessor\n","            if not sweep:\n","                gram_range = config.char_range\n","        elif ngram_type == 'dist_char':\n","            preprocessor = char_diff_preprocessor\n","            if not sweep:\n","                gram_range = config.dist_range\n","        elif ngram_type == 'word':\n","            preprocessor = word_preprocessor\n","            if not sweep:\n","                gram_range = config.word_range\n","        else:\n","            raise ValueError(f'ngram_type was not set properly, should be in [char, dist_char, word], got {ngram_type}')\n","\n","        probas = ngram(analyzer=ngram_type, train_texts=train_texts, train_labels=train_labels, test_texts=test_texts,\n","                       test_labels=test_labels, gram_range=gram_range, preprocessor=preprocessor,\n","                       max_features=config.max_features,\n","                       min_df=config.min_df, sublinear_tf=config.sublinear_tf, use_lsa=config.use_lsa,\n","                       lsa_factors=config.lsa_factors, dual=not config.primal, log_prefix=log_prefix,\n","                       save_path=save_path, project=config.project, logistic_regression=config.logistic_regression,\n","                       num_workers=config.num_workers)\n","\n","    return probas\n","\n","\n","def ensemble(config, test_labels, probas_word, probas_char, probas_dist, prefix='', wandb_name=''):\n","\n","    if isinstance(config, dict):\n","        config = argparse.Namespace(**config)\n","\n","    config.model = 'ensemble'\n","    wandb_tags = config.wandb_tags\n","\n","    with wandb.init(project=args.project, config=vars(config), reinit=True, name=wandb_name, tags=eval(wandb_tags)):\n","\n","        logging.info('ensembling the models')\n","\n","        avg_probas = np.average([probas_word, probas_dist, probas_char], axis=0)\n","        avg_predictions = []\n","        for text_probs in avg_probas:\n","            ind_best = np.argmax(text_probs)\n","            avg_predictions.append(ind_best)\n","\n","        ensemble_results = aa_metrics(test_labels, avg_predictions, avg_probas, prefix=prefix, no_auc=True)\n","        wandb.log(ensemble_results)\n","        wandb.finish()\n","        logging.info('done')\n"],"metadata":{"id":"v3X2lN98xc6v"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Run"],"metadata":{"id":"TcTBOj3qY4OE"}},{"cell_type":"markdown","source":["## IMDB62"],"metadata":{"id":"xZRD5-5VY6Nz"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='imdb62_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_train.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_val.csv')\n","    # parser.add_argument('--val_dataset', type=str,\n","    #                     default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_test.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(lower=True, use_lsa=True, sublinear_tf=True, primal=True, logistic_regression=True)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"UbZ5TTLipqD2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='imdb62_2')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_train.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_val.csv')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_test.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(lower=True, use_lsa=True, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"Wq9LU64A1PZb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='imdb62_3')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_train.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_val.csv')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_test.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"7Bx5GmoWEGlc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='imdb62_4')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_train.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_val.csv')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_test.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=False, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"LfjU2ejFYZbC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='imdb62_3')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_train.csv')\n","    # parser.add_argument('--val_dataset', type=str,\n","    #                     default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_val.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/imdb62/processed/imdb62_AA_test.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"rWxQWL5yycea"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## blogs50"],"metadata":{"id":"jckZE6bVY_Kd"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='blogs50_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/blogs50/processed/blogs50_train.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/blogs50/processed/blogs50_AA_val.csv')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/blogs50/processed/blogs50_AA_test.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"UydnMIc1Gu3w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='blogs50_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/blogs50/processed/blogs50_train.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/blogs50/processed/blogs50_AA_test.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"1NhS9OhwCL8n"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# diffusionDB"],"metadata":{"id":"NQaeB45gVUpo"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='diffusiondbpara_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/train_random100_label_1.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"u9Sj6v2N73g1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='diffusiondbpara_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/train_random100_label_1.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/paraphrased/test_random100_label_1.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"ZtoBEJXIZvtm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='diffusiondbclean_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/clean/train_random100_label_1.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/clean/test_random100_label_1.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"jOhSCkTp06YH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# diffusiondb_vary"],"metadata":{"id":"bam6ToZ8oVul"}},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='diffusiondbvary60_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/vary/train_random60_label_1.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random60_label_1.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"T_1j6B1soUoS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='diffusiondbvary80_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/vary/train_random80_label_1.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random80_label_1.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"R-CMjjJgrzGV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='diffusiondbvary120_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/vary/train_random120_label_1.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random120_label_1.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"T-3nhGzfs0L2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='diffusiondbvary100150_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/vary/train_random100_150_label_1.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random100_150_label_1.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"8dyd1focuD1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='diffusiondbvary100200_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/vary/train_random100_200_label_1.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/vary/test_random100_200_label_1.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"DgduIU4jvlHK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# others"],"metadata":{"id":"H2bANzAqrxLq"}},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/msc_project/data/preprocessed_diffusiondb.csv')\n","\n","df_selected = df[['user_name', 'prompt']]\n","df_filtered = df_selected.groupby('user_name').filter(lambda x: len(x) >= 100)\n","print('number of authors', len(df_filtered['user_name'].drop_duplicates()))\n","\n","for idx in range(3):\n","  sampled_authors = df_filtered['user_name'].drop_duplicates().sample(n=100)\n","  df_sampled = df_filtered[df_filtered['user_name'].isin(sampled_authors)]\n","  df_final = df_sampled.groupby('user_name').apply(lambda x: x.sample(n=100)).reset_index(drop=True)\n","\n","  train_data = pd.DataFrame()\n","  test_data = pd.DataFrame()\n","\n","  for author in df_final['user_name'].unique():\n","      author_data = df_final[df_final['user_name'] == author]\n","      train, test = train_test_split(author_data, test_size=0.2)\n","      train_data = pd.concat([train_data, train])\n","      test_data = pd.concat([test_data, test])\n","\n","  train_data.to_csv(f'/content/drive/MyDrive/msc_project/data/diffusiondb/train_random100_{idx+1}.csv', index=False)\n","  test_data.to_csv(f'/content/drive/MyDrive/msc_project/data/diffusiondb/test_random100_{idx+1}.csv', index=False)\n","  # print(train_data)\n","  # print(test_data)"],"metadata":{"id":"XY0R4MmW6D3W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/train_random100_1.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_1.csv')\n","\n","label_encoder = LabelEncoder()\n","\n","df_train['user_name'] = label_encoder.fit_transform(df_train['user_name'])\n","df_test['user_name'] = label_encoder.transform(df_test['user_name'])\n","\n","with open('/content/drive/MyDrive/msc_project/data/diffusiondb/processed/label_encoder.pkl', 'wb') as le_file:\n","    pickle.dump(label_encoder, le_file)\n","\n","df_train.to_csv(f'/content/drive/MyDrive/msc_project/data/diffusiondb/processed/train_random100_label_1.csv', index=False)\n","df_test.to_csv(f'/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv', index=False)"],"metadata":{"id":"6pVDF53Q_QqG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='diffusiondb_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/processed/train_random100_label_1.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/diffusiondb/processed/test_random100_label_1.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"2jE3G1T0JELM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# twitter_micro"],"metadata":{"id":"Mcowp3wZBzE2"}},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/author_texts_cleaned.csv')\n","df.columns = ['user_name', 'prompts']\n","# df_selected = df[['user_name', 'prompt']]\n","df_filtered = df.groupby('user_name').filter(lambda x: len(x) >= 100)\n","print('number of authors', len(df_filtered['user_name'].drop_duplicates()))\n","\n","for idx in range(3):\n","  sampled_authors = df_filtered['user_name'].drop_duplicates().sample(n=100)\n","  df_sampled = df_filtered[df_filtered['user_name'].isin(sampled_authors)]\n","  df_final = df_sampled.groupby('user_name').apply(lambda x: x.sample(n=100)).reset_index(drop=True)\n","\n","  train_data = pd.DataFrame()\n","  test_data = pd.DataFrame()\n","\n","  for author in df_final['user_name'].unique():\n","      author_data = df_final[df_final['user_name'] == author]\n","      train, test = train_test_split(author_data, test_size=0.2)\n","      train_data = pd.concat([train_data, train])\n","      test_data = pd.concat([test_data, test])\n","\n","  train_data.to_csv(f'/content/drive/MyDrive/msc_project/data/twitter_micro/processed/train_random100_{idx+1}.csv', index=False)\n","  test_data.to_csv(f'/content/drive/MyDrive/msc_project/data/twitter_micro/processed/test_random100_{idx+1}.csv', index=False)\n","  # print(train_data)\n","  # print(test_data)"],"metadata":{"id":"zFJVU9K7Bz9N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_train = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/processed/train_random100_1.csv')\n","df_test = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/processed/test_random100_1.csv')\n","\n","label_encoder = LabelEncoder()\n","\n","df_train['user_name'] = label_encoder.fit_transform(df_train['user_name'])\n","df_test['user_name'] = label_encoder.transform(df_test['user_name'])\n","\n","with open('/content/drive/MyDrive/msc_project/data/twitter_micro/processed/label_encoder.pkl', 'wb') as le_file:\n","    pickle.dump(label_encoder, le_file)\n","\n","df_train.to_csv(f'/content/drive/MyDrive/msc_project/data/twitter_micro/processed/train_random100_label_1.csv', index=False)\n","df_test.to_csv(f'/content/drive/MyDrive/msc_project/data/twitter_micro/processed/test_random100_label_1.csv', index=False)"],"metadata":{"id":"hXCbAmgpD3JX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == '__main__':\n","    # get command line args\n","    parser = argparse.ArgumentParser(description='Run a N-Gram model from the command line')\n","\n","    parser.add_argument('--project', type=str, default='CharNGram',\n","                        help='the mlflow experiment name')\n","    parser.add_argument('--wandb_name', type=str, default='twitter_1')\n","    parser.add_argument('--wandb_tags', type=str, default='[\"baseline\"]')\n","    # parser.add_argument('--wandb_notes', type=str, default='default hyperparameters')\n","    parser.add_argument('--train_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/twitter_micro/processed/train_random100_label_1.csv')\n","    parser.add_argument('--val_dataset', type=str,\n","                        default='')\n","    parser.add_argument('--test_dataset', type=str,\n","                        default='/content/drive/MyDrive/msc_project/data/twitter_micro/processed/test_random100_label_1.csv')\n","    parser.add_argument('--save_path', type=str, default=None)\n","    parser.add_argument('--seed', metavar='seed', type=int, default=0)\n","    parser.add_argument('--word_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--dist_range', nargs='+', type=int, default=[1, 3])\n","    parser.add_argument('--char_range', nargs='+', type=int, default=[2, 5])\n","    parser.add_argument('--n_best_factor', type=float, default=0.5)\n","    parser.add_argument('--pt', type=float, default=0.1)\n","    parser.add_argument('--lower', action='store_true')\n","    parser.add_argument('--use_lsa', action='store_true') # to reduce the number of features\n","    parser.add_argument('--lsa_factors', type=int, default=63)\n","    parser.add_argument('--sublinear_tf', action='store_true') # suitable when you want to reduce the impact of very frequent terms or when working with datasets where document length varies significantly\n","    parser.add_argument('--primal', action='store_true') # set to True if the number of samples is greater than the number of features\n","    parser.add_argument('--max_features', type=int, default=100_000)\n","    parser.add_argument('--min_df', type=float, default=0.01)\n","    parser.add_argument('--type', type=str, default='')\n","    parser.add_argument('--logistic_regression', action='store_true')\n","    parser.add_argument('--num_workers', type=int, default=10)\n","\n","    parser.set_defaults(use_lsa=False, sublinear_tf=True, primal=True, logistic_regression=False)\n","\n","    # args = parser.parse_args()\n","    args, unknown = parser.parse_known_args()\n","    print(args)\n","    print(unknown)\n","\n","    args.word_range = tuple(args.word_range)\n","    args.dist_range = tuple(args.dist_range)\n","    args.char_range = tuple(args.char_range)\n","\n","    np.random.seed(args.seed)\n","\n","    wandb.login()\n","\n","    log_prf = 'test' if 'test' in args.test_dataset else 'val'\n","\n","    total_time_start = time.time()\n","\n","    if args.type == '':\n","        char_probas = run_ngram(args, 'char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_char')\n","        dist_probas = run_ngram(args, 'dist_char', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_dist')\n","        word_probas = run_ngram(args, 'word', train_pth=args.train_dataset, val_pth=args.val_dataset,\n","                                test_pth=args.test_dataset, wandb_name=args.wandb_name+'_word')\n","        # now ensemble the results\n","        test_lbls = [lbl for lbl, _ in get_aa_dataset(args.test_dataset)]\n","        ensemble(args, test_labels=test_lbls, probas_char=char_probas, probas_dist=dist_probas, probas_word=word_probas, wandb_name=args.wandb_name+'_ensemble')\n","    else:\n","        run_ngram(args, args.type, train_pth=args.train_dataset, val_pth=args.val_dataset, test_pth=args.test_dataset)\n","\n","    logging.info(f'this run took {(time.time() - total_time_start)/60} minutes')"],"metadata":{"id":"vzIVB1DsCB7b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_csv('/content/drive/MyDrive/msc_project/data/twitter_micro/processed/train_random100_label_1.csv')\n","df"],"metadata":{"id":"0KwjhtWTDEPP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"j4dJh9E_DyOQ"},"execution_count":null,"outputs":[]}]}